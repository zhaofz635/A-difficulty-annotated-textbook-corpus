[
    {
        "text": "this is the charpter",
        "image_path": "",
        "cognitive_load_label": 1
    },
    {
        "text": "we wll Learning some interesting things",
        "image_path": "",
        "cognitive_load_label": 1
    },
    {
        "text": "Technical Principles and Applications of Large Language Models",
        "image_path": "",
        "cognitive_load_label": 1
    },
    {
        "text": "Learning Task 1: Fully understand the roles of different elements in the Transformer model structure.",
        "image_path": "",
        "cognitive_load_label": 1
    },
    {
        "text": "Transformer Model Structure: We can divide this diagram into two parts: the left side is the Encoder, which understands the input text, and the right side is the Decoder, which generates new text.",
        "image_path": "dataset/images/transformer_structure.svg",
        "cognitive_load_label": 3.2
    },
    {
        "text": "Encoder Side: Input Embedding - turning each word into a vector holding its meaning. Positional Encoding - adds position info to the words. Multi-Head Attention - allows the model to focus on different parts of the sentence.",
        "image_path": "",
        "cognitive_load_label": 2.9
    },
    {
        "text": "Decoder Side: Masked Multi-Head Attention - generating words by only looking at previous words. Encoder-Decoder Attention - connects input sentences to generated text.",
        "image_path": "",
        "cognitive_load_label": 3.5
    },
    {
        "text": "Euler's Formula: One of the most beautiful equations in mathematics: e^{iπ} + 1 = 0. It involves fundamental constants: e (Euler’s number), i (imaginary unit), and π (pi).",
        "image_path": "",
        "cognitive_load_label": 3.0
    },
    {
        "text": "In linear regression, the Mean Squared Error (MSE) is a commonly used loss function to measure the difference between predicted values and actual values: $ L(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 $. $ L(\\theta) $: The loss function. $ m $: The number of training samples. $ h_\\theta(x^{(i)}) $: The predicted value from the model, where $ h_\\theta(x) = \\theta_0 + \\theta_1 x $. $ y^{(i)} $: The actual value of the $ i $-th training sample.",
        "image_path":"/dataset/images/MeanSquaredError.jpeg",
        "cognitive_load_label": 2.5
    },
    {
        "text": "In mathematical analysis, the theorem $e^{i\\pi} + 1 = 0$ is a fundamental result.",
        "image_path":"/dataset/images/partial.png",
        "cognitive_load_label": 2.5
    },
        {
            "text": "Cognitive Load Assessment Dimensions for Teaching Materials",
            "image_path": "dataset/images/cognitive_load_metrics_table.png",
            "cognitive_load_label": 2.8,
            "structured_data": {
                "table_type": "cognitive_load_metrics",
                "columns": ["Dimension", "Principle", "Quantitative Metrics", "Calculation Method / Tools"],
                "rows": [
                    {
                        "dimension": "Structural Chaos",
                        "principle": "Structural chaos reflects the logical coherence of the text. When paragraph lengths fluctuate significantly, readers struggle to establish a stable reading rhythm.",
                        "metrics": ["Chapter Matching", "Text Length Stability", "Logical Leap"],
                        "calculation": {
                            "methods": [
                                {
                                    "name": "Regular Expressions",
                                    "description": "Count chapter markers (e.g., Chapter 1, Section 2.3, Slide 4)"
                                },
                                {
                                    "name": "Standard Deviation Calculation",
                                    "description": "Calculate the standard deviation of text character count/word count (np.std())"
                                },
                                {
                                    "name": "Short Sentence Ratio",
                                    "description": "Count the proportion of short sentences (<5 words) and line breaks (nltk.sent_tokenize())"
                                }
                            ],
                            "fusion_formula": "Weighted calculation: Chapter Matching (40%) + Length Stability (30%) + Logical Leap (30%)"
                        }
                    },
                    {
                        "dimension": "Language Complexity",
                        "principle": "Language complexity reflects the syntactic complexity and comprehension difficulty. Long sentences, multiple clauses, and technical terms increase cognitive load during reading.",
                        "metrics": ["Average Sentence Length", "Number of Clauses", "Proportion of Technical Terms (TF-IDF)"],
                        "calculation": {
                            "tools": ["spaCy", "Stanford NLP", "TF-IDF"],
                            "description": "Parse syntactic structures to calculate sentence length and clause count, and use TF-IDF to evaluate term density"
                        }
                    }
                ]
            }
    },
    {
        "text": "Figure 1.1 Computing is everywhere, affecting everyone, for better and for worse. (credit: modification of Whereas design is expansive, engineering is narrowing by Jessie Huynh Critically Conscious Computing, CC0)",
        "image_path":"/dataset/images/Figure 1.1.jpeg",
        "cognitive_load_label": 2
    },
    {
        "text": "This textbook will introduce you to the exciting and complex world of computer science. In this chapter, you’ll review the history of computer science, learn about its use in different fields, and explore how computer science will impact the future of society. Computer science is a powerful tool, and computer scientists have used their vast knowledge of technology to create and implement technology that has transformed societies around the world.",
        "cognitive_load_label": 1
    },

    {
        "text": "This book will also introduce the computational thinking aspects of problem-solving and analytical thinking that enable the study of algorithms, which are step-by-step instructions for solving specific problems or carrying out computations. Therefore, this book also covers algorithms and their realization via programming languages, computer systems architectures, networks, and operating systems. The book subsequently delves into computer science areas that enable the design and development of software solutions using high-level programming languages (i.e., coding languages designed to be more intuitive for humans), architectural styles and related models, data management systems, and software engineering. Finally, the book demonstrates how to leverage computer science realizations and areas to build modern end-to-end solutions to business and social problems. In particular, the book focuses on modern web applications development, cloud-native applications development, and hybrid Cloud/on-premise digital solutions. The various chapters emphasize how to achieve software solution qualities such as performance and scalability. The last chapter explains how to secure software applications and their applications in the context of various cyber threats. It also explains how to make the right decisions about using computers and information in society to navigate social, ethical, economic, and political issues that could result from the misuse of technology. To conclude this textbook, we’ll introduce you to cybersecurity and help you understand why responsible computing is essential to promote ethical behavior in computer science. The book is designed to help students grasp the full meaning of computer science as a tool that can help them think, build meaningful solutions to complex problems, and motivate their careers in information technology (IT).",
        "cognitive_load_label": 2.4
    },  
    {
        "text": "You’re already familiar with computer science. Whenever you use a laptop, tablet, cell phone, credit card reader, and other technology, you interact with items made possible by computer science. Computer science is a challenging field, and the outputs of computer science offer many benefits for society. At the same time, we have to be cautious about how we use computer science to ensure it impacts society in ethical ways. To help you understand this, the next section will explain how computer science came to be and discuss the field’s potential.",
        "cognitive_load_label": 3.5
    },  
    {
        "text": "You’re already familiar with computer science. Whenever you use a laptop, tablet, cell phone, credit card reader, and other technology, you interact with items made possible by computer science. Computer science is a challenging field, and the outputs of computer science offer many benefits for society. At the same time, we have to be cautious about how we use computer science to ensure it impacts society in ethical ways. To help you understand this, the next section will explain how computer science came to be and discuss the field’s potential.",
        "cognitive_load_label": 2.1
    },

    {
        "text": "1.1 computer science The field of computer science (CS) is the study of computing, which includes all phenomena related to computers, such as the Internet. With foundations in engineering and mathematics, computer science focuses on studying algorithms. An algorithm is a sequence of precise instructions that enables computing. This includes components computers use to process information. By studying and applying algorithms, computer science creates applications and solutions that impact all areas of society. For example, computer science developed the programs that enable online shopping, texting with friends, streaming music, and other technological processes.",
        "cognitive_load_label": 2.6
    },
    {
        "text": "While computers are common today, they weren’t always this pervasive. For those whose lives have been shaped by computer technology, it can sometimes seem like computer technology is ahistorical: computing often focuses on rapid innovation and improvement, wasting no time looking back and reflecting on the past. Yet the foundations of computer science defined over 50, and as much as 100, years ago very much shape what is possible with computing today.",
        "cognitive_load_label": 2.7
    },
    {
        "text": "1.1.1 The Early History of Computing. The first computing devices were not at all like the computers we know today. They were physical calculation devices such as the abacus, which first appeared in many societies across the world thousands of years ago. They allowed people to tally, count, or add numbers (Figure 1.2). Today, abaci are still used in some situations, such as helping small children learn basic arithmetic, keeping score in games, and as a calculating tool for people with visual impairments. However, abaci are not common today because of the invention of number systems such as the Arabic number system (0, 1, 2, 3, . . .), which included zero and place values that cannot be computed with abaci. The concept of an algorithm was also invented around this time. Algorithms use inputs and a finite number of steps to carry out arithmetic operations like addition, subtraction, multiplication, and division, and produce outputs used in computing. Today’s computers still rely on the same foundations of numbers, calculations, and algorithms, except at the scale of billions of numbers and billions of calculations per second. ",
        "cognitive_load_label": 3.3
    },
    {
        "text": "To introduce a concrete example of an algorithm, let us consider binary search algorithm, which is used to locate a number in a sorted array of integers efficiently. The algorithm operates by repeatedly dividing the search interval in half to perform the search. If the number being searched is less than the integer in the middle of the interval, the interval is narrowed to the lower half. In the alternative, the interval is narrowed to the upper half. The algorithm repeatedly checks until the number is found or the interval is empty.",
        "cognitive_load_label": 2.1
    },
        {
        "text": "Algorithms may sound complicated, but they can be quite simple. For example, recipes to prepare food are algorithms with precise directions for ingredient amounts, the process to combine these, and the temperatures and cooking methods needed to transform the combined ingredients into a specific dish. The dish is the output produced by following the algorithm of a recipe.",
        "cognitive_load_label": 2.0
    },
        {
        "text": "The next major development in the evolution of computing occurred in 1614 when John Napier, a Scottish mathematician, developed logarithms, which express exponents by denoting the power that a number must be raised to obtain another value. Logarithms provided a shortcut for making tedious calculations and became the foundation for multiple analog calculating machines invented during the 1600s.",
        "cognitive_load_label": 2.9
    },
        {
        "text": "Scientists continued to explore different ways to speed up or automate calculations. In the 1820s, English mathematician Charles Babbage invented the Difference Engine with the goal of preventing human errors in manual calculations. The Difference Engine provided a means to automate the calculations of polynomial functions and astronomical calculations.",
        "cognitive_load_label": 2.4
    },
        {
        "text": "Babbage followed the Difference Engine with his invention of the Analytical Engine. With assistance from Ada Lovelace, the Analytical Engine was program-controlled and included features like an integrated memory and an arithmetic logic unit. Lovelace used punched cards to create sequencing instructions that could be read by the Analytical Engine to automatically perform any calculation included in the programming code. With her work on the Analytical Engine, Lovelace became the world’s first computer programmer.",
        "cognitive_load_label": 2.5
    },
        {
        "text": "The next major development in computing occurred in the late 1800s when Herman Hollerith, an employee of the U.S. Census Office, developed a machine that could punch cards and count them. In 1890, Hollerith’s invention was used to tabulate and prepare statistics for the U.S. census.",
        "cognitive_load_label": 2.4
    },
        {
        "text": "By the end of the 1800s and leading into the early 1900s, calculators, adding machines, typewriters, and related machines became more commonplace, setting the stage for the invention of the computer. In the 1940s, multiple computers became available, including IBM’s Harvard Mark 1. These were the forerunners to the advent of the digital computer in the 1950s, which changed everything and evolved into the computers and related technology we have today.",
        "cognitive_load_label": 2.3
    },
        {
        "text": "Around this time, computer science emerged as an academic discipline rooted in the principles of mathematics, situated primarily in elite institutions, and funded by demand from the military for use in missile guidance systems, airplanes, and other military applications. As computers could execute programs faster than humans, computer science replaced human-powered calculation with computer-powered problem-solving methods. In this way, the earliest academic computer scientists envisioned computer science as a discipline that was far more intellectual and cognitive compared to the manual calculation work that preceded it.",
        "cognitive_load_label": 2.8
    },
        {
        "text": "Richard Bellman was a significant contributor to this effort. A mathematics professor at Princeton and later at Stanford in the 1940s, Bellman later went to work for the Rand Corporation, where he studied the theory of multistage decision processes. In 1953, Bellman invented dynamic programming,1 which is a mathematical optimization methodology and a technique for computer programming. With dynamic programming, complex problems are divided into more manageable subproblems. Each subproblem is solved, and the results are stored, ultimately resulting in a solution to the overall complex problem.2 With this approach, Bellman helped revolutionize computer programming and enable computer science to become a robust field.",
        "cognitive_load_label": 2.3
    },
        {
        "text": "1.1.2 What Is Computer Science? The term computer science was popularized by George E. Forsythe in 1961. A mathematician who founded Stanford University’s computer science department, Forsythe defined computer science as “the theory of programming, numerical analysis, data processing, and the design of computer systems.” He also argued that computer science was distinguished from other disciplines by the emphasis on algorithms, which are essential for effective computer programming.3",
        "cognitive_load_label": 2.4
    },
        {
        "text": "Computer science is not only about the study of how computers work, but also everything surrounding computers, including the people who design computers, the people who write programs that run on computers, the people who test the programs to ensure correctness, and the people who are directly and indirectly affected by computers. In this way, computer science is as much about people and how they work with computers as it is about just computers.",
        "cognitive_load_label": 2.3
    },
        {
        "text": "Not everyone agrees with this definition. Some people argue that computer science is more about computers or software than the people it affects. However, even if we were to study just the “things” of computer science, the people are still there. When someone designs a computer system, they are thinking about what kinds of programs people might want to run. Typically, effort is made to design the computer system so it is more efficient at running certain kinds of programs. A computer optimized for calculating missile trajectories, for example, won’t be optimized for running social media apps.",
        "cognitive_load_label": 2.1
    },
        {
        "text": "Many computing innovations were initially developed for military research and communication purposes, including the predecessor to the Internet, the ARPANET (Figure 1.3).",
        "cognitive_load_label": 2.9
    },
            {
        "text": "1.1.3 What Is a Computer? While computer science is about much more than just computers, it helps to know a bit more about computers because they are an important component of computer science. All computers are made of physical, real-world material that we refer to as hardware. Hardware—which has four components, including processor, memory, network, and storage—is the computer component that enables computations. The processor can be regarded as the computer’s “brain,” as it follows instructions from algorithms and processes data. The memory is a means of addressing information in a computer by storing it in consistent locations, while the network refers to the various technological devices that are connected and share information. The hardware and physical components of a computer that permanently house a computer’s data are called storage.",
        "cognitive_load_label": 3.3
    },
            {
        "text": "One way to understand computers is from a hardware perspective: computers leverage digital electronics and the physics of materials used to develop transistors. For example, many of today’s computers rely on the physical properties of a brittle, crystalline metalloid called silicon, which makes it suitable for representing information. The batteries that power many of today’s smartphones and mobile devices rely on lithium, a soft, silvery metal mostly harvested from minerals in Australia, Zimbabwe, and Brazil, as well as from continental brine deposits in Chile, Argentina, and Bolivia. Computer engineers combine these substances to build circuitry and information pathways at the microscopic scale to form the physical basis for modern computers.",
        "cognitive_load_label": 3.8
    },
            {
        "text": "However, the physical basis of computers was not always silicon. The Electronic Numerical Integrator and Computer (ENIAC) was completed in 1945, making it one of the earliest digital computers. The ENIAC operated on different physical principles. Instead of silicon, the ENIAC used the technology of a vacuum tube, a physical device like a light bulb that was used as memory in early digital computers. When the “light” in the vacuum tube is off, the vacuum tube represents the number 0. When the “light” is on, the vacuum tube represents the number 1. When thousands of vacuum tubes are combined in a logical way, we suddenly have memory. The ENIAC is notable in computer history because it was the first general-purpose computer, meaning that it could run not just a single program but rather any program specified by a programmer. The ENIAC was often run and programmed by women programmers (Figure 1.4). Despite its age and differences in hardware properties, it shares a fundamental and surprising similarity with modern computers. Anything that can be computed on today’s computers can also be computed by the ENIAC given the right circumstances—just trillions of times more slowly.",
        "cognitive_load_label": 2.6
    },
            {
        "text": "How is this possible? The algorithmic principles that determine how results are computed makes up software. Almost all computers, from the ENIAC to today’s computers, are considered Turing-complete (or Computationally Universal, as opposed to specialized computing devices such as scientific calculators) because they share the same fundamental model for computing results and every computer has the ability to run any algorithm. Alan Mathison Turing was an English mathematician who was highly influential in the development of theoretical computer science, which focuses on the mathematical processes behind software, and provided a formalization of the concepts of algorithm and computation with the Turing machine. A Turing-complete computer stores data in memory (either using vacuum tubes or silicon) and manipulates that data according to a computer program, which is an algorithm that can be run on a computer. These programs are represented using symbols and instructions written in a programming language consisting of symbols and instructions that can be interpreted by the computer. Programs are also stored in memory, which allows programmers to modify and improve programs by changing the instructions.",
        "cognitive_load_label": 3.0
    },
            {
        "text": "While both hardware and software are important to the practical operation of computers, computer science’s historical roots in mathematics also emphasize a third perspective. Whereas software focuses on the program details for solving problems with computers, theoretical computer science focuses on the mathematical processes behind software. The idea of Turing-completeness is a foundational concept in theoretical computer science, which considers how computers in general—not just the ENIAC or today’s computers, but even tomorrow’s computers that we haven’t yet invented—can solve problems. This theoretical perspective expands computer science knowledge by contributing ideas about (1) whether a problem can be computed by a Turing-complete computer at all, (2) how that problem might be computed using an algorithm, and (3) how quickly or efficiently a computer can run such an algorithm. The answers to these questions suggest the limits of what we can achieve with computers from a technical perspective: Using mathematical ideas, is it possible to use a computer to compute all problems? If the answer to a problem is yes, how much of a computing resource is needed to get the answer?",
        "cognitive_load_label": 2.9
    },
            {
        "text": "Clearly both humans and computers have their strengths and limitations. An example of a problem that humans can solve but computers struggle with is interpreting subtle emotions or making moral judgments in complex social situations. While computers can process data and recognize patterns, they cannot fully understand the nuances of human emotions or ethics, which often involve context, empathy, and experience. On the flip side, there are tasks that neither computers nor humans can perform, such as accurately predicting chaotic systems like long-term weather patterns. Despite advancements in artificial intelligence (AI), computer functions that perform tasks, such as visual perception and decision-making processes that usually are performed by human intelligence, these problems remain beyond our collective reach due to the inherent unpredictability and complexity of certain natural systems.",
        "cognitive_load_label": 2.7
    },
            {
        "text": "Theoretical computer science is often emphasized in undergraduate computer science programs because academic computer science emerged from mathematics, often to the detriment of perspectives that center on the social and technical values embodied by applications of computer technology. These perspectives, however, are gradually changing. Just as the design of ARPANET shaped the design of the Internet, computer scientists are also learning that the physical aspects of computer hardware determine what can be efficiently computed. For example, many of today’s artificial intelligence technologies rely on highly specialized computer hardware that is fundamentally different at the physical level compared to the general-purpose programmable silicon that has been the traditional focus of computer science. Organizations that develop human-computer interaction (HCI), a subfield of computer science that emphasizes the social aspects of computation, now host annual conferences that bring together thousands of researchers in academia and professionals in the industry. Computer science education is another subfield that emphasizes the cognitive, social, and communal aspects of learning computer science. Although these human-centered subfields are not yet in every computer science department, their increasing representation reflects computer scientists’ growing desire to serve not only more engaged students, but also a more engaged public in making sense of the values of computer technologies.",
        "cognitive_load_label": 2.9
    },
            {
        "text": "1.1.4The Capabilities and Limitations of Computer Science. Computers can be understood as sources, tools, and opportunities for changing social conditions. Many people have used computer science to achieve diverse goals beyond this dominant vision for computer science. For example, consider computers in education.",
        "cognitive_load_label": 2.0
    },
            {
        "text": "Around the same time that the ARPANET began development in the late 1960s, Wally Feurzeig, Seymour Papert, and Cynthia Solomon designed the LOGO programming language to enable new kinds of computer-mediated expression and communication. Compared to contemporary programming languages such as FORTRAN (FORmula TRANslation System) that emphasized computation toward scientific and engineering applications, LOGO is well known for its use of turtle graphics, whereby programs were used to control the actions of a digital turtle using instructions such as moving forward some number of units and turning left or right some number of degrees. Papert argued that this turtle programming enabled body-syntonic reasoning, a kind of experience that could help students more effectively learn concepts in mathematics such as angles, distance, and geometric shapes by instructing the turtle to draw them, and physics by constructing their own understandings via reasoning through the physical motion of turtle programs by showing concepts of velocity, repeated commands to move forward the same amount; acceleration, by making the turtle move forward in increasing amounts; and even friction, by having the turtle slow down by moving forward by decreasing amounts. In this way, computers could not only be used to further education in computer science, but also offer new, more dynamic ways to learn other subjects. Papert’s ideas have been expanded beyond the realm of mathematics and physics to areas such as the social sciences, where interactive data visualization can help students identify interesting correlations and patterns that precipitated social change and turning points in history while also learning new data fluencies and the limits of data-based approaches.4",
        "cognitive_load_label": 3.3
    },
            {
        "text": "Yet despite these roots in aspirations for computers as a medium for learning anything and everything, the study of computer science education emerged in the 1970s as a field narrowly concerned with producing more effective software engineers. Higher-education computer science faculty, motivated by the demand for software engineers, designed their computer science curricula to teach the concepts that early computer companies such as IBM desperately needed. These courses had an emphasis on efficiency, performance, and scalability, because a university computer science education was only intended to produce software engineers. We live with the consequences of this design even today: the structure of this textbook inherits the borders between concepts originally imagined in the 1970s when university computer science education was only intended to prepare students for software development jobs. We now know that there are many more roles for computer scientists to play in society—not only software engineers, but also data analysts, product managers, entrepreneurs, political advisors or politicians, environmental engineers, social activists, and scientists across every field from accounting to zoology.",
        "cognitive_load_label": 2.9
    },
            {
        "text": "Although the role of computers expanded with the introduction of the Internet in the late 1990s, Papert’s vision for computation as a learning medium has been challenging to implement, at least partly because of funding constraints. But as computers evolve, primary and secondary education in the United States is striving for ways to help teachers use computers to more effectively teach all things—not just computers for their own sake, but using computers to learn everything.",
        "cognitive_load_label": 2.7
    },
            {
        "text": "1.1.5 Computers and Racial Justice. Our histories so far have centered the interests of White American men in computer science. But there are also countless untold, marginalized histories of people of other backgrounds, races, ethnicities, and genders in computing. The book and movie Hidden Figures shares the stories of important Black women who were not only human computers, but also some of the first computer scientists for the early digital computers that powered human spaceflight at NASA (Figure 1.5).",
        "cognitive_load_label": 2.8
    },
                {
        "text": "In one chapter of Black Software, Charlton McIlwain shares stories from “The Vanguard” of Black men and women who made a mark on computer science in its early years from the 1950s through the 1990s through the rise of personal computing and the Internet, but whose histories have largely been erased by the dominant Silicon Valley narratives. Their accomplishments include leading computer stores and developing early Internet social media platforms, news, and blog websites. For example, Roy L. Clay Sr., a member of the Silicon Valley Engineering Hall of Fame, helped Hewlett-Packard develop its first computer lab and create the company’s first computers. Later, Clay provided information to venture capitalists that motivated them to invest in start-ups such as Intel and Compaq.5 In another example, Mark Dean was an engineer for IBM whose work was instrumental in helping IBM develop the Industry Standard Architecture (ISA) bus, which created a method of connecting a computer’s processor with other components and enabling them to communicate. This led to the creation of PCs, with Dean owning three of the nine patents used to create the original PC.6",
        "cognitive_load_label": 3.2
    },
                {
        "text": "Yet their efforts were often hampered by the way that computer science failed to center, or even accommodate, Black people. Historically, American Indians and Hispanic people did not have the same access as even Black Americans to computers and higher education. Kamal Al-Mansour, a technical contract negotiator at the NASA Jet Propulsion Lab, worked on space projects while Ronald Reagan was president. He recounts:“It was conflicting . . . doing a gig . . . supporting missiles in the sky, (while) trying to find my own identity and culture . . . JPL was somewhat hostile . . . and I would come home each day [thinking] What did I accomplish that benefited people like me? And the answer every day would be ‘Nothing.’”7",
        "cognitive_load_label": 2.7
    },
                {
        "text": "Al-Mansour would go on to start a new company, AfroLink, finding purpose in creating software that centered on Black and African history and culture. This story of computer technologies in service of African American communities is reflected in the creation of the Afronet (an early social media for connecting Black technologists) and the NetNoir (a website that sought to popularize Black culture). These examples serve as early indicators of the ways that Black technologists invented computer technologies for Black people in the United States. Yet Black Software also raises challenging political implications of the historical exclusion of Black technologists. Black culture on the Internet has greatly influenced mainstream media and culture in the United States, but these Black cultural products are ultimately driving attention and money to dominant platforms such as X and TikTok rather than those that directly benefit Black people, content creators, and entrepreneurs. Computer technologies risk reproducing social inequities through the ways in which they distribute benefits and harms.",
        "cognitive_load_label": 3.2
    },
                {
        "text": "The digital divide has emerged as a significant issue, as many aspects of society -- including education, employment, and social mobility -- become tied to computing, computer science, and connectivity. The divide refers to the uneven and unequal access and distribution of technology across populations from different geographies, socioeconomic statuses, races, ethnicities, and other differentiators. While technological access generally improves over time, communities within the United States and around the world have different levels of access to high-speed Internet, cell towers, and functioning school computers. Unreliable electricity can also play a significant role in computer and Internet usage. And beyond systemic infrastructure-based differences, individual product or service access can create a divide within communities. For example, if powerful AI-based search and optimization tools are only accessible through high-priced subscriptions, specific populations can be limited in benefiting from those tools.",
        "cognitive_load_label": 3.4
    },
                {
        "text": "1.1.6 Computers and Global Development.Computer technology, like any other cutting-edge technology, changes the balance of power in society. But access to new technologies is rarely ever equal. Computer science has improved the quality of life for many people who have access to computer technology and the means of controlling it to serve their interests. But for everyone else in the world, particularly people living in the Global South, computer technologies need context-sensitive designs to meet their needs. In the 1990s, for instance, consumer access to the Internet was primarily based on “dial-up” systems that ran on top of public telephone network systems. Yet many parts of the world, even today, lack telephone coverage, let alone Internet connectivity. Research in computers for global development aims to improve the quality of life for people all over the world by designing computer solutions for low-income and underserved populations across the world—not just those living in the wealthiest countries.",
        "cognitive_load_label": 2.9
    },
                {
        "text": "Computer technologies for global development require designing around unique resource constraints such as a lack of reliable power, limited or nonexistent Internet connectivity, and low literacy. Computer scientists employ a variety of methods drawing from the social sciences to produce effective solutions. However, designing for diverse communities is difficult, particularly when the designers have little direct experience with the people they wish to serve. In The Charisma Machine, Morgan Ames criticizes the One Laptop Per Child (OLPC) project, a nonprofit initiative announced in 2005 by the Massachusetts Institute of Technology Media Lab. The project attempted to bring computer technology in the form of small, sturdy, and cheap laptops that were powered by a hand crank to children in the Global South. Based on her fieldwork in Paraguay, Ames argues that the project failed to achieve its goals for a variety of reasons, such as electricity infrastructure problems, hardware reliability issues, software frustrations, and a lack of curricular materials. Ames argues that “charismatic technologies are deceptive: they make both technological adoption and social change appear straightforward instead of as a difficult process fraught with choices and politics.” When the computers did work, OLPC’s vision for education never truly materialized because children often used the computers for their own entertainment rather than the learning experiences the designers intended. Though Ames’s account of the OLPC project (Figure 1.6) itself has been criticized for presenting an oversimplified narrative, it still represents a valuable argument for the risks and potential pitfalls associated with designing technologies for global development: technology does not act on its own but is embedded in a complicated social context and history.",
        "cognitive_load_label": 3.5
    },
                {
        "text": "Addressing these risks is not as simple as practicing humility and including communities in the design process. Many challenges in computing for global development are sociopolitical or technopolitical rather than purely technical. For example, carrying out a pilot test to evaluate the effectiveness of a design can appear as favoritism toward the pilot group participants. These issues and social tensions are especially exacerbated in the Global South, where the legacies of imperialism and racial hierarchies continue to produce or expand social inequities and injustices.",
        "cognitive_load_label": 3.0
    },
                {
        "text": "The identities of people creating computer technologies for global development are ultimately just as important as the technologies they create. In Design Justice, Sasha Costanza-Chock reiterates the call for computer scientists to “build with, not for,” the communities they wish to improve. In this way, Design Justice seeks to address the social justice tensions raised when asking the question, “Who does technology ultimately benefit?” by centering the ingenuity of the marginalized “user” rather than the dominant “designer.”",
        "cognitive_load_label": 3.1
    },
                {
        "text": "In some cases, underdeveloped countries can quickly catch up without spending the money that was invested to develop the original technologies. For example, we can set up ad hoc networks quickly today and at a portion of the cost in Middle Eastern and African countries using technology that was developed (at a high cost) in the United States and Europe over the past several decades. This means that sometimes, progress in one part of the world can be shared with another part of the world, enabling that area to quickly progress and advance technologically.",
        "cognitive_load_label": 2.6
    },
                {
        "text": "Figure 1.2 An abacus is one of the first calculators. (credit: “Traditional Chinese abacus illustrating the suspended bead use” by Jccsvq/Wikimedia Commons, CC0)",
        "image_path":"/dataset/images/Figure 1.2.jpeg",
        "cognitive_load_label": 1.8
    },
                {
        "text": "Figure 1.3 The ARPANET, circa 1974, was an early predecessor to the Internet. It allowed computers at Pentagon-funded research facilities to communicate over phone lines. (credit: modification of Arpanet 1974 by Yngvar/Wikipedia, Public Domain)",
        "image_path":"/dataset/images/Figure 1.3.jpeg",
        "cognitive_load_label": 2.4
    },
                {
        "text": "Figure 1.4 This image depicts women programmers holding boards used in computers such as the ENIAC, many of which were designed expressly for ballistics and ordinance guidance research. Today, these room-size computers can be reproduced at a literally microscopic scale—basically invisible to the human eye. (credit: modification of Women holding parts of the first four Army computers by U.S. Army/Wikimedia Commons, Public Domain)",
        "image_path":"/dataset/images/Figure 1.4.jpeg",
        "cognitive_load_label": 3.0
    },
                {
        "text": "Figure 1.5 Katherine Johnson, a Black computer scientist, recalculated the computations done by early digital computers for space flight planning at NASA. Her contributions were portrayed in the book and movie Hidden Figures. (credit: “Katherine Johnson at NASA, in 1966” by NASA/Wikimedia Commons, Public Domain)",
        "image_path":"/dataset/images/Figure 1.5.jpeg",
        "cognitive_load_label": 2.7
    },
                {
        "text": "Figure 1.6 In 2005, MIT’s Media Lab started the OLPC initiative to bring laptops to children in the Global South. An unexpected outcome they discovered was that designing technologies for global communities is not as straightforward as designers may initially believe. (credit: One Laptop per Child by OLE Nepal Cover/Flickr, CC BY 2.0)",
        "image_path":"/dataset/images/Figure 1.6.jpeg",
        "cognitive_load_label": 1.9
    },
                {
        "text": "1.2  Computer Science across the Disciplines. Computer science is an incredibly diverse field not because of what it can achieve on its own but because of how it contributes to every other field of human knowledge and expertise. From its early days, it was understood that there would be cross-collaboration between computer scientists and colleagues in other disciplines. Today, almost all modern technologies either depend on computer technologies or benefit significantly from them. Computer technologies and the study of computer science have reshaped almost all facets of life today for everyone.",
        "cognitive_load_label": 2.4
    },
                {
        "text": "1.2.1 Data Science. Across business, financial, governmental, scientific, and nonprofit workplaces, millions of people are programming, and most of the time, they don’t even know it! A spreadsheet is an example of a data-centric programming environment where data is organized into cells in a table. Instead of presenting programs as primarily about algorithms, spreadsheets present programs as primarily about data. Spreadsheets are often used for data analysis by offering a way to organize, share, and communicate ideas about data. Spreadsheets are uniquely effective and accessible because they allow for the visual organization of data in whichever structure makes the most sense to the user. Instead of hiding data behind code, spreadsheets make data as transparent and up to date as possible.",
        "cognitive_load_label": 2.6
    },
                {
        "text": "Although spreadsheets make computation accessible for millions of people across the world, they have several shortcomings. Unless limits are removed, many popular spreadsheet software products such as Microsoft Excel may have a limitation to the number of rows of data they can store that is less than the data of modern computers. One such example occurred in October 2020 when Public Health England failed to report 15,841 positive cases of COVID-19 in the United Kingdom due to mismanaged row limits in the spreadsheet used. This shortcoming attests not only to the technical limit on the number of rows supported by spreadsheets, but also to the design limitations of software that fails to communicate data loss, irregularities, or errors to users. Errors in spreadsheet software data entry can often go unnoticed because spreadsheets do not enforce data types. Cells can contain any content: numbers, currencies, names, percentages, labels, and legends. The meaning of a cell is determined largely by the user rather than the software. Spreadsheets are an expressive and accessible technology for data analysis, but this creative power that spreadsheets afford to users is the very same power that limits spreadsheets as a data management and large-scale data analysis tool. The more data and the more people involved in a spreadsheet, the greater the potential for spreadsheet problems.",
        "cognitive_load_label": 2.8
    },
                {
        "text": "The interdisciplinary field that applies computing to managing data and extracting information from data is called data science. Data scientists are practitioners who combine computing and data analysis skills with the domain knowledge specific to their field or business. The demand for data scientists is becoming increasingly important as more and more research and business contexts involve analyzing big data, or very large datasets that are not easily processed using spreadsheets. These datasets often involve high-volume measurements of user interactions on the Internet at a very fine grain, such as tracking a customer’s web browser history across an online storefront. Data scientists can then analyze browser patterns using machine learning methods in order to recommend related products, target advertisements to specific customers over social media, and reengage customers over email or other means. Machine learning (ML) is a subset of artificial intelligence that relies on algorithms and data to make it possible for artificial intelligence to learn, actually mimicking the way humans learn. For example, ML is used to identify fraudulent versus legitimate banking transactions. Once a computer learns how to distinguish fraudulent transactions, it can be alert and call attention to suspicious banking activity.",
        "cognitive_load_label": 2.9
    },
                {
        "text": "1.2.2 Computational Science. Beyond data science, computer science can also fundamentally change how science is researched and developed. The field of computational science refers to the application of computing concepts and technologies to advance scientific research and practical applications of scientific knowledge in a wide range of fields, including civil engineering, finance, and medicine (among many others). For example, algorithms and computer software play a key role in enabling numerical weather prediction (Figure 1.7) or the use of mathematical models to forecast weather based on current conditions in order to assist peoples’ everyday lives and contribute to our understanding of the climate models, climate changes, and climate catastrophes. These algorithms may rely on a large amount of computer hardware power that might not be available in a single system, so the work may need to be distributed across many computers. Computational science studies methods for realizing these algorithms and computer software.",
        "cognitive_load_label": 3.1
    },
                {
        "text": "Although computer science has been used to support scientific discovery, the theory of knowledge of computer science has historically been considered quite different from that of the natural sciences, such as biology, physics, and chemistry. Computer science does not study natural objects, so to most, it would not be considered a natural science but rather an applied science. Unlike natural sciences such as biology, physics, and chemistry, which emphasize the discovery of natural phenomena, computer science often emphasizes invention or engineering. However, computer science is today deeply interdisciplinary and involves methods from across science, mathematics, and engineering. Computer scientists design, analyze, and evaluate computational structures, systems, and processes.",
        "cognitive_load_label": 2.8
    },
    {
          "structured_data": {
            "table_type": "theoretical_foundations",
            "columns": ["Dimension", "Principle", "Quantitative Metrics", "Calculation Method / Tools"],
            "rows": [
              {
                "dimension": "Mathematics",
                "principle": "Mathematics plays a key role in theoretical computer science, which emphasizes how a computational problem can be defined in mathematical terms and whether that mathematical problem can be efficiently solved with a computer.",
                "metrics": ["Computational Complexity", "Problem Solvability", "Algorithm Efficiency"],
                "calculation": {
                  "methods": [
                    {
                      "name": "Big O Notation",
                      "description": "Measures the efficiency of algorithms in terms of time and space complexity"
                    },
                    {
                      "name": "Decision Problem Analysis",
                      "description": "Determines whether a problem can be solved algorithmically within certain complexity classes (e.g., P vs NP)"
                    }
                  ],
                  "fusion_formula": "Efficiency = f(Time Complexity, Space Complexity), where f is a function evaluating scalability and performance"
                }
              },
              {
                "dimension": "Engineering",
                "principle": "Engineering plays a key role in software engineering, which emphasizes how problems can be solved with computers as well as the practices and processes that can help people design more effective software solutions.",
                "metrics": ["Software Maintainability", "System Reliability", "Development Efficiency"],
                "calculation": {
                  "tools": ["Code Quality Metrics (e.g., SonarQube)", "Failure Rate Testing", "Agile Performance Indicators"],
                  "description": "Use code review tools to assess maintainability, perform stress testing for reliability, and track sprint velocity for development efficiency"
                }
              },
              {
                "dimension": "Science",
                "principle": "Science plays a key role in human-computer interaction, which emphasizes experimentation and evaluation of the interface (boundary) between humans and computers, often toward designing better computer systems.",
                "metrics": ["User Satisfaction", "Task Success Rate", "Cognitive Load Measurement"],
                "calculation": {
                  "tools": ["Surveys (e.g., SUS)", "Eye Tracking", "EEG / Biometric Sensors"],
                  "description": "Measure user feedback via standardized questionnaires, analyze task completion rates through usability tests, and use biometric data to assess cognitive load"
                }
              }
            ]
          },
  "cognitive_load_label": 3.8
},

                {
        "text": "1.2.3 Information Science.Not only is computation interdisciplinary, but other disciplines are also becoming more and more computational. In The Invisible Future, Nobel Laureate biologist David Baltimore defines DNA in computational terms. He states that biology is an information science because DNA encodes for the outputs of biological systems. The interdisciplinary field studying information technologies and systems as they relate to people, organizations, and societies is called information science. The role of information in natural sciences can also be found in the physics of quantum waves that carry information about physical effects, in the chemical equations that specify information about chemical reactions, in the information flows that drive the evolution of economies and political organizations, and in the information processes underlying social, management, and communication sciences.13",
        "cognitive_load_label": 2.6
    },
                {
        "text": "Although information science has its roots in information classification, categorization, and management in the context of library systems, information science today is a broad field that encompasses the many diverse ways information shapes society. For example, today’s social media networks provide more personable and instantaneous information communication compared to traditional news outlets—billions of people around the world are using social media to engage with information about the world. For many people, social media may be the primary way that they learn about and make sense of the world (Figure 1.8). Yet, we’ve already seen risks associated with information technologies such as the Internet. In today’s “information age,” information has more power than ever before to reshape society. Information scientists, data scientists, computational scientists—and, therefore, computer scientists—have a social responsibility: “One cannot reap the reward when things go right but downplay the responsibility when things go wrong.”14",
        "cognitive_load_label": 3.0
    },
                {
        "text": "Despite the centrality of information to decision-making and social change, dominant approaches to computer science tend to focus on computational structures, systems, and processes (such as algorithms) that describe one kind of information by focusing on the what or how of solving problems with computers, but less often the why or who questions. Information science broadly centers people, organizations, and society in the study of information technologies.",
        "cognitive_load_label": 2.8
    },
                {
        "text": "1.2.4. Computer Science Is an Interdisciplinary Field. In presenting data science, computational science, and information science, we’ve introduced the idea that computer science can shape other disciplines. But we’ve also raised questions about what computer science is today. If computer science is the study of all “phenomena surrounding computers,” it could also involve data science, computational science, bioinformatics, cheminformatics, computational social science, medical informatics, and information science. As you will learn in Chapter 13 Hybrid Multicloud Digital Solutions Development, another aspect of computer science is responsible computing, which includes the appropriate management of cyber resources as well as robust cybersecurity. It is difficult to define computer science today because it is so widely used by people across the world in diverse capacities. Definitions are about defining boundaries and excluding practices, which may be helpful for understanding the practices of a certain culture or group that is “doing” computer science, but it can never truly represent everyone and all the things that people are doing with computer science. However, computer science’s historical roots in mathematics shape the way it categorizes subfields:",
        "cognitive_load_label": 3.0
    },
{

  "cognitive_load_label": 4.7,
  "structured_data": {
    "table_type": "computer_science_disciplines",
    "columns": ["Category", "Subfield", "Description", "Applications / Tools"],
    "rows": [
      {
        "category": "Theoretical Computer Science",
        "subfield": "Theory of Computation",
        "description": "Studies the fundamental capabilities and limitations of computation, including automata, computability, and complexity theory.",
        "applications": ["Algorithm design", "Formal verification", "Complexity analysis"]
      },
      {
        "category": "Theoretical Computer Science",
        "subfield": "Information Representation",
        "description": "Focuses on how data is encoded, stored, and interpreted in computer systems, including number systems, character encoding, and structured data formats.",
        "applications": ["Data compression", "Encoding standards (UTF-8, ASCII)", "Semantic data modeling"]
      },
      {
        "category": "Theoretical Computer Science",
        "subfield": "Data Structures and Algorithms",
        "description": "Analyzes efficient ways to organize data and perform operations on it, forming the backbone of software performance optimization.",
        "applications": ["Search engines", "Graph processing", "Optimization problems"]
      },
      {
        "category": "Theoretical Computer Science",
        "subfield": "Programming Language and Formal Methods",
        "description": "Involves the design and semantics of programming languages as well as mathematically rigorous techniques for proving correctness of software and hardware systems.",
        "applications": ["Compiler design", "Program verification", "Model checking"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Architecture",
        "description": "Concerned with the design and organization of computer hardware and how software interacts with it at a low level.",
        "applications": ["Processor design", "Memory hierarchy optimization", "Embedded systems"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Artificial Intelligence",
        "description": "Focuses on building systems that can perform tasks requiring human intelligence, such as learning, reasoning, problem-solving, and perception.",
        "applications": ["Machine learning", "Natural language processing", "Robotics"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Networks",
        "description": "Deals with communication between computers and devices, including protocols, routing, and security considerations.",
        "applications": ["Internet infrastructure", "Wireless communication", "Cloud networking"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Security",
        "description": "Protects information and systems from unauthorized access, use, disclosure, disruption, modification, or destruction.",
        "applications": ["Cryptography", "Penetration testing", "Secure software development"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Databases",
        "description": "Manages storage, retrieval, and querying of large volumes of structured data efficiently and securely.",
        "applications": ["Relational databases", "NoSQL systems", "Data warehousing"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Distributed Computing",
        "description": "Involves algorithms and systems that run on multiple interconnected computing nodes to solve large-scale problems.",
        "applications": ["Cloud computing", "Blockchain technology", "Parallel processing"]
      },
      {
        "category": "Computer Systems",
        "subfield": "Graphics",
        "description": "Focuses on rendering images, animations, and visual content using computers, combining mathematics, physics, and art.",
        "applications": ["3D modeling", "Game development", "Virtual reality"]
      },
      {
        "category": "Applied Computer Science",
        "subfield": "Scientific Computing",
        "description": "Applies computational techniques to solve complex problems in science and engineering through simulation and numerical methods.",
        "applications": ["Climate modeling", "Quantum mechanics simulations", "Bioinformatics"]
      },
      {
        "category": "Applied Computer Science",
        "subfield": "Human-Computer Interaction",
        "description": "Studies how people interact with computers and designs interfaces that are intuitive, efficient, and user-friendly.",
        "applications": ["User experience design", "Accessibility research", "Wearable technology"]
      },
      {
        "category": "Applied Computer Science",
        "subfield": "Software Engineering",
        "description": "Focuses on systematic and disciplined development, operation, and maintenance of software systems.",
        "applications": ["Agile methodologies", "DevOps practices", "Code quality assurance"]
      }
    ]
  }
},
                {
        "text": "In this hierarchy, theoretical computer science and computer systems are treated separately from applied computer science and human-computer interaction, suggesting that the mathematics of computing are pure and separate from social questions. Yet we’ve seen several examples that question this paradigm and instead point to a structure where human-computer interaction is infused throughout the study of computer science and all its subfields.",
        "cognitive_load_label": 2.7
    },
                {
        "text": "Today, computer science is a field that is just as much about people as it is about computer technology because each of these subfields is motivated by the real-world problems that people ultimately want to solve. The subfields of artificial intelligence and machine learning have applications that directly influence human decision-making, ranging from advertisement targeting to language translation to self-driving cars. Effective computational solutions to research or business problems require combining specific knowledge with computer science concepts from a combination of areas. For example, the computational science application of weather prediction combines knowledge about various subfields of computer science (algorithms, distributed computing, computer systems) with knowledge about climate systems. Theoretical computer scientists are increasingly interested in asking questions such as, “How do we design, analyze, and evaluate algorithms or information systems for fairness? How do we even define fairness in a computer system that strips away the complexities of the real world? What ideas or information are encoded in the data? And what are the limits of our approaches?” Computer science is a complex field, and its synergistic nature means that when computer science is used in an interdisciplinary manner that shapes other disciplines, its impact on society is much greater than when each discipline functions on its own.",
        "cognitive_load_label": 3.2
    },
                {
        "text": "Figure 1.7 Meteorologists collect data from a variety of sources and use the data, algorithms, and computers to predict the weather. (data source: Climate Forecast System, National Centers for Environmental Information, National Oceanic and Atmospheric Administration, https://www.ncei.noaa.gov/products/weather-climate-models/climate-forecast-system; credit: modification of CFSR Atmospheric Precipitable Water by NOAA/ncei.noaa.gov, Public Domain)",
        "image_path":"/dataset/images/Figure 1.7.jpge",
        "cognitive_load_label": 2.7
    },
                {
        "text": "Figure 1.8 While Americans used to primarily get their news from newspapers, as technology has advanced their primary source of news media has shifted. As of 2020, 53% of American adults surveyed stated they got their news from social media at least some of the time. (data source: Elisa Shearer and Amy Mitchell, Pew Research Center. About Half of Americans Get News on Social Media at Least Sometimes. From Survey of U.S. adults conducted Aug. 31–Sept. 7, 2020. In: E. Shearer, A. Mitchell, News Use Across Social Media Platforms in 2020, Jan 12, 2021.; attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 1.8.jpge",
        "cognitive_load_label": 2.4
    },
                {
        "text": "1.3 Computer Science and the Future of Society.As noted earlier, computer science is a powerful tool, and computer scientists have vast technological knowledge that continues transforming society. Computer scientists have an obligation to be ethical and good stewards of technology with an emphasis on responsible computing. Written code influences daily life, from what we see on social media to the news stories that pop up in a Google search and even who may or may not receive a job interview. When computer scientists don’t consider the ramifications of their code, there can be unintended consequences for people around the world. The Y2K problem, also known as the “millennium bug,” is a good example of shortsighted decisions that allowed computer scientists to only store the last two digits of the year instead of four. This made sense at a time when memory was expensive on both mainframe computers and early versions of personal computers. The Y2K problem was subsequently coined by John Hamre, the United States Deputy Secretary of Defense, as the “electronic equivalent of the El Niño.”15 The future of computer science will highly affect the future of the world. Although we often think of computer technologies as changing the way the world works, it is actually people and their vision for the future that are amplified by computing. The relationship between computer science and people is about how computer technologies can bias society and how the choices made through computer systems can both promote and discourage social inequities. Computer technologies can encode either value or both values in their designs. In this section, we’ll introduce three ways that computer science can shape the future of society: developing foundational technologies, evaluating negative consequences of technologies, and designing technologies for social good.",
        "cognitive_load_label": 3.1
    },
                {
        "text": "1.3.1 Developing Foundational Technologies.We’ve seen how foundational technologies like artificial intelligence, algorithms, and mathematical models enable important applications in data science, computational science, and information science.",
        "cognitive_load_label": 2.9
    },
                {
        "text": "As noted previously, artificial intelligence (AI) is the development of computer functions to perform tasks, such as visual perception and decision-making processes, that usually are performed by human intelligence. AI refers to a subfield of CS that is interested in solving problems that require the application of machine learning to human cognitive processes to achieve goals. AI research seeks to develop algorithm architectures that can make progress toward solving problems. One such example is image recognition, or the problem of identifying objects in an image. This problem is quite difficult for programmers to solve using traditional programming methods. Imagine having to define very precise rules or instructions that could identify an object in an image regardless of its position, size, lighting conditions, or perspective. As humans, we have an intuitive sense of the qualities of an object. However, representing this human intelligence in a machine requiring strict rules or instructions is a much harder task. AI methods for image recognition involve designing algorithm architectures that can generalize across all the possible ways that an object can appear in an image.",
        "cognitive_load_label": 2.6
    },
                {
        "text": "Recent approaches to AI for image recognition draw on a family of methods called neural networks instead of having programmers craft rules or instructions by hand to form an algorithm. In humans, the neural network is a complex network in the human brain that consists of neurons, or nerve cells, connected by synapses that send messages and electrical signals to all parts of the body, enabling us to think, move, feel, and function.",
        "cognitive_load_label": 2.6
    },
                {
        "text": "In computer science, a neural network (Figure 1.9) is an AI algorithm architecture that emphasizes connections between artificial nerve cells whose behavior and values change in response to stimulus or input. These neural networks are not defined by individual neurons but by the combination of all the neurons in the network. Typically, artificial neurons are arranged in a hierarchy that aims to capture the structure of an image. Although the first level of neurons might respond to individual pixels, later levels of artificial neurons might respond in aggregate to the arrangement of several artificial neurons in the preceding layer. This is similar to how the human visual system responds to edges at the lower levels, then responds in aggregate to the specific arrangement of several edges in later levels, and ultimately identifies these aggregated arrangements as objects.",
        "cognitive_load_label": 3.1
    },
                {
        "text": "The idea of neural networks, however, is not as new as it might seem. Artificial neural networks were first imagined in the mid-1900s alongside contemporary research efforts in the cognitive sciences. The ideas of multilayered, hierarchical networks of neurons and the mathematical optimization methods for learning were all there, but these early efforts were limited by the computational processing power available at the time. In addition, the large datasets that drive neural network learning were not nearly as available as they are today with the Internet. Developments in foundational technologies such as computer architecture and computer networks paved the way for the more recent developments in neural network technologies. The broad area of computer systems investigates these architectures and networks that enable new algorithms and software. Without these technologies, neural networks would not be nearly as popular and revolutionary as they are today. Yet the relationship between computer systems and AI development is not one-directional. Today, computer scientists are using neural networks to help design new, more efficient computer systems. The development of foundational computer technologies not only creates opportunities for direct and indirect applications, but also supports the development of other computer technologies.",
        "cognitive_load_label": 2.9
    },
                    {
        "text": "Just as we saw how technological fixes embodied a powerful belief about the relationship between computer solutions and social good, a similar cultural belief exists about the relationship between foundational technologies and their social values. The belief that technologies are inherently neutral and that it is the people using technology who ultimately make it “good” or “bad” is considered social determination of technology.",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "Today’s neural networks are designed to identify patterns and reproduce existing data. It is widely accepted that many big datasets can encode social preferences and values, particularly when the data is collected from users on the Internet. A social determination of technology accepts this explanation of AI bias and leaves the design of AI algorithms and techniques as neutral: the bias in an AI system is attributed to the social values of the data rather than the design of the AI algorithms. Critics of social determination point out that the way AI algorithms learn from big data represents a social value, one that encodes a default preference for reproducing the biases inherent in big data. This applies whether the AI application is about fair housing, medical imaging, ad targeting, drone strikes, or another topic. This is an issue that computer scientists must consider as they practice responsible computing and strive to ensure that data is gathered and handled as ethically as possible.",
        "cognitive_load_label": 2.7
    },
                    {
        "text": "1.3.2 Evaluating Negative Consequences of Technology. Today’s AI technologies work by reproducing existing patterns rather than imagining radically different futures. As much as neural networks are inspired by the human brain, it would be a stretch to suggest that AI systems have any semblance of general intelligence. Though these systems might be quite effective at identifying lettuce plants from weed plants in an image, their capacity for humanlike intelligence is limited by design. A neural network learns to recognize similar patterns that appear across millions or billions of sample images and represent these patterns with millions or billions of numbers. Mathematical optimization methods are used to choose the numeric values that best encode correlations across the sample images. However, current approaches lack a deeper, conceptual representation of objects. One criticism of very large neural networks is that there are often more numeric values than there are sample images—the network can effectively memorize the details of a million sample images by encoding them in a billion numbers. Many of today’s neural networks recognize objects in images not by relying on some intrinsic idea or concept of objects but by memorizing every single configuration of edges as they appear in the sample images.",
        "cognitive_load_label": 2.8
    },
                    {
        "text": "This limitation can lead to peculiar outcomes for image recognition systems. Often, neural network approaches for image recognition have certain examples of images where objects are misidentified in unusual ways: a person’s face might be recognized in a piece of toast or in a bunch of clouds in the sky. In these examples, the pattern of edges might coincidentally trigger the neural network values so that it misidentifies objects. These are among the more human-understandable examples; there are many other odd situations that are less explainable. An adversarial attack is a sample input (e.g., an image) that is designed to cause a system to behave problematically. Researchers have found that even tweaking the color of just a single point in an image can cause a chain reaction in the neural network, leading it to severely misidentify objects. The adversary can choose the color of the point in such a way as to almost entirely control the output of some neural networks: changing a single specific point in an image of a dog might cause the system to recognize the object as a car, airplane, human, or almost anything that the adversary so desires. Moreover, these adversarial attacks can often be engineered to cause the neural network to report extremely high confidence in its wrong answers. Self-driving cars that use neural networks for image recognition can be at risk of real-world adversarial attacks when specially designed stickers are placed on signs that cause the system to recognize a red light as a green light (Figure 1.10). By studying adversarial attacks, researchers can design neural networks that are more robust and resilient to these attacks.",
        "cognitive_load_label": 3.1
    },
                    {
        "text": "In general, research is an important part of computer science. Through research, computer scientists analyze ways that technology can be used and gain insight and answers to address issues and improve various aspects of society. Research enables computer scientists to make advancements like the design of new algorithms, development of new hardware and software, and applications for emerging technologies such as AI.",
        "cognitive_load_label": 2.6
    },
                    {
        "text": "One important use of research is to investigate adversarial attacks to gather answers needed for computer scientists to improve foundational technologies by evaluating the negative consequences of technologies. Computer technologies offer a unique medium for learning things (not just learning computer science), connecting with each other, and enhancing the lives of people all around the world. Yet, in each of these examples, we also raised concerns about how these technologies unfolded and affected people’s lives in both positive and negative ways. While we can rarely, if ever, paint any one technology as purely “good” or “bad,” computer scientists are interested in studying questions around how technologies are designed to center social values. Social scientists are not solely responsible for answering questions about technology, but computer scientists can also contribute important knowledge and methods toward understanding computer technologies.",
        "cognitive_load_label": 2.7
    },
                    {
        "text": "1.3.3 Designing Technologies for Social Good. Computer science can advance social good by benefiting many people in many different areas, including public health, agricultural sustainability, climate sustainability, and education.",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "Computer technologies accelerate medical treatments for public and personal health from initial research and development to clinical trials to large-scale production and distribution. In January 2020, Chinese officials posted the genetic sequence of the coronavirus SARS-CoV-2. This helped pharmaceutical companies to begin developing potential vaccines for the virus at a significantly faster rate than for any other virus in the past (Figure 1.11).",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "Computational science enables the miracles of modern medicine. Viral sequences can be digitized and rapidly shared between researchers across the world via the Internet. Computer algorithms and models can simulate the human immune system responses to particular treatments within hours rather than years. The first treatments can then be produced at a small scale using computer-engineered cells in less than a month from the initial sequencing. To ensure the treatments are safe and effective, clinical trials are held at disease transmission “hot spots” predicted using data science methods drawing on data aggregated and monitored from across the world. Once a treatment is proven safe and effective, it is mass-produced with the help of computer-controlled robots and automated assembly lines. Algorithms manage the inventory supply and demand and control the transportation of treatments on trucks and planes guided by computer navigation systems. Web apps and services notify people throughout the process.",
        "cognitive_load_label": 2.7
    },
                    {
        "text": "Yet the use of computer technology throughout modern medicine is anything but politically neutral. Computers, algorithms, and mathematical models solve the problems that their creators wish to solve and encode the assumptions of their target populations. Supply and demand data for the data models are determined by various factors, at least partly in response to the money and relationships between countries that control the technology, the Global North, and countries that don’t, the Global South. Within local communities, the uptake of medical treatments is often inequitable, reflecting and reinforcing historical inequities and disparities in public health. Computer technology alone often doesn’t address these issues. In fact, without people thinking about these issues, computer technologies can often amplify disparities. Consider datasets, which can be biased if they overrepresent or underrepresent specific groups of people. If decisions are made on the basis of biased data, people in the groups that are not represented fairly may receive inequitable treatment. For example, if a local government agency is working with a biased dataset, political leaders may make decisions that result in certain citizens receiving inadequate funding or services. This is an example of why responsible computing, which we will cover in Chapter 14 Cyber Resources Qualities and Cyber Computing Governance, is so important.",
        "cognitive_load_label": 2.5
    },
                    {
        "text": "These problematic histories are not only aggravated in medicine and public health, but also reflected in housing. Redlining refers to the inequitable access to basic public services based on residents’ neighborhoods and communities, which includes the practice of withholding financial services from areas with a large underrepresented population. In the United States, these communities reflect the histories of racial segregation and racial wealth inequalities. Fair housing laws are intended to prevent property owners from discriminating against buyers or renters because of race, color, ability, national origin, and other protected classes. But computer technologies also present new kinds of challenges. Microtargeted ads on social media platforms contribute to not only political polarization, but also discrimination in housing. This can be a particular problem when combined with redlining. Even if the ad targeting is not explicitly designed to discriminate, microtargeted ads can still reinforce historical redlining by incorporating data such as zip codes or neighborhoods. This may result in digital redlining, which is the practice of using technology, such as targeted ads, to promote discrimination. In 2021, a Facebook user filed a class-action lawsuit that argued nine companies in the Washington, D.C., area deliberately excluded people over the age of 50 from seeing their advertisements for housing because they wanted to attract younger people to live in their apartments.16 This is an example of an issue in technology that should be addressed by responsible computing with an emphasis on ethical behavior.",
        "cognitive_load_label": 2.7
    },
                    {
        "text": "With good intentions and attention to personal biases, technologies can be designed for social good. For example, a hypothetical algorithm for fair housing could evenly distribute new housing to people across protected classes and marginalized identities, such as older populations. Of course, algorithmic control and automated decision-making is challenged to consider the underlying conditions behind social problems. Still, algorithms can be important tools to enable us to distribute outcomes more fairly from a statistical perspective, and this can be an important step in addressing the larger societal systems and inequities that produce social problems.",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "As part of responsible computing, computer scientists must be aware of technological fix, which refers to the idea that technologies can solve social problems, but is now often used to critique blind faith in technological solutions to human problems. Unless the process is handled responsibly, the “fix” may cause more problems than it resolves. When considering how to address social and political problems, computer scientists must take care to ensure that they select the appropriate technology to address specific problems.",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "To address social problems and advance social good, recall that human-centered computing emphasizes people rather than technologies in the design of computer solutions. A human-centered approach to fair housing might begin by centering local communities directly affected by redlining. Rather than replacing or disrupting the people and organizations already working on a problem, a human-centered approach would center them in the design process as experts. A human-centered approach requires that the designer ask why they are not already working with people in the community impacted by their work.",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "Figure 1.9 (a) An artificial neural network consists of three key layers: the input layer, where raw data enters the system; the hidden layer, where information is processed and patterns are identified; and the output layer, where results are presented. (b) A natural neural network, such as those in the human body, mirrors this structure. The input layer represents sensory receptors, like those in the retina. The hidden layer corresponds to the synapse, where partial processing of the sensory data occurs. Finally, the output layer represents the information sent to the brain for final processing and interpretation. (credit a: modification of Neural network example by Wiso/Wikipedia, Public Domain; credit b: modification of work from Psychology 2e. attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure Figure 1.9.jpeg",
        "cognitive_load_label": 3.1
    },
                    {
        "text": "Figure 1.10 (a) Autopilot functions in self-driving cars generally identify roads and lanes using artificial intelligence to “see” road markings. (b) Researchers were able to trick these cars into seeing new lanes by using as few as three small stickers, to confuse the neural networks and force the cars to change lanes. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure Figure 1.10.jpeg",
        "cognitive_load_label": 2.9
    },
                    {
        "text": "Figure 1.11 The SARS-CoV-2 outbreak that began in 2020 displayed how quickly computer science could be harnessed by governments, medical facilities, and scientists to decode the virus, develop treatments, and distribute vaccinations around the world. What would have been a very difficult feat to manage manually was simplified through the use of algorithms and computer technology. (credit left: modification of COVID-19 vaccines by Agência Brasília/Flickr, CC BY 2.0; credit center: modification of T04 by Sarah Taylor/Flickr, CC BY 2.0; credit right: modification of “Back2School Brigade prepares families for upcoming school year” by Thomas Karol/DVIDS, Public Domain)",
        "image_path":"/dataset/images/Figure 1.11.jpeg",
        "cognitive_load_label": 2.8
    },
                    {

  "structured_data": {
    "table_type": "computer_science_summary",
    "columns": ["Dimension", "Principle", "Quantitative Metrics / Examples", "Calculation Method / Tools"],
    "rows": [
      {
        "dimension": "Scope and Impact",
        "principle": "Computer science is pervasive in daily life, business, scientific research, and social change.",
        "metrics": ["Daily use cases", "Industrial applications", "Research advancements", "Social impact initiatives"],
        "calculation": {
          "tools": ["Case studies", "Usage statistics", "Impact reports"],
          "description": "Assessing the breadth of computer science through its integration into various domains."
        }
      },
      {
        "dimension": "Definition and Foundations",
        "principle": "Computer science is the study of computing, including all phenomena related to computers and algorithms.",
        "metrics": ["Algorithm complexity", "Computational theory", "Hardware/software interaction"],
        "calculation": {
          "tools": ["Big O Notation", "Turing Machines", "Formal Language Theory"],
          "description": "Analyzing theoretical and practical foundations of computing systems."
        }
      },
      {
        "dimension": "Perspectives",
        "principle": "Three major perspectives on computation: hardware, software, and theoretical. Each emphasizes different aspects of computing.",
        "metrics": ["Hardware efficiency", "Software design principles", "Theoretical completeness"],
        "calculation": {
          "tools": ["Benchmarking", "Code reviews", "Complexity analysis"],
          "description": "Evaluating computing from multiple angles for a comprehensive understanding."
        }
      },
      {
        "dimension": "Historical and Social Vision",
        "principle": "Beyond commercial or military goals, computing was envisioned as a tool for education, anti-racism, and global development.",
        "metrics": ["Educational access", "Equity in tech", "Global digital literacy rates"],
        "calculation": {
          "tools": ["Surveys", "Policy analysis", "UNDP reports"],
          "description": "Measuring progress toward inclusive and socially beneficial computing."
        }
      }
    ]
  },
        "cognitive_load_label": 3.6
    },
                    {
    
  "structured_data": {
    "table_type": "computer_science_across_disciplines",
    "columns": ["Discipline", "Definition", "Focus", "Relationship with Computer Science"],
    "rows": [
      {
        "discipline": "Computer Science",
        "definition": "The study of computing, algorithms, and computational systems.",
        "focus": "Developing new computing methods and understanding computation theoretically and practically.",
        "relationship_with_computer_science": "Core field; foundational to all other disciplines."
      },
      {
        "discipline": "Data Science",
        "definition": "An interdisciplinary field that applies computing to manage data and extract information.",
        "focus": "Analyzing and interpreting complex data sets, especially large-scale (big data).",
        "relationship_with_computer_science": "Relies heavily on computer science for tools like databases, machine learning, and data processing frameworks."
      },
      {
        "discipline": "Computational Science",
        "definition": "The application of computing concepts to advance scientific research and practical applications of science knowledge.",
        "focus": "Using simulations, models, and algorithms to solve scientific problems.",
        "relationship_with_computer_science": "Leverages computer science methodologies (e.g., numerical algorithms, parallel computing) to enable scientific discovery."
      },
      {
        "discipline": "Information Science",
        "definition": "An interdisciplinary field studying information technologies and systems in relation to people, organizations, and societies.",
        "focus": "Understanding how information is created, used, and managed within social contexts.",
        "relationship_with_computer_science": "Shares overlap with computer science in areas like data management and human-computer interaction, but emphasizes the societal impact of information rather than technical computation."
      }
    ]
  }
,
        "cognitive_load_label": 3.6
    },
                    {
        
  "structured_data": {
    "table_type": "computer_science_and_future_society",
    "columns": ["Perspective", "Description", "Example", "Challenge / Consideration"],
    "rows": [
      {
        "perspective": "Developing Foundational Technologies",
        "description": "Computer science drives innovation by creating new technologies that enable future applications and industries.",
        "example": "Rapid development of artificial intelligence, particularly neural networks, enabling image recognition and other AI applications.",
        "challenge_or_consideration": "Technologies often rely on advancements in other areas (e.g., computer architecture, networking) and may encode biases from training data."
      },
      {
        "perspective": "Evaluating Negative Consequences",
        "description": "Computer science assesses the limitations and risks of emerging technologies to ensure safety and ethical use.",
        "example": "Research into adversarial attacks on neural networks helps build more robust and secure AI systems.",
        "challenge_or_consideration": "Understanding and mitigating unintended consequences requires interdisciplinary collaboration and long-term foresight."
      },
      {
        "perspective": "Designing Technologies for Social Good",
        "description": "Computer science contributes to solving societal challenges through responsible design and application of technology.",
        "example": "Supporting the full lifecycle of modern medicine, including research, development, logistics, and delivery.",
        "challenge_or_consideration": "Even well-intentioned technologies can cause harm if they fail to center human values or consider diverse user needs."
      }
    ]
  }
,
        "cognitive_load_label": 3.3
    },
                    {
        "text": "2. Computational Thinking and Design Reusability.2.1 Computational Thinking. 2.2 Architecting Solutions with Adaptive Design Reuse in Mind. 2.3 Evolving Architectures into Useable Products. ",
        "cognitive_load_label": 2.4
    },
                    {
        "text": "In the rapidly evolving landscape of technology and innovation in various domains, computational thinking promotes design reusability and is a fundamental skill set essential for problem-solving. This chapter illustrates how computational thinking, through practical insights and theoretical frameworks, facilitates the creation of reusable designs that improve the qualities (e.g., scalability, efficiency) of solutions.",
        "cognitive_load_label": 3.6
    },
                    {
        "text": "Developing innovative solutions to business problems today involves reinventing, rethinking, and rewiring existing business solutions and leveraging the latest technologies to assemble competitive products that solve business problems. It is key to apply computational thinking throughout this process to tackle new problems in specific areas. Computational thinking is a problem-solving and cognitive process rooted in principles derived from computer science. It involves breaking down complex problems into smaller, more manageable parts and devising systematic approaches to solve them.",
        "cognitive_load_label": 3.8
    },
                    {
        "text": "Adaptive design reuse also makes it possible to quickly assemble business solutions by assembling existing design building blocks that require minimal customizations to be a good match for the problem at hand. TechWorks is a company focused on developing innovative products and services. TechWorks is looking to take advantage of computational thinking and adaptive design reuse to enable the creation of next-generation, secure, super society, intelligent, autonomous business solutions. At TechWorks, a skilled team of engineers, data scientists, and designers is on a mission to revolutionize society. Their goal is to create advanced and secure autonomous business solutions. The team believes in the power of computational thinking and adaptive design reuse for success.",
        "cognitive_load_label": 4.4
    },
                    {
        "text": "Led by the CIO, the team gathered in a cutting-edge laboratory to tackle challenges in transportation, security, and automation. They embraced computational thinking by applying algorithms and machine learning to analyze data. Recognizing the efficiency of adaptive design reuse, the team explored successful projects like robotics and self-driving cars for inspiration. These projects have become the foundation for their own innovation. With minimal adjustments, they seamlessly integrate these building blocks into comprehensive solutions such as self-driven cars that can smoothly navigate the city, drones that can monitor public spaces, and robotics that automate tasks. The company plans to bring their vision of the future to life by transforming cities into hubs of interconnected, intelligent systems. Knowing that innovation is a continuous process that requires rapidly evolving solutions, the team faced challenges while implementing their initial prototype. However, they are able to adapt their super society solutions using computational thinking and adaptive design reuse to ensure that they stay ahead of technological advancements. TechWorks is a symbol of the successful integration of forward-thinking strategies to create secure and technologically advanced super society solutions.",
        "cognitive_load_label": 4.7
    },
                    {
        "text": "2.1 Computational Thinking.This chapter presents key aspects of computational thinking, including logical thinking, assessment, decomposition, pattern recognition, abstraction, generalization, componentization, and automation. These elements guide how computer scientists approach problems and create well-designed solution building blocks at both the business and technical levels. Computational thinking often involves a bottom-up approach, focusing on computing in smaller contexts, and seeks to generate innovative solutions by utilizing data structures and algorithms. Additionally, it may make use of existing design building blocks like design patterns and abstract data types to expedite the development of optimal combinations of data structures and algorithms.",
        "cognitive_load_label": 4.2
    },
                        {
        "text": "2.1.1. What Is Computational Thinking? The problem-solving and cognitive process, known as computational thinking, is rooted in principles derived from computer science. Be sure to retain key word tagging on computational thinking when sentence is revised. It involves breaking down complex problems into smaller, more manageable parts and devising systematic approaches to solve them. Complex problems are situations that are difficult because they involve many different interrelated parts or factors. These problems can be hard to understand and often don’t have simple solutions. While “computational thinking” is still perceived by some as an abstract concept without a universally accepted definition, its core value is to facilitate the application of separate strategies and tools to address complex problems. In problem-solving, computers play a central role, but their effectiveness centers on a prior comprehension of the problem and its potential solutions. Computational thinking serves as the bridge between the problem and its resolution. It empowers solution designers to navigate the complexity of a given problem, separate its components, and formulate possible solutions. These solutions, once developed, can be communicated in a manner that is comprehensible to both computers and humans, adopting effective problem-solving.",
        "cognitive_load_label": 4.0 
    },
                        {
        "text": "2.1.2 Vision. To further qualify computational thinking, Al Aho of the Columbia University Computer Science Department describes computational thinking as “the thought processes involved in formulating problems so their solutions can be represented as computational steps and algorithms.” Jeannette Wing, also of Columbia University, brought the idea of computational thinking to prominence in a paper she wrote in 2006 while at Carnegie Mellon University. She believes that computational thinking details the mental acts needed to compute a solution to a problem either by human actions or machine. Computational thinking encompasses a collection of methods and approaches for resolving (and acquiring the skills to resolve) complex challenges, closely aligned with mathematical thinking through its utilization of abstraction, generalization, modeling, and measurement (Figure 2.2). However, it differentiates itself by being more definitely aware than mathematics alone in its capacity for computation and the potential advantages it offers.",
        "image_path":"/dataset/images/Figure 2.2.jpeg",
        "cognitive_load_label": 3.7
    },
                        {
        "text": "Critical thinking is an important skill that can help with computational thinking. It boils down to understanding concepts rather than just mastering technical details for using software, prioritizing comprehension over rote learning. It’s a core skill, not an extra burden on a curriculum checklist, and it uniquely involves humans, not computers, blending problem-solving and critical thinking. Critical thinking focuses on ideas, not tangible items, applying advanced thinking to devise solutions. Critical thinking is essential for everyone and is, comparable to foundational abilities like reading, writing, and arithmetic.",
        "cognitive_load_label": 3.6
    },
                        {
        "text": "2.1.3 Computational Thinking Concepts. The description provided by the International Society for Technology in Education (ISTE) outlines the key components and dispositions of computational thinking. Let’s explore each characteristic in more detail:",
        "cognitive_load_label": 3.1
    },
   {
 "cognitive_load_label": 2.9,
  "structured_data": {
    "table_type": "computational_thinking_components",
    "columns": ["Component", "Definition", "Purpose / Function", "Example / Method"],
    "rows": [
      {
        "component": "Decomposition",
        "definition": "The analytical process of breaking down complex concepts or problems into smaller parts.",
        "purpose_or_function": "This approach helps analyze and solve problems more effectively.",
        "example_or_method": "Breaking down a large software project into modules for easier development and testing."
      },
      {
        "component": "Pattern Recognition",
        "definition": "Logically organizing and analyzing data to identify patterns and trends.",
        "purpose_or_function": "Facilitates understanding of relationships within the data and supports prediction.",
        "example_or_method": "Identifying recurring sequences in time-series data or recognizing image features in computer vision."
      },
      {
        "component": "Abstraction",
        "definition": "Representing data through an abstraction, such as a simulation or model.",
        "purpose_or_function": "Simplifies complex systems or phenomena to focus on relevant details and ignore unnecessary complexity.",
        "example_or_method": "Using a simplified climate model to simulate global warming effects without modeling every atmospheric particle."
      },
      {
        "component": "Algorithmic Thinking",
        "definition": "Designing step-by-step instructions or rules for solving problems or performing tasks.",
        "purpose_or_function": "Enables automation and efficient execution of repetitive or complex tasks.",
        "example_or_method": "Writing code to automate data cleaning, sorting, or calculations using loops and conditionals."
      },
      {
        "component": "Problem Solving (Identification, Analysis, Implementation)",
        "definition": "The process of identifying, analyzing, and implementing potential solutions to achieve optimal efficiency and effectiveness.",
        "purpose_or_function": "To develop robust strategies that can be executed systematically and computationally.",
        "example_or_method": "Developing an AI-based recommendation system by identifying user behavior patterns and building algorithms to predict preferences."
      },
      {
        "component": "Generalization and Transferability",
        "definition": "Applying problem-solving methods across different domains and contexts.",
        "purpose_or_function": "Demonstrates the adaptability and broad applicability of computational thinking.",
        "example_or_method": "Applying search algorithms developed for chess engines to route-finding applications like GPS navigation."
      }
    ]
  }
    },
                        {
        "text": "These abilities are supported and enriched by fundamental abilities integral to computational thinking. These abilities involve the following characteristics: confidence in navigating complexity, resilience in tackling challenging problems, an acceptance of ambiguity, adeptness in addressing open-ended issues, and proficiency in collaborating with others to attain shared objectives or solutions. Another illustration of computational thinking is the three As, which is organized into three phases, as visualized in Figure 2.3: Abstraction: The initial step involves problem formulation. Automation: Next, the focus shifts to expressing the solution. Analysis: Finally, the process encompasses solution execution and evaluation.",
        "cognitive_load_label": 3.6
    },
                        {
        "text": "2.1.4 Computational Thinking Techniques. In today’s technology world, mastering computational thinking techniques is important. These techniques offer a systematic way to solve problems using tools like data structures, which are like containers used to organize and store data efficiently in a computer. They define how data is logically arranged and manipulated, making it easier to access and work with information in algorithms and programs. There are four key techniques (cornerstones) to computational thinking, as illustrated in Figure 2.4:",
        "cognitive_load_label": 3.4
    },
                        {
        "text": "Decomposition is a fundamental concept in computational thinking, representing the process of systematically breaking down a complex problem or system into smaller, more manageable parts or subproblems. By breaking down complexity into simpler elements, decomposition promotes a more organized approach to problem-solving. Logical thinking and pattern recognition is a computational thinking technique that involves the process of identifying similarities among and within problems. This computational thinking technique emphasizes the ability to recognize recurring structures, relationships, or sequences in various problem-solving situations. Abstraction is a computational thinking technique that centers on focusing on important information while ignoring irrelevant details. This technique enables a clearer understanding of the core issues. Algorithms are like detailed sets of instructions for solving a problem step-by-step. They help break down complex tasks into manageable actions, ensuring a clear path to problem-solving.Figure 2.4 Users can explore the essence of computational thinking through decomposition, logical thinking, abstraction, and algorithms. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.4.jpeg",
        "cognitive_load_label": 4.6
    },
                        {
        "text": "In addition to the four techniques, computational thinking involves essential steps such as testing and debugging. Testing is crucial for uncovering errors within the step-by-step instructions or algorithms employed to tackle a problem. On the other hand, debugging entails identifying and rectifying issues within the code.",
        "cognitive_load_label": 4.6
    },
                        {
        "text": "A programmer is someone who writes instructions for a computer to follow. A typical example is that of a programmer who gives instructions to a robot and tells it to make a jam sandwich. In this case, applying computational techniques to give instructions to the robot entails the following techniques: decomposition, logical thinking and pattern recognition, abstraction, and algorithms. These techniques are explained in the following subsections as they apply to the jam sandwich example.",
        "cognitive_load_label": 3.6
    },
                        {
        "text": "2.1.5. Logical Thinking and Pattern Recognition. Pattern recognition makes it possible to group all the different features considered in decomposition into categories. In the jam sandwich example, pattern recognition leads to grouping the various things identified via decomposition into categories, in this case, ingredients, equipment, and actions. Therefore, applying decomposition and pattern recognition will lead to thinking of as many things as possible that are required to make a jam sandwich. The more things that can be thought of (i.e., ingredients, equipment, and actions), the clearer the instructions will be. A first attempt at decomposition and pattern recognition is summarized in Table 2.1.",
        "cognitive_load_label": 4.8
    },
                        {
        "text": "Table 2.1 Logical Thinking and Pattern Recognition Example The jam sandwich pattern recognition defines the ingredients, equipment, and actions needed for completion.",
        "table": {
            "header": ["Ingredients", "Equipment", "Actions"],
            "rows": [
              ["Bread", "Plate", "Repeat x times"],
              ["Jam", "Knife", "Left hand (LH)"],
              ["Butter", "", "Right hand (RH)"],
              ["", "", "Pick up"],
              ["", "", "Unscrew"]
            ]
          }
,
        "cognitive_load_label": 4.7
    },

   {
        "text": "Figure 2.3 The three As—abstraction, automation, analysis—illustrate the power of computational thinking. (credit photo: modification of “Avalanche on Everest” by Chagai/Wikimedia Commons, Public Domain; credit graph: modification of “Slope stability calculation for a model landslide” by B. Terhost and Bodo Damm/Journal of Geological Research, CC BY)",
        "image_path":"/dataset/images/Figure 2.3.jpeg",
        "cognitive_load_label": 4
    },
    {
        "text": "2.5 Abstraction. Abstraction makes it possible to pull out the important details and identify principles that apply to other problems or situations. When applying abstraction, it may be useful to write down some notes or draw diagrams to help understand how to resolve the problem. In the jam sandwich example, abstraction means forming an idea of what the sandwich should look like. To apply abstraction here, you would create a model or draw a picture representing the final appearance of the jam sandwich once it is made. This simplifies the details, providing a clearer image of the desired outcome. Simple tools like the Windows Paint program can be used to do this, as shown in Figure 2.5.Figure 2.5 This jam sandwich abstraction example illustrates what the final product should look like. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
       "image_path":"/dataset/images/Figure 2.5.jpeg",
        "cognitive_load_label": 3.5
    },
                        {
        "text": "In technology, data are represented at different levels of abstraction to simplify user interaction and manage complex operations efficiently. Users interact with a web application through a straightforward interface, like requesting help from a GenAI tool, without seeing the underlying complexity. This GenAI prompt is then processed by the application’s logic, which validates and directs it appropriately, often invisibly to the user. Finally, at the back end, the prompt is processed and a GenAI-generated response is provided. Each layer of abstraction serves a separate role, making the entire process efficient for both the user and the system (Figure 2.6).Figure 2.6 When using GenAI, a user interacts with the interface while the application processes the prompt with layers of abstraction on the back end. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.6.jpeg",
        "cognitive_load_label": 4
    },
                        {
        "text": "2.6 Algorithm. An algorithm is a sequence of steps/instructions that must be followed in a specific order to solve a problem. Algorithms make it possible to describe a solution to a problem by writing down the instructions that are required to solve the problem. Computer programs typically execute algorithms to perform certain tasks. In the jam sandwich example, the algorithm technique is about writing instructions that the robot can follow to make the jam sandwich. As you will learn in Chapter 3 Data Structures and Algorithms, algorithms are most commonly written as either pseudocode or a flowchart. An outline of the logic of algorithms using a combination of language and high-level programming concepts is called pseudocode. Each step is shown in a clearly ordered, written structure. A flowchart clearly shows the flow and direction of decisions in a visual way using a diagram. Either way is fine, and it is a matter of personal preference. Basic templates for the flowchart and pseudocode are in Figure 2.7.Figure 2.7 Pseudocode lists each step, while a flowchart visually outlines the process of decision-making. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license) ",
        "image_path":"/dataset/images/Figure 2.7.jpeg",
        "cognitive_load_label": 3.9
        },                    
                        {
        "text": "Writing algorithms requires practice. Not everyone likes butter in their jam sandwich. The robot needs a method of making sure it adds or does not add butter, depending on preferences. It is therefore necessary to account for the following steps in the pseudocode and flowchart: 1. Ask whether there should be butter on the bread. 2. Either spread butter on the bread,3. Or, do not use butter.",
        "cognitive_load_label": 2.8
    },
                        {
        "text": "These steps can be added as actions in the table previously shown and expressed as steps in the pseudocode using programming keywords such as INPUT, OUTPUT, IF, THEN, ELSE, and START. The corresponding instructions can then be converted into a flowchart using the symbols in Figure 2.8.Figure 2.8 The symbols used in a flowchart are associated with their instructions. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.7.jpeg",
        "cognitive_load_label": 5.3
    },
                        {
        "text": "2.7 Algorithm Execution Model Patterns. Various patterns of execution models may be used to step through the instructions provided in an algorithm. So far, we have only considered the traditional sequential (i.e., step-by-step) execution model for algorithm instructions. However, it is also possible to leverage parallelism/concurrency and recursion as alternative models to drive the execution of algorithms’ instructions.",
        "cognitive_load_label": 1
    },
                        {
        "text": "Parallel/concurrent execution models are typically used to optimize algorithm execution efficiency. As an example, if you and a friend are buying tickets for a movie and there are three independent lines, you may opt for a parallel processing model of execution by having you and your friend join two separate lines to buy the tickets. In that case, you are guaranteed to be able to obtain the tickets quicker assuming one of the lines operating in parallel with the other ends up serving customers faster, which is most often the case. Note that executing the same algorithm simultaneously on a computer may not be possible if you only have one central processing unit (CPU) in your machine. In that case, you can simulate parallelism by having the operating system running on the machine execute the two algorithms concurrently as separate tasks while sharing the single processor resources. This approach is less efficient than true parallelism. More detail on the differences between concurrency and parallelism will be provided in Chapter 4 Linguistic Realization of Algorithms: Low-Level Programming Languages.",
        "cognitive_load_label": 1
    },
                        {
        "text": "Recursive models of execution provide another elegant and effective alternative to the traditional sequential model of execution. The problem-solving technique where a process calls itself in order to solve smaller instances of the same problem is called recursion. It can be a powerful tool in programming because it allows for elegant solutions to complex problems by breaking them down into smaller, more manageable parts. By leveraging recursion, programmers can write concise and efficient code to solve a wide range of problems.",
        "cognitive_load_label": 1
    },
{
 "cognitive_load_label": 3.4,
  "text": "One of the key advantages of recursion is its ability to handle complex tasks with minimal code. Instead of writing lengthy iterative loops to solve repetitive tasks, recursion allows programmers to define a process that calls itself with modified input parameters, effectively reducing the amount of code needed. However, it’s essential to be cautious when using recursion, as improper implementation can lead to stack overflow errors due to excessive process calls. Programmers should ensure that recursive processes have proper base cases to terminate the recursion and avoid infinite loops.",
  "code_example": {
    "language": "C++",
    "code": "#include <iostream>\nusing namespace std;\n\nint recursiveSum (int x) {\n  // Base case\n  if (x == 0) {\n    return 0;\n  } else {\n    // Recursive step\n    return x + recursiveSum (x - 1);\n  }\n}\nint main() {\n  cout << recursiveSum (10);\n  // Answer is 55\n  return 0;\n}"
  }   
    },
{
  "cognitive_load_label": 5.2,
  "text": "In practical scenarios, recursion often manifests as a self-referential function, a structured set of commands that can be repeatedly executed with varying inputs and outputs. The base case represents the simplest computational operation for a given input, serving as a termination condition to prevent infinite recursion. To implement recursion effectively, programmers must adhere to two critical steps: (a) precisely identify the base case, which defines the boundary condition, and (b) meticulously outline the recursive steps that reduce the problem into smaller subproblems. In the context of a recursive function calculating a cumulative sum, when the input n equals 0, the sum from 0 to 0 is intuitively 0, forming the fundamental subproblem. This base case enables the construction of the function's initial framework. Additionally, recursion efficiency can be analyzed using time complexity (O(n)) and space complexity (O(n)) due to the call stack. Consider the following table of recursive sum examples:",
  "table": {
    "headers": ["Input (n)", "Base Case", "Recursive Steps", "Output"],
    "rows": [
      [0, "n == 0", "N/A", "0"],
      [5, "n == 0", "5 + recursiveSum(4)", "15"],
      [10, "n == 0", "10 + recursiveSum(9)", "55"]
    ]
  },
  "code_example": {
    "language": "C++",
    "code": "#include <iostream>\nusing namespace std;\n\n// Function to compute recursive sum with input validation\nint recursiveSum(int x) {\n  // Base case: terminate when x reaches 0\n  if (x < 0) {\n    throw runtime_error(\"Input must be non-negative\");\n  }\n  if (x == 0) {\n    return 0;\n  }\n  // Recursive step: add current value and recurse with decremented input\n  return x + recursiveSum(x - 1);\n}\n\nint main() {\n  try {\n    cout << \"Sum from 0 to 10: \" << recursiveSum(10) << endl; // Output: 55\n    cout << \"Sum from 0 to -5: \" << recursiveSum(-5) << endl; // Throws error\n  } catch (const runtime_error& e) {\n    cout << \"Error: \" << e.what() << endl;\n  }\n  return 0;\n}"
  }
},
{
  "cognitive_load_label": 5.5,
  "text": "Recursion operates through a process of simplification, progressively reducing the value of x until it reaches the base condition where x equals 0. This technique provides an alternative to iterative methods, offering a refined and effective algorithmic solution by breaking down problems into manageable subproblems. The efficiency of recursion can be enhanced by analyzing its time complexity (O(n)) and space complexity (O(n)) due to stack usage, with potential optimizations like tail recursion or memoization for larger datasets. In mathematical terms, the recursive sum can be expressed as \\( S(n) = n + S(n-1) \\) with \\( S(0) = 0 \\). Consider the following table of recursive sum evaluations:",
  "table": {
    "headers": ["Input (n)", "Recursive Expression", "Stack Depth", "Output"],
    "rows": [
      [0, "S(0)", "1", "0"],
      [5, "S(5) = 5 + S(4)", "6", "15"],
      [10, "S(10) = 10 + S(9)", "11", "55"]
    ]
  },
  "code_example": {
    "language": "C++",
    "code": "#include <iostream>\nusing namespace std;\n\n// Optimized recursive sum with tail recursion comment\nint recursiveSum(int x, int acc = 0) {\n  // Base case: return accumulated sum when x reaches 0\n  if (x < 0) {\n    throw runtime_error(\"Input must be non-negative\");\n  }\n  if (x == 0) {\n    return acc;\n  }\n  // Recursive step: accumulate and reduce x\n  // Note: Tail recursion optimization possible with compiler support\n  return recursiveSum(x - 1, acc + x);\n}\n\nint main() {\n  try {\n    cout << \"Sum from 0 to 10: \" << recursiveSum(10) << endl; // Output: 55\n    cout << \"Sum from 0 to 5: \" << recursiveSum(5) << endl;  // Output: 15\n  } catch (const runtime_error& e) {\n    cout << \"Error: \" << e.what() << endl;\n  }\n  return 0;\n}"
  }
},

                        {
        "text": "While it looks like recursion amounts to calling the same function repeatedly, it is only partially true, and you should not think about it that way. What happens is much more than repeating the call of a function. It is more useful to think of it as a chain of deferred operations. These deferred operations are not visible in your code or your output—they are in memory. The program needs to hold them somehow, to be able to execute them at the end. In fact, if you had not specified the base case, the recursive process would never end. Figure 2.9 illustrates a flowchart for an iterative solution that adds N numbers.Figure 2.9 A flowchart represents an iterative solution for adding N numbers. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
         "image_path":"/dataset/images/Figure 2.9.jpeg",
        "cognitive_load_label": 3.5
    },
                        {
        "text": "2.2.1 Layering and Componentization.A monolithic structure is a type of system or application where all the parts are closely combined into one single unit. This setup means that everything from the user interface to data handling and processing is interconnected within one big software application. While this can make the system easier to set up and manage initially, it also means that changing one part can require changes throughout the whole system, making it less flexible. As illustrated in Figure 2.10, there are many more recent variations of layered architectures that also provide complementary capabilities, such as tiered architectures, service-oriented architectures (SOA), and microservices architectures.2.Figure 2.10 The different layered architectures include tiered architectures, service-oriented architectures (SOA), and microservices architectures. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
       "image_path":"/dataset/images/Figure 2.10.jpeg",
        "cognitive_load_label": 3.6
    },
                        {
    "cognitive_load_label": 4.2,
    "text": "Enterprise architecture (EA) emerged at the beginning of the information age in the 1970s, and various enterprise architecture frameworks (EAFs) were developed and used over time. The Open Group Architecture Framework (TOGAF) is one such framework. According to TOGAF, an EA domain represents business, data, application, and technology architectures. While specific frameworks may vary, Table 2.3 illustrates the common enterprise-level architecture domains.",
    "table": {
    "headers": ["Architecture", "Purpose", "Components"],
    "rows": [
      [
        "Business architecture",
        "Defines the organization’s business strategy, goals, processes, and functions",
        "Business models, processes, capabilities, and organizational structure"
      ],
      [
        "Information architecture",
        "Manages data and information assets",
        "Data models, information flow diagrams, data governance, data standards, and metadata"
      ],
      [
        "Application architecture",
        "Designs and organizes software applications",
        "Application portfolio, application integration, and interface design"
      ],
      [
        "Technology architecture",
        "Specifies the hardware, software, and technology infrastructure",
        "Servers, networks, databases, cloud services, security protocols"
      ],
      [
        "Security architecture",
        "Ensures the protection of information assets, systems, and networks",
        "Security policies, access controls, and encryption mechanisms"
      ],
      [
        "Integration architecture",
        "Facilitates seamless communication and data exchange",
        "Middleware, messaging systems, and integration patterns"
      ],
      [
        "Process architecture",
        "Defines and optimizes business processes",
        "Process models, workflow diagrams, and performance metrics"
      ]
    ]
   }
},
                    
{
     "cognitive_load_label": 7.9,
     "image_path":"/dataset/images/Figure 2.11.jpeg",
  "text": "2.2.2 Enterprise Business Architecture and Model. The enterprise business architecture (EBA) constitutes a multifaceted, interdisciplinary, and comprehensive framework that meticulously defines the structural, operational, and strategic paradigms of an entire organization, encompassing its strategic vision, tactical implementations, and operational workflows, while ensuring alignment with enterprise-wide governance protocols and regulatory compliance mandates. It is intricately related to the overarching corporate business ecosystem, including an exhaustive array of documents, diagrams, and artifacts that delineate the architectural structure of the business, often leveraging advanced methodologies such as business process reengineering, value stream mapping, enterprise ontology modeling, and system dynamics analysis to facilitate organizational transformation and competitive differentiation. The EBA framework systematically characterizes critical aspects such as business processes, organizational hierarchies, geographical distribution, stakeholder interactions, digital transformation strategies, and cybernetic feedback loops, providing a holistic view of the enterprise's operational dynamics and enabling predictive analytics for future scalability and resilience against market volatility. EBAs are typically composed of diverse business capabilities, which are methodically grouped into distinct categories, as illustrated in Figure 2.11. In this illustrative example, sample categories include product development lifecycle, sales operations management, customer service excellence, supply chain optimization, financial risk mitigation, and cybersecurity governance. Each business capability under these categories often operates with its own intricate business model, which integrates seamlessly into the overarching business model of the organization, ensuring alignment with strategic objectives, compliance with regulatory frameworks, and optimization of resource allocation through enterprise resource planning (ERP) systems. Notably, the diagram underscores the pivotal role of layering and componentization heuristics in structuring capability models, facilitating modularity, scalability, interoperability across heterogeneous systems, and resilience through fault-tolerant design principles. Furthermore, the EBA can be quantitatively assessed using a capability maturity model, where the maturity level \\( M \\) of a capability is expressed as \\( M = w_1P + w_2E + w_3S \\), where \\( P \\), \\( E \\), and \\( S \\) represent performance, efficiency, and scalability metrics, respectively, with weights \\( w_1, w_2, w_3 \\) summing to 1. The complexity of a capability category can be modeled as \\( C = \\log_2(N + 1) \\), where \\( N \\) is the number of interconnected capabilities. Additionally, the resilience of a capability can be quantified as \\( R = 1 - \\frac{F}{T} \\), where \\( F \\) is the failure rate and \\( T \\) is the total operational time. Table 2.4 provides a detailed evaluation of these capabilities across various metrics, including risk, innovation, and resilience indices. 2.2.2.1 Capability Maturity Assessment. This subsection delves into the advanced assessment methodologies for evaluating the maturity of business capabilities, emphasizing the importance of business process reengineering, enterprise-wide interoperability, digital transformation maturity, and the application of machine learning for predictive maintenance. 2.2.2.2 Interoperability Challenges. This section explores the challenges of ensuring seamless interoperability between disparate systems, focusing on semantic alignment, data integration, API governance, and blockchain-based trust mechanisms. 2.2.2.3 Resilience and Scalability. This subsection addresses the strategies for enhancing resilience and scalability, incorporating concepts such as chaos engineering, microservices orchestration, and zero-trust security models. Figure 2.11 The EBA framework categories, including product, sales, service, supply chain, financial risk mitigation, and cybersecurity governance, support the business organizations and enable strategic alignment, operational efficiency, regulatory compliance, and long-term sustainability. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
  "table": {
    "headers": ["Category", "Capability", "Performance (P)", "Efficiency (E)", "Scalability (S)", "Risk Index (R)", "Innovation Index (I)", "Resilience (R)", "Maturity (M)"],
    "rows": [
      ["Product", "Product Development", "0.8", "0.7", "0.9", "0.3", "0.85", "0.95", "0.81"],
      ["Sales", "Customer Acquisition", "0.6", "0.85", "0.75", "0.4", "0.7", "0.9", "0.72"],
      ["Service", "Customer Support", "0.9", "0.65", "0.8", "0.2", "0.9", "0.92", "0.79"],
      ["Supply Chain", "Logistics Optimization", "0.7", "0.9", "0.85", "0.35", "0.8", "0.88", "0.82"],
      ["Finance", "Risk Mitigation", "0.65", "0.8", "0.7", "0.5", "0.75", "0.85", "0.71"],
      ["Cybersecurity", "Threat Detection", "0.85", "0.75", "0.9", "0.25", "0.95", "0.93", "0.84"]
    ]
  }
},
                        {
        "text": "Role definitions encapsulate a set of functional capabilities essential for an individual to manage one or several distinct business processes, as shown in Table 2.4. It is common for an individual to fulfill multiple roles. The process/role matrix delineates which business roles are accountable for particular business processes. Typically structured at the elementary business process level, this matrix can also be extended to more overarching levels when beneficial.",
        "cognitive_load_label": 4.2,
        "table": {
    "headers": ["Role Title", "Responsibilities", "Reporting"],
    "rows": [
      [
        "DBA (database administrator)",
        "Managing and monitoring the database activities",
        "IT manager or VP of IT"
      ],
      [
        "CIO (chief information officer)",
        "Overall IT leadership",
        "CEO (chief executive officer)"
      ],
      [
        "HR employee",
        "Human resources operations",
        "HR manager"
      ]
    ]
  }

    },
    {
        "text": "In general, the role matrix is a visual representation or chart that outlines the various roles within an organization and their respective responsibilities. It helps in clarifying and communicating the distribution of tasks and authorities among different positions or departments. In a financial instruments trading business model, various roles contribute to the overall functioning of the organization, as shown in Table 2.5.",
        "cognitive_load_label": 4.2,

  "table": {
    "headers": ["Title", "Roles and Responsibilities"],
    "rows": [
      [
        "Traders",
        "Manage trading portfolios\nDevelop strategies to maximize profits and manage risk"
      ],
      [
        "Sales team",
        "Build and maintain client relationships\nUnderstand client needs and offer suitable products and services"
      ],
      [
        "Back-office operations",
        "Process and settle trades\nHandle trade confirmation and reconciliation"
      ],
      [
        "Risk managers",
        "Monitor and manage risk exposure\nEnsure compliance with risk management policies"
      ],
      [
        "Legal advisors",
        "Provide legal advice and services\nEnsure compliance with laws and regulations"
      ],
      [
        "Technology professionals",
        "Develop and maintain trading systems and IT infrastructure\nEnsure the security and efficiency of technology resources"
      ],
      [
        "Human resources",
        "Manage recruitment, training, and employee relations\nEnsure the organization is staffed with skilled and motivated employees"
      ]
    ]
  }
    },
                        {
        "text": "2.2.7 Process Model. The business process is a series of interrelated tasks, activities, or steps performed in a coordinated manner within an organization to achieve a specific business goal. The business process model can be encapsulated within a framework of business process hierarchies. These hierarchies visually illustrate the outcome of breaking down complex processes, a method known as process decomposition. This decomposition is carried out until it reaches the elementary business process (EBP), which is a fundamental and indivisible activity within a business that is not further subdivided into smaller processes (i.e., the smallest unit of business activity). Figure 2.12 illustrates an example of business process modeling for creating an order.Figure 2.12 This flowchart outlines the business process model for creating an order, checking availability, shipping, and payments. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.12.jpeg",
        "cognitive_load_label": 3.7
    },
                        {
        "text": "EBPs possess distinct characteristics that make them noteworthy in the context of business operations. They are considered significant and candidates for in-depth analysis. EBPs are typically associated with the actions of a single user, emphasizing a focused responsibility within the organization. Furthermore, these processes may encompass both manual and automated activities, reflecting the diverse nature of contemporary business workflows. User involvement in EBPs is concentrated at a specific point in time, streamlining the temporal aspect of these activities. The preservation of crucial business distinctions between processes ensures clarity and coherence. Finally, EBPs aim to leave the conceptual entity model in a consistent state, contributing to data integrity and maintaining a reliable representation of the organization’s entities and relationships where the entity model represents the various objects or concepts and their relationships within a system. A process map displays the order of chosen processes from the process hierarchies, highlighting their connection to the roles responsible for executing them. A business process hierarchy organizes a company’s activities from broad, general processes down to specific tasks, making it easier to manage and improve how the business operates. Figure 2.13 illustrates a high-level business process hierarchy that could be used for any manufacturing business process.Figure 2.13 The trading business process hierarchy includes activities such as customer management, product development, quality control, order fulfillment, and customer payments. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.13.jpeg",
        "cognitive_load_label": 4.7
    },
                        {
        "text": "Process map diagrams illustrate the order of chosen processes from the process hierarchies, focusing on their connection to the roles responsible for executing them. The process maps specifically highlight key processes that hold particular business importance. Although every process featured in a process map is also included in the business process hierarchy, not all processes in the hierarchy are depicted in the process maps. Process flow diagrams can depict various elements, such as the business events triggering a process, outcomes of completing a process, the processes themselves, the sequence of process steps, interruptions in the process flow, possibilities for repetition, choices, exclusivity among processes, and any additional notes. Figure 2.14 illustrates a process map diagram for an order to cash business process.Figure 2.14 The order to cash business process flows from order management through product fulfillment and payment. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.14.jpeg",
        "cognitive_load_label": 4.1
    },
                        {
        "text": "Figure 2.15 illustrates the process of managing orders within a business. It begins with order management, where orders are received and verified for accuracy. Next is credit management, which assesses the customer’s credit to ensure they can fulfill payment requirements. The process then moves to fulfillment, where the order is prepared and shipped. Then, during payment, the customer is invoiced and payment is processed. Invoicing is a detailed part of this step, where an invoice is generated and sent to the customer. The final stage is reporting, where the business generates reports on sales and financial transactions. This process ends once all steps are completed, ensuring each order is handled efficiently from start to finish.Figure 2.15 This sample trading business model represents a process map diagram for the enter order process. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
       "image_path":"/dataset/images/Figure 2.15.jpeg",
        "cognitive_load_label": 3.5
    },
                        {
        "text": "2.2. 8 Application Architecture Model. The application architecture is a subset of the enterprise solution architecture that includes a process to architect and design the application architecture as well as the actual application architecture model, which represents the content of the application architecture. Application architecture helps organizations decide how to invest in new software and systems. It makes sure that any new software being looked at, designed, or put into use can work well with the systems the organization already has. This includes brand-new software, updates to old software, making old software better, and buying and updating software packages. Figure 2.16 illustrates a conceptual application architecture model of the sample trading business model that was introduced earlier in this section. The goal of a conceptual application architecture is to demonstrate how members of the organization interact with the application architecture from different locations when invoking business processes. This example illustrates three distinct offices accommodating a diverse range of users, including management, trading, IT, and others.Figure 2.16 A conceptual trading application architecture moves through various levels, including users, functions, enterprise services, and third-party systems. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.16.jpeg",
        "cognitive_load_label": 4.4
    },
                        {
        "text": "Figure 2.17 takes the conceptual trading application architecture a bit further and describes what one of the alternative application architectures may look like at the logical level. The goal of the logical application architecture is to identify the various functional blocks within the architecture without getting into the details of how they connect with and/or use each other.Figure 2.17 The logical trading application architecture illustrates each function block for the processing of customer orders. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.17.jpeg",
        "cognitive_load_label": 3.4
    },
                        {
        "text": "Figure 2.18 is a simplified view of the logical trading application architecture model that provides callouts to explain what the various functions are meant to accomplish. It is good practice to document the functions in that way. It is also good practice to provide an additional diagram (not included here) that includes callouts identifying the actual third-party technologies that are used to implement the various functions.Figure 2.18 This logical trading application architecture includes function callouts that identify the technologies used for this process. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.18.jpeg",
        "cognitive_load_label": 3.4
    },
                        {
        "text": "2.2.9 Data Architecture Model. A data architecture model is a conceptual framework that outlines how an organization structures, organizes, and manages its data assets. Data architecture forms a component of the broader enterprise solution architecture. It encompasses the design process for both information and actual data architecture models, representing the content within the data architecture. This framework aids organizations in strategically planning investments in data management solutions and associated systems. The evaluated, designed, and delivered data management solutions must seamlessly coexist with established ones, managing newly developed databases as well as legacy database extensions. In general, information can be extracted from raw data, knowledge can be gleaned from information, and wisdom can be obtained from knowledge. Most enterprises refer to the relationship between data, information, knowledge, and wisdom as the pyramid of knowledge. A wealth of additional information related to data management and the pyramid of knowledge is provided in Chapter 8 Data Management. Information modeling is the process used to describe the metadata necessary to understand the data, processes, and rules that are relevant to the enterprise, as illustrated in Figure 2.19.Figure 2.19 Examining and learning from data is integral to an organization’s success, as outlined in this role of information modeling figure. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.19.jpeg",
        "cognitive_load_label": 4.6
    },
                        {
        "text": "In the collaborative process of data modeling, IT and business stakeholders establish a shared understanding of essential business terms, known as entities, which typically end up being represented as tables that contain data in a relational database management system. This involves defining the attributes that characterize these terms and establishing the relationships between them. The capability to uphold and document the data model is integral to an organization’s capacity to address varied data acquisition needs across critical business projects. In essence, a well-maintained data model serves as a foundational element for ensuring coherence and effectiveness in managing diverse data requirements.Figure 2.20 is an example of an enterprise-level conceptual data architecture for an insurance company. The goal of the enterprise conceptual data architecture is to illustrate the various types of data repositories and the way data is collected and managed within the enterprise.Figure 2.20 The enterprise conceptual data architecture for a fictitious insurance company highlights the different ways in which data is handled and managed. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.20.jpeg",
        "cognitive_load_label": 4.3
    },
                        {
        "text": "Figure 2.21 illustrates a possible physical architecture for the sample trading business model that was introduced earlier in this section. This diagram depicts the layout of the actual hardware components that make up the infrastructure of the trading solution. It also delineates where the functional blocks of the application architecture are physically deployed. Note that this physical technology architecture leverages the layout and components of the enterprise application architecture illustrated previously at the conceptual level.Figure 2.21 The physical trading application architecture highlights the use of hardware to meet the organization’s goals. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.21.jpeg",
        "cognitive_load_label": 3.8
    },
                        {
        "text": "An alternative physical application architecture for the trading solution is shown in Figure 2.22. The diagram does not delineate where the functional blocks of the application architecture are physically deployed.Figure 2.22 This alternative physical trading application architecture outlines possible changes from the existing web solution. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.22.jpeg",
        "cognitive_load_label": 5.4
    },
                        {
        "text": "2.2.9 Breadth of Applicability of Models. Figure 2.23 illustrates the key dimensions for representing and categorizing architecture models, accompanying diagrams, and related patterns. These architecture domains align with the TOGAF standard, as discussed earlier. The diagram also illustrates the levels of abstraction to characterize various architectural models. Additionally, it introduces the architecture scope as another dimension, classifying the models’ breadth of applicability at the enterprise, portfolio, or project level. The architecture scope is the extent and boundaries within which architectural considerations, decisions, and solutions apply.Figure 2.23 TOGAF architectural dimensions include various levels of abstraction. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
         "image_path":"/dataset/images/Figure 2.23.jpeg",
        "cognitive_load_label": 5.3
    },
                        {
        "text": "2.2.3 Leveraging Architectural Similarities and Applying Patterns. A solutions continuum is a strategy where existing solutions, components, or patterns are leveraged and adapted for use in different contexts. Figure 2.24 illustrates the TOGAF model of reuse that is referred to as the solutions continuum. As mentioned earlier, TOGAF does not provide a prescriptive approach to creating and/or managing a catalog of patterns. However, various pattern repositories are available on the Internet and the adaptive design technique can be used to avoid having to reinvent the wheel when architectural patterns and related component implementations exist and can be customized and assembled with new components. More information on this topic is provided in Chapter 10 Enterprise and Solution Architectures Management.Figure 2.24 This TOGAF solutions continuum illustrates how each architecture guides and supports the others. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
         "image_path":"/dataset/images/Figure 2.24.jpeg",
        "cognitive_load_label": 4.9
    },
                        {
        "text": "As illustrated in Figure 2.25, the TOGAF solutions continuum offers a limited set of dimensions. It serves as a guideline, and The Open Group allows interested parties to enhance the model by incorporating additional dimensions that are relevant to their specific needs.Figure 2.25 The TOGAF architecture continuum can suggest extensions but may only be able to focus on one aspect at a time. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
         "image_path":"/dataset/images/Figure 2.25.jpeg",
        "cognitive_load_label": 4.4
    },
                        {
        "text": "2.2.4 Responsive Web 2.0 Business Solutions. World Wide Web Consortium (W3C) is an international community that develops guidelines to ensure the long-term growth and accessibility of the World Wide Web. Web 2.0 is the second generation of the World Wide Web when we shift from static web pages to dynamic content. Web 3.0 is the third generation of the World Wide Web and represents a vision for the future of the Internet characterized by advanced technologies. Most modern websites rely on the Web 2.0 architectural model set forth by W3C. A sample logical application architecture model is illustrated in Figure 2.26.Figure 2.26 The logical application architecture of Microsoft Azure-hosted web applications allows for responsive web and mobile solutions for users. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.26.jpeg",
        "cognitive_load_label": 4.7
    },
                        {
        "text": "2.3.5 Native Mobile Business Solutions. A web application (web app) is a software application that is accessed and interacted with through a web browser over the Internet. Many web-based solutions leverage the inherent capabilities of mobile devices, offering web apps tailored for various types of phones in addition to responsive websites. Numerous frameworks exist to facilitate the development of native web apps, streamlining the process of creating applications that can run seamlessly on different mobile platforms. These frameworks often provide a unified and efficient approach to building cross-platform mobile applications, ensuring a consistent user experience across various devices.. In certain frameworks and development environments, React Native UI component libraries can be leveraged to, port web apps to mobile devices. Examples include React Native support for Android apps using the Android Studio (Android Studio provides a comprehensive environment for developing, testing, and debugging Android apps) or iPhone web app using XCode IDEs (Xcode is an integrated development environment [IDE] developed by Apple for macOS that offers a suite of tools for building software for Apple platforms, including macOS, iOS, watchOS, and tvOS). Figure 2.27 illustrates the logical application architecture of mobile web apps that use React Native. In addition to these capabilities, the adaptive design reuse approach may be used to create the custom part of the native web app. More information related to the development of native web app solutions is provided in Chapter 9 Software Engineering, Chapter 10 Enterprise and Solution Architectures Management, and Chapter 11 Web Applications Development.Figure 2.27 The logical application architecture of React Native mobile web apps shows the back-end processes that allow both Android and IOS customers to use the same application. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.27.jpeg",
        "cognitive_load_label": 6.2
    },
                        {
        "text": "2.3.6 Web 3.0 Business Solutions.The secure and transparent way of recording transactions that uses a chain of blocks, each storing a list of encrypted transactions is called blockchain. Once a block is full, it is linked to the previous one, forming a chain. Blockchain technology decentralizes processing to ensure the integrity of transactions across multiple computer nodes. This ensures that no single computer node gets assigned to processing transactions repeatedly, thereby preventing possible fraudulent modifications of transactions. A smart contract is an automated agreement written in code that runs on blockchain technology. They enforce contract terms automatically when specific conditions are met, removing the need for intermediaries and ensuring security. The use of blockchain smart contracts within web applications is becoming more popular. The logical application architecture model in Figure 2.28 illustrates how this is made possible by creating hybrid Web 2.0 websites that interact with Web 3.0 smart contracts.Figure 2.28 The flowchart show the logical application architecture of a Web 2.0 and Web 3.0 Website. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.28.jpeg",
        "cognitive_load_label": 5.2
    },
                        {
        "text": "2.3.7 Cloud-Native Business Solutions. A way of building software by breaking it into small, independent pieces where each piece, or service, does a specific job and works on its own is called microservices. A large number of businesses have been migrating their legacy business solutions to the cloud to take advantage of microservices that are designed around specific business functions and can be deployed independently using automated deployment systems. Figure 2.29 illustrates how secure, managed, and monetized APIs that are critical for a digital enterprise can be created by leveraging a combination of API-led integration frameworks and cloud-native technologies. The use of such frameworks and technologies helps streamline the migration of legacy business solutions. The process of migrating legacy business solutions means upgrading or replacing old systems with newer, more efficient ones. In addition to these capabilities, the adaptive design reuse approach may be used to create the custom part of the cloud-native applications. More information related to the development of cloud-native solutions is provided in Chapter 9 Software Engineering, Chapter 10 Enterprise and Solution Architectures Management, and Chapter 12 Cloud-Native Applications Development.Figure 2.29 The cloud-native application architecture view of a digital enterprise shows both client-side and server-side processes through each layer. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 2.29.jpeg",
        "cognitive_load_label": 5.2
    },
                        {
        "text": "2.3.8 Innovative Cloud Mashups.Figure 2.30 and Figure 2.31 illustrate models of solutions that are used today to support a variety of mobile health (MHealth), body area networks (BANs), emotions monitoring, and social media applications. In addition to the capabilities provided by the Big Clouds, the adaptive design reuse approach may be used to create the custom part of these hybrid solutions. Google Maps and Zillow are prime examples of applications that utilize location-based data to deliver valuable services. A GPS device identifies a user’s location, and that information flows through the central network. Apps then display this data in a user-friendly manner, connecting users with real-time geographic information in Google Maps or housing market details in Zillow. The integration of GPS with other IoT systems allows for the seamless presentation of customized, location-specific content to enhance the user experience. More information related to the development of web solutions is provided in Chapter 9 Software Engineering, Chapter 10 Enterprise and Solution Architectures Management, and Chapter 13 Hybrid Multicloud Digital Solutions Development.Figure 2.30 IoT devices use sensors, applications, and connectivity to interact, collect, and exchange data. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
       "image_path":"/dataset/images/Figure 2.30.jpeg",
        "cognitive_load_label":4.2
    },
                        {
        "text": "Internet of Things (IoT) refers to the network of physical devices embedded with sensors, software, and connectivity, enabling them to collect and exchange data. The process of examining, processing, and extracting valuable insights from large datasets is called big data analytics. Developing algorithms that enable computers to learn from data and make decisions without explicit programming is called machine learning. This is made possible by creating mashups of platform services provided by various public cloud vendors to gain access to these disruptive technologies.Figure 2.31 This diagram depicts an architecture of a body area network. (credit: modification of Body Area Network by Jtel /Wikimedia Commons, Public Domain)",
        "image_path":"/dataset/images/Figure 2.31.jpeg",
        "cognitive_load_label": 3.9
    },
                        {
        "text": "3 Chapter Outline. 3.1 Introduction to Data Structures and Algorithms.3.2 Algorithm Design and Discovery. 3.3 Formal Properties of Algorithms. 3.4 Algorithmic Paradigms. 3.5 Sample Algorithms by Problem. 3.6 Computer Science TheoryOnline maps help people navigate a rapidly changing world. It was not long ago that maps were on paper and that knowledge came from non-digital, trusted sources. In this chapter, we will study how computer scientists design and analyze the foundational structures behind many of today’s technologies. Data structures and algorithms are not only foundational to map apps, but also enable an amazing variety of other technologies too. From self-driving cars to inventory management to simulating the movement of galaxies to transferring data between computers—all these applications use data structures and algorithms to efficiently organize and process large amounts of information.Figure 3.1 Online mapping applications represent places, locations, and map data while providing functionality to look around, search for places, and get navigation directions. The right combination of data structures to manage collections of places, locations, and map data along with efficient search and navigation algorithms will help optimize the experience of users trying to find their way through the map and will also make optimal use of computing resources. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license; data source: OpenStreetMap under Open Database License; attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.1.jpeg",
        "cognitive_load_label": 5.1
    },
                        {
        "text": "Among the different types of universal data structures, computer scientists have found it helpful to categorize data structures according to their functionality without considering their specific representation. An abstract data type (ADT) consists of all data structures that share common functionality but differ in specific representation.Common abstract data types for complex data follow, and list and set types are shown in Figure 3.2. We will discuss each abstract data type in more detail together with their data structure implementations.Figure 3.2 Lists and sets are common abstract data types used to represent complex data and can be in the form of integers or string data. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.2.jpeg",
  "data_structures": {
    "headers": ["Data Structure", "Description", "Key Features / Operations", "Example Use Case"],
    "rows": [
      [
        "List",
        "An ordered sequence of elements that allows adding, retrieving, and removing elements from any position.",
        "Indexed access, insertion/removal at any position, maintains order",
        "To-do list where each task is in chronological order"
      ],
      [
        "Set",
        "An unordered collection of unique elements that allows adding, retrieving, and removing elements.",
        "No duplicate elements, faster membership checks, unordered",
        "A set of unique place names you want to visit"
      ],
      [
        "Map (Dictionary)",
        "An unordered collection of key-value pairs where each key appears only once.",
        "Associates keys with values, efficient lookups by key",
        "A travel wish list mapping each place to a list of things to do"
      ],
      [
        "Priority Queue",
        "A collection of elements where each element has an associated priority value.",
        "Insertion of elements, retrieval/removal of highest-priority element",
        "Hospital emergency room patient triage system"
      ],
      [
        "Graph",
        "Represents binary relations among entities using vertices and edges.",
        "Supports adding/removing vertices and edges, adjacency queries",
        "Friendship graph representing relationships between people"
      ]
    ]
  },

    "cognitive_load_label": 6.3
    },
                        {
        "text": "3.1.1 Linear Data Structures. If a problem can be solved with an ordered sequence of elements (e.g., numbers, payroll records, or text messages), the simplest approach might be to store them in a list. Some problems require that actions be performed in a strict chronological order, such as processing items in the order that they arrive or in the reverse order. In these situations, a linear data structure, which is a category of data structures where elements are ordered in a line, is appropriate. There are two possible implementations for the list abstract data type. The first, an array list (Figure 3.3), is a data structure that stores list elements next to each other in memory. The other is a linked list (Figure 3.4), which is a list data structure that does not necessarily store list elements next to each other, but instead works by maintaining, for each element, a link to the next element in the list. Both array lists and linked lists are linear data structures because their elements are organized in a line, one after the other. An advantage of array lists is that they allow (random) access to every element in the list in a single step. This is in sharp contrast with linked lists, which only supports “sequential access.” On the other hand, linked lists support fast insertion and deletion operations, which array lists do not.Figure 3.3 An array list stores elements next to each other in memory in the exact list order. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.3.jpeg",
        "cognitive_load_label": 5.8
    },
                        {
        "text": "3.1.2 Tree Data Structures.A tree is a hierarchical data structure. While there are many kinds of tree data structures, all of them share the same basic organizing structure: a node represents an element in a tree or graph. A node may or may not have a descendant. A child node is a descendant of another node. Often, the primary node is referred to as the “parent node.” Trees maintain a hierarchy through parent-child relationships, which repeat from the root node at the top of the tree down to each leaf node, which is at the bottom of the tree and has no children. The height of a tree corresponds to the depth of the hierarchy of descendants. Figure 3.5 illustrates the structure and elements of a tree.Figure 3.5 A tree is a hierarchical data structure with nodes where each node can have zero or more descendant child nodes. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.5.jpeg",
        "cognitive_load_label": 6.7
    },
                        {
        "text": "3.1.3 Binary Search Trees.A binary search tree is a kind of tree data structure often used to implement sets and maps with the binary tree property, which requires that each node can have either zero, one, or two children, and the search tree property, which requires that elements in the tree are organized least-to-greatest from left-to-right. In other words, the values of all the elements of the left subtree of a node have a lesser value than that of the node. Similarly, the values of all the elements of the right subtree of a node have a greater value than that of the node. The search tree property suggests that when elements are read left-to-right in a search tree, we will get the elements in sorted order. For numbers, we can compare and sort numbers by their numeric values. For more complex data like words or sentences, we can compare and sort them in dictionary order. Binary search trees use these intrinsic properties of data to organize elements in a searchable hierarchy (Figure 3.6).Figure 3.6 A binary search tree organizes elements least to greatest from left to right. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.6.jpeg",
        "cognitive_load_label": 7.2
    },
                        {
        "text": "3.1.4 Balanced Binary Search Trees. Binary search trees are not as effective as we have described. The dictionary example represents a best-case scenario for binary search trees. We can only rule out half of the remaining elements each time if the binary search tree is perfectly balanced, which means that for every node in the binary search tree, its left and right subtrees contain the same number of elements. This is a strong requirement, since the order in which elements are added to a binary search tree determines the shape of the tree. In other words, binary search trees can easily become unbalanced. It is possible for a binary search tree to look exactly like a linked list, in which each node contains either zero children or one child, which is no more efficient than a linear data structure. An AVL tree (named after its inventors, Adelson-Velsky and Landis) is a balanced binary search tree data structure often used to implement sets or maps with one additional tree property: the AVL tree property, which requires the left and right subtrees to be balanced at every node of the tree. AVL trees are just one among many “self-balancing” binary search trees. A balanced binary search tree introduces additional properties that ensure that the tree reorganizes elements to maintain balance (Figure 3.7).Figure 3.7 An AVL tree rotates nodes in a binary search tree to maintain balance. This sequence of steps illustrates the insertion of numbers 1, 2, 3, 4, 5, 6 into an initially empty AVL tree. (The steps in which rotation occurs are represented by the solid black arrows.) (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.7.jpeg",
        "cognitive_load_label": 7.5
    },
                        {
        "text": "3.1.5 Binary Heaps.Priority queues focus on retrieving and removing the highest-priority elements first, adding an element to a priority queue also involves specifying an associated priority value that is used to determine which elements are served next. For example, patients in an emergency room might be served according to the severity of their health concerns rather than according to arrival time. A binary heap is a type of binary tree data structure that is also the most common implementation for the priority queue abstract data type (Figure 3.8). A binary heap is not a search tree, but rather a hybrid data structure between a binary tree and an array list. Data is stored as an array list in memory, but the binary heap helps visualize data in the same way that a binary tree does, which makes it easier to understand how data are stored and manipulated. Binary heaps organize elements according to the heap property, which requires that the priority value of each node in the heap is greater than or equal to the priority values of its children. The heap property suggests that the highest-priority element will always be the root node where it is efficient to access.Figure 3.8 A binary heap is the most common implementation of the priority queue abstract data type. The priority value of each node in the binary heap is greater than or equal to the priority values of the children. Note that the value stored in the root node of the right subtree can be smaller than the value stored in any node in the left subtree, while not violating the heap property. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.8.jpeg",
        "cognitive_load_label": 6.9
    },
                        {
        "text": "3.2 Search Algorithms. In computer science, searching is the problem of retrieving a target element from a collection that contains many elements. There are many ways to understand search algorithms; depending on the exact context of the problem and the input data, the expected output might differ. For example, suppose we want to find the target term in a dictionary that contains thousands or millions of terms and their associated definitions. If we represent this dictionary as a list, the search algorithm would return the index of the term in the dictionary. If we represent this dictionary as a set, the search algorithm would return whether the target is in the dictionary. If we represent this dictionary as a map, the search algorithm would return the definition associated with the term. The dictionary data structure has implications on the output of the search algorithm. Algorithmic problem-solving tends to be iterative because we might sometime later realize that our data structures need to change. Changing the data structures, in turn, often also requires changing the algorithm design.Despite these differences in output, the underlying canonical searching algorithm can still follow the same general procedure. The two most well-known canonical searching algorithms are known as sequential search and binary search, which are conducted on linear data structures, such as array lists. Sequential search (Figure 3.9). Open the dictionary to the first term. If that term happens to be the target, then great—we have found the target. If not, then repeat the process by reading the next term in the dictionary until we have checked all the terms in the dictionary.Figure 3.9 A sequential search can find the number 47 in an array by checking each number in order. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.9.jpeg",
        "cognitive_load_label": 5.4
    },{
            "text": "3.2 Search Algorithms. In computer science, searching is the problem of retrieving a target element from a collection that contains many elements. There are many ways to understand search algorithms; depending on the exact context of the problem and the input data, the expected output might differ. For example, suppose we want to find the target term in a dictionary that contains thousands or millions of terms and their associated definitions. If we represent this dictionary as a list, the search algorithm would return the index of the term in the dictionary. If we represent this dictionary as a set, the search algorithm would return whether the target is in the dictionary. If we represent this dictionary as a map, the search algorithm would return the definition associated with the term. The dictionary data structure has implications on the output of the search algorithm. Algorithmic problem-solving tends to be iterative because we might sometime later realize that our data structures need to change. Changing the data structures, in turn, often also requires changing the algorithm design.Despite these differences in output, the underlying canonical searching algorithm can still follow the same general procedure. The two most well-known canonical searching algorithms are known as sequential search and binary search, which are conducted on linear data structures, such as array lists. Sequential search (Figure 3.9). Open the dictionary to the first term. If that term happens to be the target, then great—we have found the target. If not, then repeat the process by reading the next term in the dictionary until we have checked all the terms in the dictionary.Figure 3.10 A binary search can find the number 47 in an array by determining whether the desired number comes before or after a chosen number. It eliminates half of existing data points and then searches in the remaining half, repeating the pattern, until the number is found. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.10.jpeg",
        "cognitive_load_label": 3.3
    },
                        {
        "text": "3.3 Algorithm Design Patterns.This case study of canonical search algorithms demonstrates some ideas about algorithmic problem-solving, such as how algorithm design involves iterative improvement (from sequential search to binary search). But this case study does not demonstrate how algorithms are designed in practice. Algorithm designers are occasionally inspired by real-world analogies and metaphors, such as relying on sorted order to divide a dictionary into two equal halves. More often, they depend on knowledge of an existing algorithm design pattern, or a solution to a well-known computing problem, such as sorting and searching. Rather than develop wholly new ideas each time they face a new problem, algorithm designers instead apply one or more algorithm design patterns to solve new problems. By focusing on algorithm design patterns, programmers can solve a wide variety of problems without having to invent a new algorithm every time.Figure 3.11 Sequential search needs to check every term to see if it matches the prefix “Sea,” whereas two binary searches can be used to find the start and end points of the matching terms in the list. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.11.jpeg",
        "cognitive_load_label": 2.9
    },
                        {
        "text": "3.3Performance Profiling. Modern computer systems are complicated. Algorithms are just one component in a much larger ecosystem that involves communication between many other subsystems, other computers in a data center, and other systems on the Internet. Algorithmic runtime analysis focuses on the properties of the algorithm rather than all the different ways the algorithm interacts with the rest of the world. But once an algorithm is implemented as a computer program, these interactions with the computing ecosystem play an important role in determining program performance.A profiler is a tool that measures the performance (runtime and memory usage) of a program. Profilers are commonly used to diagnose real-world performance issues by producing graphs of how computational resources are used in a program. A common graph is a flame graph (Figure 3.12) that visualizes resource utilization by each part of a program to help identify the most resource-intensive parts of a program. Saving even a few percentage points of resources can lead to significantly reduced time, money, and energy expenditure. As the global demand for computation continues to increase, performance engineers who know how to leverage profilers to analyze systems and implement resource-saving changes will be key to a green energy future.Figure 3.12 A flame graph shows which parts of a program require the most resources. The x-axis shows relative duration and the width indicates the percentage of total duration spent in a function. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.12.jpeg",
        "cognitive_load_label": 3.5
    },
                        {
        "text": "Finally, we can formalize our description using either precise English or a special mathematical notation called Big O notation, which is the most common type of asymptotic notation in computer science used to measure worst-case complexity. In precise English, we might say that the time complexity for this sequential search algorithm has two cases (Figure 3.13):In the worst-case situation (when the target word is either at the end of the list or not in the list at all), sequential search takes N repetitions where N is the number of words in the list.Figure 3.13 The best case for sequential search in a sorted list is to find the word at the top of the list, whereas the worst case is to find the word at the bottom of the list (or not in the list at all). (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.13.jpeg",
        "cognitive_load_label": 3.3
    },
                        {
        "text": "This prediction is a useful outcome of time complexity analysis. It allows us to estimate the runtime of the sequential search algorithm on a problem of any size, before writing the program or obtaining a dictionary of words that large. Moreover, it helps us decide if we want to use this algorithm or explore other algorithm designs and approaches. We might compare this sequential search algorithm to the binary search algorithm and adjust our algorithm design accordingly.Figure 3.14 The order of growth of an algorithm is useful to estimate its runtime efficiency as the input size increases (e.g., constant, logarithmic, and other orders of growth) to help determine which algorithmic approach to take. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.14.jpeg",
        "cognitive_load_label": 3.3
    },
                        {
        "text": "The constant order of growth is described in Big O notation as “O(1)” while the linear order of growth is described in Big O notation as “O(N) with respect to N, the size of the problem.” Big O notation formalizes the concept of a prediction. Given the size of the problem, N, calculate how long it takes to run the algorithm on a problem of that size. For large lists, in order to double the worst-case runtime of sequential search, we would need to double the size of the list.",
  "big_o_orders_of_growth": {
    "headers": ["Order of Growth", "Notation", "Description"],
    "rows": [
      [
        "Constant",
        "O(1)",
        "Execution time remains constant regardless of input size."
      ],
      [
        "Logarithmic",
        "O(log N)",
        "Execution time grows logarithmically with input size."
      ],
      [
        "Linear",
        "O(N)",
        "Execution time grows linearly with input size."
      ],
      [
        "Linearithmic",
        "O(N log N)",
        "Execution time grows proportionally to N multiplied by log N."
      ],
      [
        "Quadratic",
        "O(N²)",
        "Execution time grows quadratically with input size."
      ],
      [
        "Cubic",
        "O(N³)",
        "Execution time grows cubically with input size."
      ],
      [
        "Exponential",
        "O(2ᴺ), O(3ᴺ), ...",
        "Execution time grows exponentially with input size."
      ],
      [
        "Factorial",
        "O(N!)",
        "Execution time grows factorially with input size."
      ]
    ]
  },
        "cognitive_load_label": 5.3
    },
                        {
        "text": "Another way to understand orders of growth is to consider how a change in the size of the problem results in a change to the resource usage. When we double the size of the input problem, algorithms in each order of growth respond differently (Figure 3.15).Figure 3.15 This chart shows the time it would take for an algorithm with each of the given orders of growth to finish running on a problem of the given size, N. When an algorithm takes longer than 1025 years to compute, that means it takes longer than the current age of the universe. (data source: Geeks for Geeks, “Big O Notation Tutorial—A Guide to Big O Analysis,” last updated March 29, 2024; attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.15.jpeg",
  "big_o_response_to_input_growth": {
    "headers": ["Order of Growth", "Effect on Resource Usage When Input Size Doubles"],
    "rows": [
      [
        "O(1)",
        "Will not require any more resources"
      ],
      [
        "O(log N)",
        "Will require 1 additional resource unit"
      ],
      [
        "O(N)",
        "Will require 2 times the number of resources"
      ],
      [
        "O(N log N)",
        "Will require a little more than 2 times the number of resources"
      ],
      [
        "O(N²)",
        "Will require 4 times the number of resources"
      ],
      [
        "O(N³)",
        "Will require 8 times the number of resources"
      ],
      [
        "O(2ᴺ), O(3ᴺ), ...",
        "Will require the squared or cubed number of resources"
      ],
      [
        "O(N!)",
        "Will require even more resources"
      ]
    ]
  },
        "cognitive_load_label": 5.1
    },
                        {
        "text": "Earlier, we introduced binary search to find a target within a sorted list as an analogy for finding a term in a dictionary sorted alphabetically. Instead of starting from the beginning of the dictionary and checking each term, as in a sequential search, we could instead start from the middle and look left or right based on where we would expect to find the term in the dictionary. But binary search can also be understood as an example of a divide and conquer algorithm (Figure 3.16).The problem of finding a target within the entire sorted list is broken down (divided) into the subproblem of finding a target within half of the list after comparing the middle element to the target. Half of the list can be ruled out based on this comparison, leaving binary search to find the target within the remaining half.Binary search is repeated on the remaining half of the sorted list (conquer). This process continues recursively until the target is found in the sorted list (or reported as not in the list at all).To solve the original problem of finding a target within the entire sorted list, the result of the subproblem must inform the overall solution. The original call to binary search reports the same result as its subproblem.Figure 3.16 Binary search is a divide and conquer algorithm that repeatedly makes one recursive call on half of remaining elements. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.16.jpeg",
        "cognitive_load_label": 3.5
    },
                        {
        "text": "Given a list of elements in an unknown order, a sorting algorithm should return a new list containing the same elements rearranged into a logical order, such as least to greatest. One canonical divide and conquer algorithm for comparison sorting is called merge sort. The problem of comparison sorting is grounded in the comparison operation. The comparison operation is like a traditional weighing scale that tells whether one element is heavier, lighter, or the same weight as another element, but provides no information about the exact weight or value of the element. Though this might seem like a serious restriction, comparison sorting is actually a very rich problem in computer science—it is perhaps the most deeply studied problem in computer science. Choosing comparison as the fundamental operation is also practical for complex data, where it might be hard (or even impossible) to assign an exact numeric ranking (Figure 3.17).The problem of sorting the list is broken down (divided) into two subproblems: the subproblem of sorting the left half and the subproblem of sorting the right half.Merge sort is repeated to sort each half (conquer). This process continues recursively until the sublists are one element long. To sort a one-element list, the algorithm does not need to do anything, since the elements in the list are already arranged in a logical order.To solve the original problem of sorting the entire list, combine adjacent sorted sublists by merging them while maintaining sorted order. Merging each pair of adjacent sorted sublists repeats to form larger and larger sorted sublists until the entire list is fully sorted.Figure 3.17 Merge sort is a divide and conquer algorithm that repeatedly makes two recursive calls on both halves of the sublist. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.17.jpeg",
        "cognitive_load_label": 3.2
    },
                        {
        "text": "For example, a greedy interval scheduling algorithm might choose to work on the task that takes the least amount of time to complete; in other words, the cheapest way to mark one task as completed (Figure 3.18). If the tasks are scheduled in advance and we can only work on one task at a time, choosing the task that takes the least amount of time to complete might prevent us from completing multiple other (longer) tasks that just so happen to overlap in time. This greedy algorithm does not compute the right output—it finds a solution but not the optimal solution.Figure 3.18 Greedy interval scheduling will not work if the simple rule repeatedly selects the shortest interval. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
         "image_path":"/dataset/images/Figure 3.18.jpeg",
        "cognitive_load_label": 2.9
    },
                        {
        "text": "Consider the municipal broadband planning problem or, more formally, the minimum spanning tree problem, of finding a lowest-cost way to connect all the vertices in a connected graph to each other. If we want to minimize the sum of the selected edge weights, one idea is to repeatedly select the next edge (connections between vertices) with the lowest weight so long as it extends the reach of the network. Or, in the context of the municipal broadband planning problem, we want to ensure that the next-cheapest connection that we choose to build reaches someone who needs access to the Internet (Figure 3.19).Figure 3.19 Municipal broadband planning can be represented as a minimum spanning trees graph problem where the weight of each edge represents the cost of building a connection between two vertices or places. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.19.jpeg",
        "cognitive_load_label": 3.3
    },
                        {
        "text": "3.41.1 Reduction Algorithms.Algorithm designers spend much of their time modeling problems by selecting and adapting relevant data structures and algorithms to represent the problem and a solution in a computer program. This process of modeling often involves modifying an algorithm design pattern so that it can be applied to the problem. But there is also a different approach to algorithm design that solves problems by changing the input data and output data to fit a preexisting problem. Rather than solve the problem directly, a reduction algorithm solves problems by transforming them into other problems (Figure 3.20).Preprocess: Transform the input data so that it is acceptable to an algorithm meant for the other problem.Apply the algorithm meant for the other problem on the preprocessed data.Postprocess: Transform the output of the algorithm meant for the other problem so that it matches the expected output for the original problem.Figure 3.20 A reduction algorithm preprocesses the input data, passes it to another algorithm, and then postprocesses the algorithm’s output to solve the original problem. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.20.jpeg",
        "cognitive_load_label": 3.4
    },
                        {
        "text": "Consider a slightly different version of the municipal broadband planning problem, where instead of only considering connections (edges), we expand the problem to consider the possibility of installing broadband nodes directly to each address without relying on potentially expensive neighboring connections. That is, all vertices installed with broadband nodes are inter-connected with each other through another broadband network. If we were to run an algorithm for solving the minimum spanning tree problem on this graph directly, then our result would never consider installing broadband nodes directly, since minimum spanning tree algorithms do not consider vertex weights (Figure 3.21).Figure 3.21 The problem of finding a minimum spanning tree in a graph with vertex weights can be reduced to the problem of finding a minimum spanning tree in a graph without vertex weights. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.21.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5 Data Structure Problems.Data structure problems are not only useful for implementing data structures, but also as fundamental algorithm design patterns for organizing data to enable efficient solutions to almost every other computing problem.3.5.1 Searching.Searching is the problem of retrieving a target element from a collection of elements. Searching in a linear data structure, such as an array list, can be done using either sequential search or binary search.3.5.2. Sequential Search Algorithm. A sequential search algorithm is a searching algorithm that sequentially checks the collection element-by-element for the target. The runtime of sequential search is in O(N) with respect to N, the number of elements in the list (Figure 3.22).Figure 3.22 Sequential search is a search algorithm that checks the collection element by element to find a target element. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.22.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.5 Quicksort Algorithm.A quicksort algorithm recursively sorts data by applying the binary search tree algorithm design pattern to partition data around pivot elements. Whereas the merge sort algorithm rearranges elements by repeatedly merging sorted sublists after each recursive subproblem, the quicksort algorithm rearranges elements by partitioning data before each recursive subproblem. The partition operation takes a pivot and rearranges the elements into three sections, from left to right: the sublist of all elements less than the pivot, the pivot element, and the sublist of all elements greater than (or equal to) the pivot. Each of the two sublists resulting from the partition operation is a recursive quicksort subproblem; when both of the sublists are sorted, the entire list will be sorted. The runtime of quicksort depends on the choice of each pivot element during the execution of the recursive algorithm, but in practice, for most of the inputs, the runtime is in O(N log N) with respect to N, the number of elements (Figure 3.23).Figure 3.23 Quicksort is a divide and conquer sorting algorithm that sorts elements by recursively partitioning elements around a pivot. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.23.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.6 Heapsort Algorithm. A heapsort algorithm adds all elements to a binary heap priority queue data structure using the comparison operation to determine priority value, and returns the sorted list by repeatedly removing from the priority queue element by element. The runtime of heapsort is in O(N log N) with respect to N, the number of elements. The logarithmic time factor is due to the time it takes to add or remove each element from the binary heap (Figure 3.24).Figure 3.24 Heapsort uses the binary heap data structure to sort elements by adding and then removing all elements from the heap. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.24.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.6. Hashing.Hashing has a variety of applications spanning computer systems, database systems, computer security, and searching algorithms. For example, hashing algorithms are often used designing secure systems for protecting stored passwords even after a security breach occurs. In the context of data structure problems, hashing offers a different approach than binary search. Instead of relying on pairwise comparisons to narrow down the expected location of an element in a sorted list in logarithmic time, hashing search algorithms can instead directly index an element by hash value in constant time. If binary search trees implement sets and maps by applying the concept of binary search, a hash table implements sets and maps by applying the concept of hashing (Figure 3.25). Rather than organize elements in sorted order from left to right as in a binary search tree, hash tables store and retrieve elements in an array indexed by hash value.Figure 3.25 Hash tables data structures apply hashing to implement abstract data types such as sets and maps, but must handle collisions between elements that share the same hash value. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.25.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.7Depth-First Search.A depth-first search is a graph traversal algorithm that recursively explores each neighbor, continuing as far possible along each subproblem depth-first (Figure 3.26). Explored vertices are added to a global set to ensure that the algorithm only explores each vertex once. The runtime of depth-first search is in O(|V| + |E|) with respect to |V|, the number of vertices, and |E|, the number of edges.Figure 3.26 Depth-first search is a graph traversal algorithm that continues as far down a path as possible from a start vertex before backtracking. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.26.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.8. Breadth-First Search.A breadth-first search iteratively explores each neighbor, expanding the search level-by-level breadth-first (Figure 3.27). Explored vertices are also added to a global set to ensure that the algorithm explores each vertex once and in the correct level-order. The runtime of breadth-first search is also in O(|V| + |E|) with respect to |V|, the number of vertices, and |E|, the number of edges..Graph traversal algorithms are the foundational algorithm design patterns for most graph processing algorithms. Many algorithms require some amount of exploration, and that exploration typically starts at some vertex and continues processing each reachable vertex at most once. A reachable vertex can be reached if a path or sequence of edges from the start vertex exists. As opposed to depth-first search, breadth-first search has the benefit of exploring vertices closer to the start before exploring vertices further from the start, which can be useful for solving problems such as unweighted shortest paths.Figure 3.27 Depth-first search is a graph traversal algorithm that continues as far down a path as possible from a start vertex before backtracking. Breadth-first search is a graph traversal algorithm that explores level by level expanding outward from the start vertex. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.27.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.9.Minimum Spanning Trees.Minimum spanning trees is the problem of finding a lowest-cost way to connect all the vertices to each other, where cost is the sum of the selected edge weights. The two canonical greedy algorithms for finding a minimum spanning tree in a graph are Kruskal’s algorithm and Prim’s algorithm. Both algorithms repeatedly apply the rule of selecting the next lowest-weight edge to an unconnected part of the graph. The output of a minimum spanning tree algorithm is a set of |V| - 1 edges connecting all the vertices in the graph with the least total sum of edge weights, where |V| is the number of vertices.3.5.10. Kruskal’s Algorithm.Kruskal’s algorithm begins by considering each vertex as an independent island, and the goal of the algorithm is to repeatedly connect islands by selecting the lowest-cost edges. A specialized data structure (called disjoint sets) is typically used to keep track of the independent islands. The runtime of Kruskal’s algorithm is in O(|E| log |E|) with respect to |E|, the number of edges (Figure 3.28).Figure 3.28 Kruskal’s algorithm repeatedly selects the next lightest edge that connects two independent islands. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.28.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.11.Prim’s Algorithm.Prim’s algorithm grows a minimum spanning tree one edge at a time by selecting the lowest-weight edge to an unexplored vertex. The runtime of Prim’s algorithm is in O(|E| log |V| + |V| log |V|) with respect to |V|, the number of vertices, and |E|, the number of edges (Figure 3.29).Figure 3.29 Prim’s algorithm expands outward from the start vertex by repeatedly selecting the next lightest edge to an unreached vertex. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.29.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.12.Shortest Paths.The output of a shortest paths algorithm is a shortest paths tree, the lowest-cost way to get from one vertex to every other vertex in a graph (Figure 3.30). The unweighted shortest path is the problem of finding the shortest paths in terms of the number of edges. Given a start vertex, breadth-first search can compute the unweighted shortest paths tree from the start vertex to every other reachable vertex in the graph by maintaining a map data structure of the path used to reach each vertex.Figure 3.30 Three shortest paths trees of the lowest-cost way to get from the start vertex to every other vertex in the graph are shown. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.30.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.5.13.Dijkstra’s Algorithm.Dijkstra’s algorithm maintains a priority queue of vertices in the graph ordered by distance from the start and repeatedly selects the next shortest path to an unconnected part of the graph. Dijkstra’s algorithm is almost identical to Prim’s algorithm except processing shortest paths (sequences of edges) rather than individual edges. Dijkstra’s algorithm grows a shortest paths tree one shortest path at a time by selecting the next shortest path to an unexplored vertex. The runtime of Dijkstra’s algorithm is in O(|E| log |V| + |V| log |V|) with respect to |V|, the number of vertices, and |E|, the number of edges (Figure 3.31).Dijkstra’s algorithm displayed with edges 0, 2, 3, 4, 5, 6 and decimal numbers in between. First algorithm shows 6,0 connected with red line. Next algorithm displays 0, 6, 3, 2. Next displays 0, 6, 3, 2, 4. Last one displays 5, 4, 2, 3, 6, 0.Figure 3.31 Dijkstra’s algorithm expands outward from the start vertex by repeatedly selecting the next lowest-cost path to an unreached vertex. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
        "image_path":"/dataset/images/Figure 3.31.jpeg",
        "cognitive_load_label": 1
    },
                        {
        "text": "3.6.NP-complete Problems.NP-complete refers to all the hardest NP problems—the combinatorial problems for which we do not have deterministic polynomial-time algorithms. More precisely, a problem PI is said to be NP-complete if PI is in NP and for every problem in NP, there is a reduction that reduces the problem to PI. For example, a longest path, or the problem of finding the highest-cost way to get from one vertex to another without repeating vertices, is an NP-complete problem opposite to shortest paths (Figure 3.32). What makes longest paths so much harder to solve than shortest paths? For one, there is no underlying structure to the solution that we can use to repeatedly apply a simple rule as in a greedy algorithm. With the shortest paths problem, we could rely on the shortest paths tree to inform the solution. But in longest paths, the goal is to wander around the graph. The longest path between any two vertices will probably involve traversing as many edges as possible to maximize distance, visiting many vertices along the way. For some graphs, the longest paths might even visit all the vertices in the graph. In this situation, the longest paths do not form a tree and instead involve ordering all the vertices in the graph for each longest path. Identifying the correct longest path then requires listing out all the possible paths in the graph—a combinatorial explosion in the combinations of edges and vertices that can be selected to form a solution.Figure 3.32 The longest path in a graph maximizes the distance, which often (but not always) involves visiting many vertices along the way. (attribution: Copyright Rice University, OpenStax, under CC BY 4.0 license)",
         "image_path":"/dataset/images/Figure 3.32.jpeg",
        "cognitive_load_label": 1
    },
  {
    "cognitive_load_label": 3.4,
    "name": "Information Gain (IG)",
    "formula": "IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)",
    "text": "**Definition**: Measures the reduction in entropy when splitting dataset S by feature A. **Symbols**: H(S) = entropy of set S, |S_v| = size of subset where feature A has value v. **Usage**: Feature selection in decision trees. **Significance**: Higher values indicate more informative features. **Calculation**: 1) Calculate original entropy H(S) 2) Partition data by feature values 3) Compute weighted average of subset entropies 4) Subtract from original entropy."
  },
  {
    "cognitive_load_label": 3.0,
    "name": "Gradient Descent Update",
    "formula": "\\theta_{t+1} = \\theta_t - \\eta \\nabla J(\\theta_t)",
    "text": "**Definition**: Iterative optimization algorithm for minimizing differentiable functions. **Symbols**: θ = parameters, η = learning rate, ∇J = gradient of cost function. **Usage**: Training machine learning models. **Significance**: Foundation of most modern ML optimization. **Calculation**: 1) Compute gradient at current position 2) Multiply by learning rate 3) Update parameters."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Sigmoid Function",
    "formula": "\\sigma(z) = \\frac{1}{1 + e^{-z}}",
    "text": "**Definition**: S-shaped activation function that maps real numbers to (0,1) range. **Symbols**: z = input value. **Usage**: Binary classification output, neural network activations. **Significance**: Provides smooth, differentiable transition. **Calculation**: 1) Compute negative exponent 2) Add 1 3) Take reciprocal."
  },
  {
    "cognitive_load_label": 3.0,
    "name": "Euclidean Distance",
    "formula": "d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{\\sum_{i=1}^n (q_i - p_i)^2}",
    "text": "**Definition**: Straight-line distance between two points in n-dimensional space. **Symbols**: p,q = vectors, n = number of dimensions. **Usage**: K-nearest neighbors, clustering algorithms. **Significance**: Default distance metric in many applications. **Calculation**: 1) Subtract vectors element-wise 2) Square differences 3) Sum 4) Take square root."
  },
  {
    "cognitive_load_label": 3.1,
    "name": "Cross-Entropy Loss",
    "formula": "L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C y_{i,c} \\log(p_{i,c})",
    "text": "**Definition**: Measurement of difference between two probability distributions. **Symbols**: y = true distribution (one-hot), p = predicted distribution. **Usage**: Classification model evaluation. **Significance**: Penalizes confident incorrect predictions. **Calculation**: For each sample, sum the log probabilities of true classes."
  },
  {
    "cognitive_load_label": 3.1,
    "name": "Softmax Function",
    "formula": "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}",
    "text": "**Definition**: Normalized exponential function converting logits to probabilities. **Symbols**: z = input vector, K = number of classes. **Usage**: Multi-class classification output layer. **Significance**: Maintains relative ordering while normalizing. **Calculation**: 1) Exponentiate inputs 2) Sum exponentials 3) Divide each exponent by sum."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Confusion Matrix Accuracy",
    "formula": "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}",
    "text": "**Definition**: Ratio of correct predictions to total predictions. **Symbols**: TP = True Positives, TN = True Negatives, etc. **Usage**: Classification model evaluation. **Significance**: Overall correctness measure. **Calculation**: Sum correct predictions, divide by total cases."
  },
  {
    "cognitive_load_label": 3.1,
    "name": "F1 Score",
    "formula": "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}",
    "text": "**Definition**: Harmonic mean of precision and recall. **Symbols**: Precision = TP/(TP+FP), Recall = TP/(TP+FN). **Usage**: Imbalanced classification evaluation. **Significance**: Balances precision and recall concerns. **Calculation**: 1) Compute precision and recall 2) Multiply them 3) Divide by sum 4) Multiply by 2."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "L2 Regularization",
    "formula": "R(\\mathbf{w}) = \\lambda \\sum_{i=1}^n w_i^2",
    "text": "**Definition**: Penalization of large weight magnitudes. **Symbols**: w = model weights, λ = regularization strength. **Usage**: Preventing overfitting in regression. **Significance**: Encourages smaller, distributed weights. **Calculation**: Sum squared weights multiplied by λ."
  },
  {
    "cognitive_load_label": 2.8,
    "name": "Bayes' Theorem",
    "formula": "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}",
    "text": "**Definition**: Probability of event A given that B is true. **Symbols**: P(A|B) = posterior, P(A) = prior, P(B|A) = likelihood. **Usage**: Statistical inference, spam filtering. **Significance**: Foundation of Bayesian statistics. **Calculation**: Multiply prior by likelihood, divide by evidence."
  },
  {
    "cognitive_load_label": 2.7,
    "name": "Principal Component Analysis (Variance)",
    "formula": "\\text{Var}(X) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2",
    "text": "**Definition**: Measure of data dispersion around the mean. **Symbols**: x̄ = sample mean, n = sample size. **Usage**: Dimensionality reduction. **Significance**: Basis for PCA component selection. **Calculation**: 1) Compute mean 2) Square deviations 3) Average with Bessel correction."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Gini Impurity",
    "formula": "G = 1 - \\sum_{i=1}^C p_i^2",
    "text": "**Definition**: Measure of node purity in decision trees. **Symbols**: p_i = proportion of class i. **Usage**: Decision tree splitting criterion. **Significance**: Lower values indicate purer nodes. **Calculation**: 1) Square class proportions 2) Sum squares 3) Subtract from 1."
  },
  {
    "cognitive_load_label": 2.7,
    "name": "Matrix Multiplication",
    "formula": "C_{ij} = \\sum_{k=1}^n A_{ik}B_{kj}",
    "text": "**Definition**: Operation producing new matrix from two matrices. **Symbols**: A = m×n matrix, B = n×p matrix. **Usage**: Linear transformations, neural networks. **Significance**: Fundamental linear algebra operation. **Calculation**: Dot product of rows and columns."
  },
  {
    "cognitive_load_label": 2.8,
    "name": "Huber Loss",
    "formula": "L_\\delta(a) = \\begin{cases} \\frac{1}{2}a^2 & \\text{for } |a| \\le \\delta, \\\\ \\delta(|a| - \\frac{1}{2}\\delta) & \\text{otherwise} \\end{cases}",
    "text": "**Definition**: Robust loss function combining L1 and L2 losses. **Symbols**: a = error, δ = threshold. **Usage**: Regression with outliers. **Significance**: Less sensitive to outliers than MSE. **Calculation**: Quadratic for small errors, linear for large."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Backpropagation (Chain Rule)",
    "formula": "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}",
    "text": "**Definition**: Algorithm for computing gradients in neural networks. **Symbols**: L = loss, y = activation, w = weight. **Usage**: Training deep neural networks. **Significance**: Enables efficient gradient computation. **Calculation**: Multiply upstream gradient by local derivative."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "ROC-AUC Score",
    "formula": "\\text{AUC} = \\int_{0}^{1} \\text{TPR}(\\text{FPR}^{-1}(x)) dx",
    "text": "**Definition**: Area under Receiver Operating Characteristic curve. **Symbols**: TPR = True Positive Rate, FPR = False Positive Rate. **Usage**: Binary classifier evaluation. **Significance**: Threshold-independent performance measure. **Calculation**: Integrate TPR over FPR range 0-1."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Markov Chain Transition",
    "formula": "P(X_{t+1}=x | X_t=x_t) = T_{x_t,x}",
    "text": "**Definition**: Probability of transitioning between states. **Symbols**: T = transition matrix, x_t = current state. **Usage**: Sequence modeling, MCMC. **Significance**: Memoryless property. **Calculation**: Lookup in stochastic matrix."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Fourier Transform",
    "formula": "\\hat{f}(\\xi) = \\int_{-\\infty}^\\infty f(x)e^{-2\\pi i x\\xi} dx",
    "text": "**Definition**: Decomposition of function into frequency components. **Symbols**: f(x) = time function, ξ = frequency. **Usage**: Signal processing, image analysis. **Significance**: Reveals spectral content. **Calculation**: Integrate product with complex exponentials."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "PageRank",
    "formula": "PR(p_i) = \\frac{1-d}{N} + d \\sum_{p_j \\in M(p_i)} \\frac{PR(p_j)}{L(p_j)}",
    "text": "**Definition**: Algorithm ranking web pages by importance. **Symbols**: d = damping factor, L = outbound links. **Usage**: Web search ranking. **Significance**: Models random surfer behavior. **Calculation**: Iteratively distribute rank through links."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "K-Means Objective",
    "formula": "J = \\sum_{i=1}^k \\sum_{x \\in C_i} ||x - \\mu_i||^2",
    "text": "**Definition**: Sum of squared distances to cluster centroids. **Symbols**: C_i = cluster, μ_i = centroid. **Usage**: Unsupervised clustering. **Significance**: Measures cluster compactness. **Calculation**: 1) Assign points 2) Update centroids 3) Repeat."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Singular Value Decomposition",
    "formula": "A = U\\Sigma V^T",
    "text": "**Definition**: Matrix factorization into orthogonal components. **Symbols**: Σ = singular values, U/V = orthogonal matrices. **Usage**: Dimensionality reduction. **Significance**: Optimal low-rank approximation. **Calculation**: Eigen decomposition of A^TA."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Eigenvalue Equation",
    "formula": "A\\mathbf{v} = \\lambda\\mathbf{v}",
    "text": "**Definition**: Characteristic equation of linear transformations. **Symbols**: λ = eigenvalue, v = eigenvector. **Usage**: PCA, stability analysis. **Significance**: Reveals invariant directions. **Calculation**: Solve det(A - λI) = 0."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Covariance Matrix",
    "formula": "\\text{Cov}(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})",
    "text": "**Definition**: Measure of joint variability. **Symbols**: x̄ = mean, n = sample size. **Usage**: Multivariate statistics. **Significance**: Captures feature relationships. **Calculation**: Mean of centered products."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Central Limit Theorem",
    "formula": "\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)",
    "text": "**Definition**: Convergence of sample means to normality. **Symbols**: μ = population mean, σ² = variance. **Usage**: Statistical inference. **Significance**: Justifies parametric tests. **Calculation**: Sample means approximate normality as n→∞."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "L1 Regularization (Lasso)",
    "formula": "R(\\mathbf{w}) = \\lambda \\sum_{i=1}^n |w_i|",
    "text": "**Definition**: Penalization of absolute weight magnitudes. **Symbols**: w = weights, λ = strength. **Usage**: Feature selection, sparse models. **Significance**: Produces sparse solutions. **Calculation**: Sum absolute weights multiplied by λ."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Precision",
    "formula": "\\text{Precision} = \\frac{TP}{TP + FP}",
    "text": "**Definition**: Proportion of positive identifications that were correct. **Symbols**: TP = True Positives, FP = False Positives. **Usage**: Classification evaluation. **Significance**: Measures exactness. **Calculation**: Correct positives divided by all predicted positives."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Recall",
    "formula": "\\text{Recall} = \\frac{TP}{TP + FN}",
    "text": "**Definition**: Proportion of actual positives correctly identified. **Symbols**: TP = True Positives, FN = False Negatives. **Usage**: Classification evaluation. **Significance**: Measures completeness. **Calculation**: Correct positives divided by all actual positives."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Newton-Raphson Method",
    "formula": "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}",
    "text": "**Definition**: Iterative root-finding algorithm. **Symbols**: f = function, f' = derivative. **Usage**: Optimization problems. **Significance**: Quadratic convergence. **Calculation**: Update guess using function/derivative ratio."
  },
  {
    "cognitive_load_label": 3.2,
    "name": "Adam Optimizer",
    "formula": "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t \\\\ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 \\\\ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\\\ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}",
    "text": "**Definition**: Adaptive moment estimation optimizer. **Symbols**: β = decay rates, g = gradient. **Usage**: Deep learning training. **Significance**: Combines momentum and RMSprop. **Calculation**: Compute biased moments, correct bias, update parameters."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Poisson Distribution PMF",
    "formula": "P(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}",
    "text": "**Definition**: Probability of k events in fixed interval. **Symbols**: λ = rate parameter. **Usage**: Modeling rare events. **Significance**: Count-based distribution. **Calculation**: Exponentiate λ, multiply by e^{-λ}, divide by factorial."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Bernoulli Distribution",
    "formula": "P(k) = p^k(1-p)^{1-k} \\quad \\text{for } k \\in \\{0,1\\}",
    "text": "**Definition**: Discrete probability distribution of binary outcome. **Symbols**: p = success probability. **Usage**: Binary classification. **Significance**: Foundation of logistic regression. **Calculation**: p if k=1, (1-p) if k=0."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Gaussian (Normal) Distribution",
    "formula": "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}",
    "text": "**Definition**: Continuous probability distribution. **Symbols**: μ = mean, σ = std dev. **Usage**: Natural phenomena modeling. **Significance**: Central to statistical theory. **Calculation**: Exponentiate squared standardized distance."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Dot Product",
    "formula": "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i",
    "text": "**Definition**: Algebraic operation returning single number from vectors. **Symbols**: a,b = vectors, n = dimension. **Usage**: Similarity measurement. **Significance**: Fundamental linear algebra operation. **Calculation**: Multiply corresponding components and sum."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Vector Norm (L2)",
    "formula": "||\\mathbf{v}||_2 = \\sqrt{\\sum_{i=1}^n v_i^2}",
    "text": "**Definition**: Euclidean length of vector. **Symbols**: v = vector, n = dimension. **Usage**: Distance measurement. **Significance**: Standard vector magnitude. **Calculation**: Square root of sum of squared components."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Matrix Determinant",
    "formula": "\\det(A) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^n a_{i,\\sigma(i)}",
    "text": "**Definition**: Scalar value encoding matrix properties. **Symbols**: A = square matrix, σ = permutation. **Usage**: Matrix invertibility test. **Significance**: Volume scaling factor. **Calculation**: Sum of signed permutation products."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Matrix Trace",
    "formula": "\\text{Tr}(A) = \\sum_{i=1}^n A_{ii}",
    "text": "**Definition**: Sum of diagonal elements. **Symbols**: A = square matrix. **Usage**: Matrix similarity. **Significance**: Invariant under rotation. **Calculation**: Sum elements where row=column."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Jaccard Index",
    "formula": "J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}",
    "text": "**Definition**: Similarity measure between finite sets. **Symbols**: A,B = sets. **Usage**: Text mining, recommendation. **Significance**: Ignores non-shared elements. **Calculation**: Intersection size divided by union size."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Cosine Similarity",
    "formula": "\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| \\cdot ||\\mathbf{B}||}",
    "text": "**Definition**: Measure of angle between vectors. **Symbols**: A,B = vectors. **Usage**: Text similarity, NLP. **Significance**: Direction-based similarity. **Calculation**: Dot product divided by norm product."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Entropy (Information Theory)",
    "formula": "H(X) = -\\sum_{i=1}^n P(x_i) \\log P(x_i)",
    "text": "**Definition**: Measure of uncertainty in random variable. **Symbols**: P = probability mass function. **Usage**: Decision trees, compression. **Significance**: Fundamental information measure. **Calculation**: Negative sum of p*log(p)."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Kullback-Leibler Divergence",
    "formula": "D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}",
    "text": "**Definition**: Measure of difference between distributions. **Symbols**: P,Q = probability distributions. **Usage**: Variational inference. **Significance**: Non-symmetric divergence. **Calculation**: Expected log-ratio under P."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Mutual Information",
    "formula": "I(X;Y) = \\sum_{y \\in Y} \\sum_{x \\in X} P(x,y) \\log \\frac{P(x,y)}{P(x)P(y)}",
    "text": "**Definition**: Measure of dependence between variables. **Symbols**: P(x,y) = joint distribution. **Usage**: Feature selection. **Significance**: Non-negative dependence measure. **Calculation**: KL divergence between joint and product."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "ReLU Activation",
    "formula": "f(x) = \\max(0, x)",
    "text": "**Definition**: Piecewise linear activation function. **Symbols**: x = input. **Usage**: Neural networks. **Significance**: Avoids vanishing gradients. **Calculation**: Output x if positive, else 0."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Leaky ReLU",
    "formula": "f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{otherwise} \\end{cases}",
    "text": "**Definition**: Variant of ReLU with small negative slope. **Symbols**: α = small positive constant. **Usage**: Deep networks. **Significance**: Prevents dying neurons. **Calculation**: Linear pass-through with small negative slope."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "ELU Activation",
    "formula": "f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^x - 1) & \\text{otherwise} \\end{cases}",
    "text": "**Definition**: Exponential Linear Unit activation. **Symbols**: α = hyperparameter. **Usage**: Deep learning. **Significance**: Smooth transition for negatives. **Calculation**: Identity for positives, exponential for negatives."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Log Loss (Binomial)",
    "formula": "L = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log p_i + (1-y_i)\\log(1-p_i)]",
    "text": "**Definition**: Logistic regression loss function. **Symbols**: y = true label, p = predicted probability. **Usage**: Binary classification. **Significance**: Proper scoring rule. **Calculation**: Negative log likelihood of correct class."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Hinge Loss",
    "formula": "L = \\max(0, 1 - y_i(\\mathbf{w}^T\\mathbf{x}_i + b))",
    "text": "**Definition**: Loss function for maximum-margin classification. **Symbols**: y = ±1 label, w = weights. **Usage**: Support Vector Machines. **Significance**: Encourages margin maximization. **Calculation**: Zero if correct with margin, linear penalty otherwise."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Mahalanobis Distance",
    "formula": "D_M(\\mathbf{x}) = \\sqrt{(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu})}",
    "text": "**Definition**: Distance measure accounting for covariance. **Symbols**: μ = mean, Σ = covariance. **Usage**: Multivariate outlier detection. **Significance**: Scale-invariant distance. **Calculation**: Standardize using inverse covariance."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Manhattan Distance",
    "formula": "d(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^n |p_i - q_i|",
    "text": "**Definition**: Distance along axes at right angles. **Symbols**: p,q = vectors. **Usage**: Robust regression. **Significance**: Less sensitive to outliers. **Calculation**: Sum of absolute differences."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Minkowski Distance",
    "formula": "d(\\mathbf{p}, \\mathbf{q}) = \\left(\\sum_{i=1}^n |p_i - q_i|^k \\right)^{1/k}",
    "text": "**Definition**: Generalized distance metric. **Symbols**: k = order parameter. **Usage**: Parameterized distance. **Significance**: Generalizes L1/L2 distances. **Calculation**: k-th root of sum of powered differences."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Chebyshev Distance",
    "formula": "d(\\mathbf{p}, \\mathbf{q}) = \\max_i |p_i - q_i|",
    "text": "**Definition**: Maximum coordinate difference. **Symbols**: p,q = vectors. **Usage**: Chessboard distance. **Significance**: L∞ norm. **Calculation**: Greatest absolute difference."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Hamming Distance",
    "formula": "D_H = \\sum_{i=1}^n \\delta(a_i, b_i) \\quad \\text{where } \\delta(a,b) = \\begin{cases} 0 & a = b \\\\ 1 & a \\neq b \\end{cases}",
    "text": "**Definition**: Number of differing positions. **Symbols**: a,b = strings/vectors. **Usage**: Error detection. **Significance**: Simple dissimilarity measure. **Calculation**: Count mismatched positions."
  },
  {
    "cognitive_load_label": 6.2,
    "name": "R² Score (Temporal Weighted)",
    "formula": "R^2(t) = 1 - \\frac{\\sum_{i} w_i(t) (y_i(t) - \\hat{y}_i(t))^2}{\\sum_{i} w_i(t) (y_i(t) - \\bar{y}(t))^2}",
    "text": "**Definition**: Time-dependent proportion of variance explained with dynamic weighting. **Symbols**: w(t) = time-varying weights, ŷ(t) = predictions over time. **Usage**: Adaptive regression evaluation in evolving systems. **Significance**: Captures temporal fit quality, 1=perfect, 0=baseline. **Calculation**: Weighted ratio of residual to total sum of squares over time."
  },
  {
    "cognitive_load_label": 6.5,
    "name": "Adjusted R² (Regularized)",
    "formula": "R^2_{\\text{adj}}(t) = 1 - \\frac{(1 - R^2(t))(n - 1 + \\lambda \\sum_{p} \\phi_p)}{n - p - 1 + \\int_0^t \\psi(s) ds}",
    "text": "**Definition**: Regularized and time-integrated adjustment for predictor complexity. **Symbols**: λ = regularization, φ = penalty terms, ψ = time function. **Usage**: Robust model comparison in dynamic settings. **Significance**: Mitigates overfitting with temporal regularization. **Calculation**: Adjust R² with penalized degrees of freedom."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "Mean Absolute Error (Weighted Temporal)",
    "formula": "\\text{MAE}(t) = \\frac{1}{n} \\int_0^t \\sum_{i=1}^n w_i(s) |y_i(s) - \\hat{y}_i(s)| ds",
    "text": "**Definition**: Time-integrated weighted average of absolute errors. **Symbols**: w(s) = time-varying weights. **Usage**: Robust regression analysis in dynamic environments. **Significance**: Adapts to temporal outliers. **Calculation**: Integrate weighted absolute differences over time."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Mean Squared Error (Stochastic)",
    "formula": "\\text{MSE}(t) = \\frac{1}{n} \\sum_{i=1}^n E[ (y_i(t) - \\hat{y}_i(t, \\theta))^2 | \\mathcal{F}_t ]",
    "text": "**Definition**: Expected squared error under stochastic filtration. **Symbols**: E = expectation, F_t = information set. **Usage**: Probabilistic regression evaluation. **Significance**: Accounts for stochastic noise. **Calculation**: Conditional expectation of squared residuals."
  },
  {
    "cognitive_load_label": 6.6,
    "name": "Root Mean Squared Error (Adaptive)",
    "formula": "\\text{RMSE}(t) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n w_i(t) (y_i(t) - \\hat{y}_i(t))^2 + \\int_0^t \\sigma(s)^2 ds}",
    "text": "**Definition**: Adaptive RMSE with temporal variance integration. **Symbols**: σ(s) = time-varying variance. **Usage**: Dynamic error assessment. **Significance**: Incorporates evolving uncertainty. **Calculation**: Weighted MSE with variance integral."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Silhouette Coefficient (Dynamic)",
    "formula": "s_i(t) = \\frac{b_i(t) - a_i(t)}{\\max(a_i(t), b_i(t))} \\cdot \\exp\\left(-\\int_0^t \\lambda(s) ds\\right)",
    "text": "**Definition**: Time-decaying measure of cluster cohesion vs separation. **Symbols**: a(t), b(t) = dynamic distances, λ(s) = decay rate. **Usage**: Evolving clustering evaluation. **Significance**: Adjusts for temporal drift. **Calculation**: Normalized difference with exponential decay."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "Davies-Bouldin Index (Stochastic)",
    "formula": "DB(t) = \\frac{1}{k} \\sum_{i=1}^k \\max_{j \\neq i} E\\left[ \\frac{\\sigma_i(t) + \\sigma_j(t)}{d(c_i(t), c_j(t))} \\Big| \\mathcal{F}_t \\right]",
    "text": "**Definition**: Stochastic ratio of within- to between-cluster separation. **Symbols**: E = conditional expectation, F_t = filtration. **Usage**: Probabilistic clustering quality. **Significance**: Handles noisy clusters. **Calculation**: Expected worst-case ratio over time."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Rand Index (Temporal Weighted)",
    "formula": "RI(t) = \\frac{\\sum_{i,j} w_{ij}(t) (TP_{ij}(t) + TN_{ij}(t))}{\\sum_{i,j} w_{ij}(t) (TP_{ij}(t) + FP_{ij}(t) + FN_{ij}(t) + TN_{ij}(t))}",
    "text": "**Definition**: Time-weighted similarity between dynamic clusterings. **Symbols**: w(t) = weights. **Usage**: Evolving clustering validation. **Significance**: Adapts to temporal changes. **Calculation**: Weighted fraction of agreeing pairs."
  },
  {
    "cognitive_load_label": 6.6,
    "name": "Normalized Mutual Information (Conditional)",
    "formula": "NMI(t) = \\frac{I(X(t);Y(t)|Z(t))}{\\sqrt{H(X(t)|Z(t))H(Y(t)|Z(t))}}",
    "text": "**Definition**: Conditional information shared over time. **Symbols**: Z(t) = conditioning variable. **Usage**: Advanced clustering comparison. **Significance**: Accounts for dependencies. **Calculation**: Conditional MI over entropies."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "T-Distribution (Multivariate)",
    "formula": "f(\\mathbf{t}; \\nu, \\mu, \\Sigma) = \\frac{\\Gamma(\\frac{\\nu + p}{2})}{(\\pi\\nu)^{p/2} \\Gamma(\\frac{\\nu}{2}) |\\Sigma|^{1/2}} \\left(1 + \\frac{(\\mathbf{t} - \\mu)^T \\Sigma^{-1} (\\mathbf{t} - \\mu)}{\\nu} \\right)^{-\\frac{\\nu + p}{2}}",
    "text": "**Definition**: Multivariate heavy-tailed distribution. **Symbols**: μ = mean vector, Σ = covariance. **Usage**: Multi-dimensional small sample inference. **Significance**: Generalizes univariate case. **Calculation**: Multivariate gamma adjustment."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "Chi-Squared Distribution (Weighted)",
    "formula": "f(x; k, w) = \\frac{1}{2^{k/2}\\Gamma(k/2)} \\sum_{i=1}^m w_i x_i^{k/2 -1} e^{-x_i/2}",
    "text": "**Definition**: Weighted sum of squared normals. **Symbols**: w = weights. **Usage**: Adjusted goodness-of-fit tests. **Significance**: Incorporates weighted observations. **Calculation**: Weighted gamma distribution."
  },
  {
    "cognitive_load_label": 6.2,
    "name": "Z-Score (Time-Varying)",
    "formula": "z(t) = \\frac{x(t) - \\mu(t)}{\\sigma(t) + \\int_0^t \\eta(s) ds}",
    "text": "**Definition**: Time-dependent standardized deviation. **Symbols**: η(s) = drift term. **Usage**: Dynamic outlier detection. **Significance**: Adapts to evolving distributions. **Calculation**: Adjust with temporal drift."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Pearson Correlation (Multivariate)",
    "formula": "r_{XY}(t) = \\frac{\\sum_{i,j} (x_i(t) - \\bar{x}(t))(y_j(t) - \\bar{y}(t)) w_{ij}(t)}{\\sqrt{\\sum_{i,j} (x_i(t) - \\bar{x}(t))^2 w_{ij}(t)} \\sqrt{\\sum_{i,j} (y_j(t) - \\bar{y}(t))^2 w_{ij}(t)}}",
    "text": "**Definition**: Weighted multivariate correlation over time. **Symbols**: w(t) = weight matrix. **Usage**: Complex feature analysis. **Significance**: Captures dynamic relationships. **Calculation**: Weighted covariance ratio."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Spearman's Rank Correlation (Dynamic)",
    "formula": "\\rho(t) = 1 - \\frac{6 \\sum w_i(t) d_i(t)^2}{\\int_0^t n(s)(n(s)^2 - 1) ds}",
    "text": "**Definition**: Time-weighted rank correlation. **Symbols**: w(t) = weights. **Usage**: Evolving monotonic relationships. **Significance**: Adapts to rank changes. **Calculation**: Weighted rank differences over time."
  },
  {
    "cognitive_load_label": 6.5,
    "name": "Kendall's Tau (Conditional)",
    "formula": "\\tau(t) = \\frac{C(t) - D(t)}{\\binom{n(t)}{2}} \\cdot P(Z(t) | X(t), Y(t))",
    "text": "**Definition**: Conditional rank correlation with probability. **Symbols**: Z(t) = conditioning variable. **Usage**: Complex ordinal data. **Significance**: Incorporates dependencies. **Calculation**: Adjusted pair differences."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "ANOVA F-Statistic (Hierarchical)",
    "formula": "F(t) = \\frac{\\sum_{g=1}^G w_g(t) \\text{MS}_{\\text{between},g}(t)}{\\sum_{h=1}^H w_h(t) \\text{MS}_{\\text{within},h}(t) + \\int_0^t \\sigma(s)^2 ds}",
    "text": "**Definition**: Hierarchical variance ratio with temporal noise. **Symbols**: w(t) = weights, σ(s) = variance. **Usage**: Multi-level group analysis. **Significance**: Accounts for hierarchy and noise. **Calculation**: Weighted ratio with integral."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "Binomial Coefficient (Multivariate)",
    "formula": "\\binom{n_1, n_2, \\ldots, n_k}{k_1, k_2, \\ldots, k_k} = \\frac{\\prod_{i=1}^k n_i!}{\\prod_{i=1}^k k_i! (n_i - k_i)!}",
    "text": "**Definition**: Multivariate combination count. **Symbols**: n_i = item counts. **Usage**: Advanced probability. **Significance**: Generalizes binomial case. **Calculation**: Product of factorials."
  },
  {
    "cognitive_load_label":6.9,
    "name": "Taylor Series Expansion (Multivariate)",
    "formula": "f(\\mathbf{x}) = \\sum_{n=0}^\\infty \\frac{1}{n!} \\sum_{|\\alpha|=n} \\frac{D^\\alpha f(\\mathbf{a})}{\\alpha!} (\\mathbf{x} - \\mathbf{a})^\\alpha",
    "text": "**Definition**: Multivariate polynomial approximation. **Symbols**: α = multi-index. **Usage**: Complex function analysis. **Significance**: Extends to multiple dimensions. **Calculation**: Multi-indexed derivative sum."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Fourier Series (Stochastic)",
    "formula": "f(t) = a_0 + \\sum_{n=1}^\\infty E[ a_n(t) \\cos(n\\omega(t) t) + b_n(t) \\sin(n\\omega(t) t) | \\mathcal{F}_t ]",
    "text": "**Definition**: Stochastic periodic function decomposition. **Symbols**: E = expectation, F_t = filtration. **Usage**: Noisy signal analysis. **Significance**: Handles stochastic frequencies. **Calculation**: Expected sinusoidal projection."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "Laplace Transform (Fractional)",
    "formula": "F(s) = \\int_0^\\infty f(t) e^{-s t^\\alpha} dt",
    "text": "**Definition**: Fractional integral transform. **Symbols**: α = fractional order. **Usage**: Fractional differential equations. **Significance**: Generalizes classical transform. **Calculation**: Integrate with fractional exponent."
  },
  {
    "cognitive_load_label": 6.6,
    "name": "Convolution (Nonlinear)",
    "formula": "(f * g)(t) = \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) e^{-k|\\tau|} d\\tau",
    "text": "**Definition**: Nonlinear weighted convolution. **Symbols**: k = decay parameter. **Usage**: Advanced signal processing. **Significance**: Introduces temporal weighting. **Calculation**: Exponential-modified integration."
  },
  {
    "cognitive_load_label":6.4,
    "name": "Law of Total Probability (Conditional)",
    "formula": "P(A|t) = \\sum_{i=1}^n P(A | B_i, t) P(B_i|t) \\cdot e^{-\\int_0^t \\lambda(s) ds}",
    "text": "**Definition**: Time-decaying conditional probability. **Symbols**: λ(s) = decay rate. **Usage**: Dynamic Bayesian networks. **Significance**: Incorporates temporal effects. **Calculation**: Weighted conditional sum."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "Viterbi Algorithm (Stochastic)",
    "formula": "V_{t,k}(t) = \\max_{s_{t-1}} E[ V_{t-1,s}(t-1) \\cdot P(s_t=k | s_{t-1}, \\mathcal{F}_t) \\cdot P(e_t | s_t=k) | \\mathcal{F}_t ]",
    "text": "**Definition**: Stochastic dynamic programming for HMM decoding. **Symbols**: E = expectation, F_t = filtration. **Usage**: Noisy sequence alignment. **Significance**: Handles stochastic transitions. **Calculation**: Expected recursive max-product."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "Expectation-Maximization (Hierarchical E-Step)",
    "formula": "Q(\\theta|\\theta^{(t)}) = E_{Z_1,Z_2|X,\\theta^{(t)}}[\\sum_{i=1}^n \\log L(\\theta; X_i, Z_{1i}, Z_{2i})]",
    "text": "**Definition**: Hierarchical expected log-likelihood. **Symbols**: Z_1, Z_2 = nested latents. **Usage**: Complex clustering models. **Significance**: Handles multi-level data. **Calculation**: Nested expectation."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "Markov Inequality (Time-Varying)",
    "formula": "P(X(t) \\geq a(t)) \\leq \\frac{E[X(t)]}{a(t) + \\int_0^t \\mu(s) ds}",
    "text": "**Definition**: Time-adjusted upper bound. **Symbols**: μ(s) = drift term. **Usage**: Dynamic probability bounds. **Significance**: Adapts to evolving variables. **Calculation**: Adjusted mean ratio."
  },
  {
    "cognitive_load_label": 6.5,
    "name": "Chebyshev Inequality (Weighted)",
    "formula": "P(|X - \\mu| \\geq k(t) \\sigma(t)) \\leq \\frac{\\sum_{i} w_i E[X_i]^2}{k(t)^2 \\sum_{i} w_i \\sigma_i^2}",
    "text": "**Definition**: Weighted deviation bound. **Symbols**: w_i = weights. **Usage**: Robust outlier analysis. **Significance**: Incorporates weighted variance. **Calculation**: Weighted inverse square."
  },
  {
    "cognitive_load_label": 6.6,
    "name": "Hoeffding's Inequality (Multivariate)",
    "formula": "P(\\sum_{j=1}^d (\\bar{X}_j - E[\\bar{X}_j]) \\geq t) \\leq \\prod_{j=1}^d e^{-2n_j t_j^2/(b_j-a_j)^2}",
    "text": "**Definition**: Multivariate tail bound. **Symbols**: d = dimensions. **Usage**: High-dimensional learning. **Significance**: Extends concentration. **Calculation**: Product of exponentials."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Bias-Variance Tradeoff (Dynamic)",
    "formula": "E[(y(t) - \\hat{f}(t))^2 | \\mathcal{F}_t] = \\text{Bias}(\\hat{f}(t)|\\mathcal{F}_t)^2 + \\text{Var}(\\hat{f}(t)|\\mathcal{F}_t) + \\int_0^t \\sigma(s)^2 ds",
    "text": "**Definition**: Time-dependent error decomposition. **Symbols**: F_t = filtration, σ(s) = noise. **Usage**: Adaptive model selection. **Significance**: Incorporates temporal noise. **Calculation**: Conditional bias, variance, and integral."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Hamming Window (Dynamic)",
    "formula": "w(n,t) = 0.54 - 0.46 \\cos\\left(\\frac{2\\pi n}{N(t)-1} + \\phi(t)\\right)",
    "text": "**Definition**: Time-varying tapered window. **Symbols**: N(t) = dynamic length, φ(t) = phase. **Usage**: Adaptive signal processing. **Significance**: Adjusts to signal changes. **Calculation**: Dynamic cosine weighting."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "Heaviside Step Function (Smoothed)",
    "formula": "H(x,t) = \\frac{1}{1 + e^{-k(t)(x - x_0(t))}}",
    "text": "**Definition**: Smoothed time-dependent activation. **Symbols**: k(t) = steepness, x_0(t) = threshold. **Usage**: Dynamic neural models. **Significance**: Soft threshold transition. **Calculation**: Time-adjusted sigmoid."
  },
  {
    "cognitive_load_label":  6.5,
    "name": "Logistic Function (Stochastic)",
    "formula": "f(x,t) = \\frac{L(t)}{1 + E[e^{-k(t)(x-x_0(t))}|\\mathcal{F}_t]}",
    "text": "**Definition**: Stochastic growth curve with expectation. **Symbols**: E = conditional expectation. **Usage**: Probabilistic sigmoid models. **Significance**: Handles noisy growth. **Calculation**: Expected sigmoid."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "Hellinger Distance (Time-Varying)",
    "formula": "H(P(t),Q(t)) = \\frac{1}{\\sqrt{2}} \\sqrt{\\int_0^t \\sum_{i=1}^k (\\sqrt{p_i(s)} - \\sqrt{q_i(s)})^2 w(s) ds}",
    "text": "**Definition**: Time-integrated distribution distance. **Symbols**: w(s) = weight function. **Usage**: Dynamic probability metrics. **Significance**: Captures temporal divergence. **Calculation**: Weighted integral of differences."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Bhattacharyya Coefficient (Conditional)",
    "formula": "BC(P,Q|t) = \\sum_{i=1}^n \\sqrt{p_i(t) q_i(t)} \\cdot P(Z(t)|X(t), Y(t))",
    "text": "**Definition**: Conditional overlap measure. **Symbols**: Z(t) = conditioning variable. **Usage**: Adaptive feature selection. **Significance**: Adjusts for context. **Calculation**: Weighted geometric means."
  },
  {
    "cognitive_load_label":6.9,
    "name": "Wasserstein Distance (Regularized)",
    "formula": "W_p(P,Q,t) = \\left( \\inf_{\\gamma \\in \\Gamma(P,Q)} \\int (d(x,y)^p + \\lambda(t) R(x,y,t)) d\\gamma(x,y) \\right)^{1/p}",
    "text": "**Definition**: Regularized transport cost over time. **Symbols**: λ(t) = time-varying penalty. **Usage**: Dynamic GAN training. **Significance**: Balances cost and smoothness. **Calculation**: Optimize with temporal regularization."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Jensen-Shannon Divergence (Dynamic)",
    "formula": "JSD(P(t)||Q(t)) = \\frac{1}{2} \\int_0^t D_{KL}(P(s)||M(s)) w(s) ds + \\frac{1}{2} \\int_0^t D_{KL}(Q(s)||M(s)) w(s) ds",
    "text": "**Definition**: Time-weighted symmetrized divergence. **Symbols**: M(s) = midpoint, w(s) = weight. **Usage**: Evolving distribution comparison. **Significance**: Integrates temporal changes. **Calculation**: Weighted KL integral."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "Hausdorff Distance (Stochastic)",
    "formula": "d_H(X(t),Y(t)) = \\max\\left\\{ E\\left[ \\sup_{x \\in X(t)} \\inf_{y \\in Y(t)} d(x,y) | \\mathcal{F}_t \\right], E\\left[ \\sup_{y \\in Y(t)} \\inf_{x \\in X(t)} d(x,y) | \\mathcal{F}_t \\right] \\right\\}",
    "text": "**Definition**: Stochastic maximum mismatch. **Symbols**: E = expectation, F_t = filtration. **Usage**: Noisy image segmentation. **Significance**: Handles stochastic sets. **Calculation**: Expected supremum of distances."
  },
  {
    "cognitive_load_label":6.8,
    "name": "Dynamic Time Warping (Weighted)",
    "formula": "DTW(Q,C,t) = \\min_{\\pi(t)} \\int_0^t \\sum_{(i,j) \\in \\pi(s)} w(s) d(q_i(s), c_j(s)) ds",
    "text": "**Definition**: Time-integrated weighted sequence alignment. **Symbols**: w(s) = time weight. **Usage**: Adaptive time series analysis. **Significance**: Adjusts for temporal variation. **Calculation**: Weighted path integral."
  },
  {
    "cognitive_load_label": 6.6,
    "name": "Levenshtein Distance (Probabilistic)",
    "formula": "\\text{lev}_{a,b}(i,j,t) = \\begin{cases} \\max(i,j) & \\min(i,j)=0 \\\\ \\min \\begin{cases} \\text{lev}(i-1,j,t) + P(\\text{insert}|t) \\\\ \\text{lev}(i,j-1,t) + P(\\text{delete}|t) \\\\ \\text{lev}(i-1,j-1,t) + P(\\text{substitute}|t) \\end{cases} & \\text{otherwise} \\end{cases}",
    "text": "**Definition**: Probabilistic edit distance over time. **Symbols**: P = time-varying probabilities. **Usage**: Dynamic string alignment. **Significance**: Incorporates uncertainty. **Calculation**: Probabilistic recursive alignment."
  },
  {
    "cognitive_load_label": 6.5,
    "name": "Dijkstra's Algorithm (Stochastic Weights)",
    "formula": "d[v](t) = \\min(d[v](t), E[d[u](t) + w(u,v,t) | \\mathcal{F}_t])",
    "text": "**Definition**: Stochastic shortest paths with expectation. **Symbols**: w(t) = random weights. **Usage**: Noisy routing systems. **Significance**: Handles uncertainty. **Calculation**: Expected edge relaxation."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Bellman-Ford Algorithm (Dynamic)",
    "formula": "d[v](t) = \\min(d[v](t), d[u](t) + w(u,v,t)) \\quad \\forall (u,v) \\in E(t)",
    "text": "**Definition**: Time-varying shortest paths. **Symbols**: E(t) = dynamic edges. **Usage**: Evolving networks. **Significance**: Adapts to changes. **Calculation**: Relax edges over time."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Floyd-Warshall Algorithm (Stochastic)",
    "formula": "d_{ij}^{(k)}(t) = \\min(d_{ij}^{(k-1)}(t), E[d_{ik}^{(k-1)}(t) + d_{kj}^{(k-1)}(t) | \\mathcal{F}_t])",
    "text": "**Definition**: Stochastic all-pairs shortest paths. **Symbols**: E = expectation. **Usage**: Noisy network analysis. **Significance**: Handles random distances. **Calculation**: Expected path relaxation."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "Prim's Algorithm (Weighted Dynamic)",
    "formula": "\\text{Select edge with min } E[w(u,v,t) | \\mathcal{F}_t] \\text{ connecting tree to non-tree vertex}",
    "text": "**Definition**: Dynamic minimum spanning tree with expectation. **Symbols**: w(t) = weights. **Usage**: Evolving network design. **Significance**: Adapts to changes. **Calculation**: Expected minimum edge."
  },
  {
    "cognitive_load_label": 6.4,
    "name": "Kruskal's Algorithm (Stochastic)",
    "formula": "\\text{Sort edges by } E[w(u,v,t) | \\mathcal{F}_t], \\text{ add next edge that doesn't form cycle}",
    "text": "**Definition**: Stochastic minimum spanning tree. **Symbols**: w(t) = random weights. **Usage**: Noisy cluster analysis. **Significance**: Handles uncertainty. **Calculation**: Expected edge sorting."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "Karatsuba Multiplication (Recursive)",
    "formula": "xy = \\sum_{k=0}^{2n-1} \\left( \\sum_{m=0}^k a_{n-1-m} c_m \\cdot 10^{2n-1-k} + (a_{n-1-m} d_m + b_{n-1-m} c_m) \\cdot 10^{n-1-k} + b_{n-1-m} d_m \\cdot 10^{-k} \\right)",
    "text": "**Definition**: Recursive fast multiplication. **Symbols**: a,b,c,d = digit splits. **Usage**: High-precision arithmetic. **Significance**: Extends divide-and-conquer. **Calculation**: Nested summations."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "Strassen's Algorithm (Stochastic)",
    "formula": "\\begin{bmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{bmatrix} = \\begin{bmatrix} E[M_1 + M_4 - M_5 + M_7 | \\mathcal{F}_t] + \\epsilon_1 & E[M_3 + M_5 | \\mathcal{F}_t] + \\epsilon_2 \\\\ E[M_2 + M_4 | \\mathcal{F}_t] + \\epsilon_3 & E[M_1 - M_2 + M_3 + M_6 | \\mathcal{F}_t] + \\epsilon_4 \\end{bmatrix}",
    "text": "**Definition**: Stochastic fast matrix multiplication. **Symbols**: E = expectation, ε = noise. **Usage**: Noisy linear algebra. **Significance**: Handles perturbations. **Calculation**: Expected submatrix operations."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Monte Carlo Integration (Stratified)",
    "formula": "\\int_a^b f(x)dx \\approx \\sum_{l=1}^L \\frac{b-a}{L} \\frac{1}{N_l} \\sum_{i=1}^{N_l} f(x_{li})",
    "text": "**Definition**: Stratified random sampling integration. **Symbols**: L = strata, N_l = samples per stratum. **Usage**: High-precision integrals. **Significance**: Reduces variance. **Calculation**: Weighted stratum averages."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "Metropolis-Hastings Algorithm (Adaptive)",
    "formula": "A(x'|x,t) = \\min\\left(1, \\frac{P(x'|t)Q(x|x',t)}{P(x|t)Q(x'|x,t)} \\cdot e^{-\\int_0^t \\lambda(s) ds}\\right)",
    "text": "**Definition**: Adaptive MCMC with time decay. **Symbols**: λ(s) = decay rate. **Usage**: Dynamic Bayesian sampling. **Significance**: Adjusts to evolving distributions. **Calculation**: Time-decayed acceptance ratio."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "Batch Normalization (Temporal)",
    "formula": "\\hat{x}_i(t) = \\frac{x_i(t) - E[\\mu_B(t)|\\mathcal{F}_t]}{\\sqrt{E[\\sigma_B^2(t)|\\mathcal{F}_t] + \\epsilon}} \\\\ y_i(t) = \\gamma(t) \\hat{x}_i(t) + \\beta(t)",
    "text": "**Definition**: Time-varying layer normalization. **Symbols**: E = expectation, F_t = filtration. **Usage**: Dynamic neural networks. **Significance**: Adapts to temporal data. **Calculation**: Expected statistics with scaling."
  },
  {
    "cognitive_load_label": 6.7,
    "name": "Attention Mechanism (Multi-Head)",
    "formula": "\\text{Attention}(Q, K, V) = \\sum_{h=1}^H \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}}\\right)V_h",
    "text": "**Definition**: Multi-head context retrieval. **Symbols**: H = heads. **Usage**: Advanced transformers. **Significance**: Captures diverse patterns. **Calculation**: Sum of head-specific attentions."
  },
  {
    "cognitive_load_label": 6.5,
    "name": "Transformer Positional Encoding (Adaptive)",
    "formula": "PE_{(pos,t,2i)} = \\sin(pos/10000^{2i/d(t)}) \\cdot e^{-\\int_0^t \\lambda(s) ds} \\\\ PE_{(pos,t,2i+1)} = \\cos(pos/10000^{2i/d(t)}) \\cdot e^{-\\int_0^t \\lambda(s) ds}",
    "text": "**Definition**: Adaptive position injection with decay. **Symbols**: d(t) = dynamic dimension. **Usage**: Evolving sequence models. **Significance**: Adjusts order awareness. **Calculation**: Decayed sine/cosine waves."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "XGBoost Loss Function (Hierarchical)",
    "formula": "\\mathcal{L}(t) = \\sum_{i=1}^n l(y_i(t), \\hat{y}_i(t)) + \\sum_{k=1}^K \\sum_{l=1}^L \\Omega(f_{kl}(t))",
    "text": "**Definition**: Hierarchical regularized boosting objective. **Symbols**: L = levels. **Usage**: Multi-level gradient boosting. **Significance**: Controls complex overfitting. **Calculation**: Nested regularization."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "ResNet Residual Block (Stochastic)",
    "formula": "\\mathbf{y}(t) = E[\\mathcal{F}(\\mathbf{x}(t), \\{W_i(t)\\}) | \\mathcal{F}_t] + W_s(t) \\mathbf{x}(t) + \\epsilon(t)",
    "text": "**Definition**: Stochastic skip connection block. **Symbols**: ε(t) = noise. **Usage**: Noisy deep CNNs. **Significance**: Robust to perturbations. **Calculation**: Expected mapping plus noise."
  },
  {
    "cognitive_load_label": 6.8,
    "name": "GAN Loss Function (Conditional)",
    "formula": "\\min_G \\max_D V(D, G, z) = E_{x\\sim p_{data}}[\\log D(x|z)] + E_{z\\sim p_z}[\\log(1 - D(G(z|x)))]",
    "text": "**Definition**: Conditional adversarial objective. **Symbols**: z = conditioning. **Usage**: Context-aware generative modeling. **Significance**: Enhances generation quality. **Calculation**: Conditional min-max game."
  },
  {
    "cognitive_load_label": 6.9,
    "name": "VAE ELBO (Temporal)",
    "formula": "\\log p(x(t)) \\geq E_{z(t)\\sim q(t)}[\\log p(x(t)|z(t))] - \\int_0^t D_{KL}(q(z(s)|x(s)) || p(z(s))) ds",
    "text": "**Definition**: Time-integrated variational bound. **Symbols**: q(t) = dynamic encoder. **Usage**: Temporal latent models. **Significance**: Captures evolution. **Calculation**: Integrated reconstruction and KL."
  },
  {
    "cognitive_load_label": 6.3,
    "name": "McCulloch-Pitts Neuron (Weighted)",
    "formula": "y(t) = \\begin{cases} 1 & \\text{if } \\sum_{i} w_i(t) x_i(t) \\geq \\theta(t) \\\\ 0 & \\text{otherwise} \\end{cases}",
    "text": "**Definition**: Time-varying weighted binary neuron. **Symbols**: w(t) = weights. **Usage**: Dynamic neural foundations. **Significance**: Adapts threshold logic. **Calculation**: Weighted sum comparison."
  },
  {
    "name": "GAT Attention (Time-Varying)",
    "formula": "\\alpha_{ij}(t) = \\frac{\\exp(\\text{LeakyReLU}(\\mathbf{a}(t)^T [W(t) h_i(t) || W(t) h_j(t)]))}{\\sum_{k \\in \\mathcal{N}_i(t)} \\exp(\\text{LeakyReLU}(\\mathbf{a}(t)^T [W(t) h_i(t) || W(t) h_k(t)]))}",
    "text": "**Definition**: Time-varying graph attention coefficients for dynamic networks. **Symbols**: a(t) = time-dependent attention vector, W(t) = time-varying weight matrix, h(t) = node features over time. **Usage**: Dynamic graph attention networks in evolving systems. **Significance**: Adapts edge importance to temporal changes. **Calculation**: Compute normalized attention scores with temporal adjustments over dynamic neighborhoods.",
    "cognitive_load_label": 4.2
  },
  {
    "name": "Diffusion Equation (Anisotropic)",
    "formula": "\\frac{\\partial u}{\\partial t} = \\nabla \\cdot (D(x) \\nabla u) + \\epsilon \\frac{\\partial^2 u}{\\partial t^2}",
    "text": "**Definition**: Anisotropic PDE with second-order temporal effects for quantity spreading. **Symbols**: D(x) = spatially varying diffusion coefficient, ε = temporal regularization. **Usage**: Advanced image processing and physics simulations. **Significance**: Models complex diffusion with inertia. **Calculation**: Solve with spatial and temporal derivatives.",
    "cognitive_load_label": 4.5
  },
  {
    "name": "Normalizing Flow (Conditional)",
    "formula": "p_X(x|z) = p_Z(f^{-1}(x|z)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x|z)}{\\partial x} \\right) \\right|",
    "text": "**Definition**: Conditional invertible density transformation for generative modeling. **Symbols**: z = conditioning variable, f = conditional bijective function. **Usage**: Conditional generative models. **Significance**: Enables context-aware likelihood computation. **Calculation**: Apply change of variables with conditioning.",
    "cognitive_load_label": 4.4
  },
  {
    "name": "Neural ODE (Parameterized)",
    "formula": "\\frac{dh(t)}{dt} = f_\\theta(h(t), t, \\lambda) + \\int_0^t g(h(s)) ds",
    "text": "**Definition**: Parameterized continuous-depth network with integral term. **Symbols**: λ = additional parameter, g = auxiliary function. **Usage**: Complex time series and control systems. **Significance**: Enhances adaptive computation with memory. **Calculation**: Solve ODE with integrated history.",
    "cognitive_load_label": 4.6
  },
  {
    "name": "SVM Dual Problem (Weighted Kernel)",
    "formula": "\\max_\\alpha \\sum_i w_i \\alpha_i - \\frac{1}{2} \\sum_{i,j} w_i w_j \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)",
    "text": "**Definition**: Weighted kernelized optimization with sample importance. **Symbols**: w_i = sample weights, α = Lagrange multipliers. **Usage**: Robust support vector machines. **Significance**: Incorporates data importance in kernel trick. **Calculation**: Solve weighted quadratic programming.",
    "cognitive_load_label": 4.3
  },
  {
    "name": "Optimal Transport (Regularized)",
    "formula": "W_p(\\mu, \\nu) = \\left( \\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\int (d(x,y)^p + \\lambda R(x,y)) d\\gamma(x,y) \\right)^{1/p}",
    "text": "**Definition**: Regularized minimal transport cost between distributions. **Symbols**: λ = regularization parameter, R = regularization term. **Usage**: Stabilized generative models. **Significance**: Balances cost and smoothness. **Calculation**: Optimize with regularized linear programming.",
    "cognitive_load_label": 4.5
  },
  {
    "name": "Sinkhorn Iteration (Adaptive)",
    "formula": "K(t) = e^{-C(t)/\\epsilon(t)}, \\quad u(t) \\leftarrow \\frac{a(t)}{K(t)v(t)}, \\quad v(t) \\leftarrow \\frac{b(t)}{K(t)^T u(t)}",
    "text": "**Definition**: Adaptive efficient transport approximation over time. **Symbols**: C(t) = time-varying cost, ε(t) = dynamic regularization. **Usage**: Temporal scalable optimal transport. **Significance**: Adapts to changing data distributions. **Calculation**: Alternate normalization with time dependence.",
    "cognitive_load_label": 4.4
  },
  {
    "name": "Beta Distribution (Time-Dependent)",
    "formula": "f(x,t; \\alpha(t), \\beta(t)) = \\frac{x^{\\alpha(t)-1}(1-x)^{\\beta(t)-1}}{B(\\alpha(t), \\beta(t))}",
    "text": "**Definition**: Time-dependent distribution over [0,1] interval. **Symbols**: α(t), β(t) = time-varying parameters. **Usage**: Dynamic Bayesian inference. **Significance**: Models evolving probabilities. **Calculation**: Normalize with time-dependent beta function.",
    "cognitive_load_label": 4.2
  },
  {
    "name": "Dirichlet Distribution (Weighted)",
    "formula": "f(\\mathbf{x}; \\alpha, w) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^K w_i x_i^{\\alpha_i - 1}",
    "text": "**Definition**: Weighted multivariate generalization of beta. **Symbols**: w_i = weight factors. **Usage**: Weighted topic modeling. **Significance**: Adjusts simplex distribution. **Calculation**: Weighted multivariate gamma ratio.",
    "cognitive_load_label": 4.3
  },
  {
    "name": "Gamma Function (Parameterized)",
    "formula": "\\Gamma(z, \\lambda) = \\int_0^\\infty x^{z-1} e^{-\\lambda x} dx",
    "text": "**Definition**: Parameterized complex factorial extension. **Symbols**: λ = scaling parameter. **Usage**: Adjusted probability distributions. **Significance**: Generalizes special function. **Calculation**: Scaled Euler integral.",
    "cognitive_load_label": 4.5
  },
  {
    "name": "Bessel Function (Generalized Order)",
    "formula": "J_{n(t)}(x) = \\frac{1}{\\pi} \\int_0^\\pi \\cos(n(t)\\tau - x \\sin \\tau + \\phi(t)) d\\tau",
    "text": "**Definition**: Generalized order solution to Bessel's equation. **Symbols**: n(t) = time-varying order, φ(t) = phase shift. **Usage**: Dynamic wave propagation. **Significance**: Adapts to changing harmonics. **Calculation**: Integral with dynamic terms.",
    "cognitive_load_label": 4.6
  },
  {
    "name": "Legendre Transform (Multivariate)",
    "formula": "f^*(p_1, p_2) = \\sup_{x_1, x_2} (p_1 x_1 + p_2 x_2 - f(x_1, x_2))",
    "text": "**Definition**: Multivariate convex conjugate function. **Symbols**: p_1, p_2 = multiple slopes. **Usage**: Multi-dimensional optimization. **Significance**: Extends duality. **Calculation**: Maximize over multiple variables.",
    "cognitive_load_label": 4.4
  },
  {
    "name": "Hamiltonian Mechanics (Damped)",
    "formula": "\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q} - \\gamma p, \\quad \\frac{dq}{dt} = \\frac{\\partial H}{\\partial p}",
    "text": "**Definition**: Damped reformulation of Newtonian mechanics. **Symbols**: γ = damping coefficient. **Usage**: Dissipative physics simulation. **Significance**: Includes energy loss. **Calculation**: Solve damped coupled PDEs.",
    "cognitive_load_label": 4.5
  },
  {
    "name": "Lagrangian Mechanics (Forced)",
    "formula": "\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = F(q, t)",
    "text": "**Definition**: Forced energy-based mechanics formulation. **Symbols**: F = external force. **Usage**: Driven constrained systems. **Significance**: Accounts for external effects. **Calculation**: Euler-Lagrange with forcing term.",
    "cognitive_load_label": 4.3
  },
  {
    "name": "Noether's Theorem (Time-Dependent)",
    "formula": "\\frac{\\partial L}{\\partial \\dot{q}_i} \\frac{\\partial \\phi_i(t)}{\\partial s} = \\text{constant}(t)",
    "text": "**Definition**: Time-dependent symmetry to conservation laws. **Symbols**: φ(t) = time-varying transformation. **Usage**: Dynamic theoretical physics. **Significance**: Evolves conserved quantities. **Calculation**: Adjust constants over time.",
    "cognitive_load_label": 4.4
  },
  {
    "name": "Boltzmann Distribution (Spatial)",
    "formula": "p_i(x) = \\frac{e^{-\\epsilon_i(x)/kT}}{\\int_{\\Omega} e^{-\\epsilon_j(x)/kT} dx}",
    "text": "**Definition**: Spatial probability of states in equilibrium. **Symbols**: Ω = spatial domain. **Usage**: Spatial statistical mechanics. **Significance**: Incorporates spatial energy. **Calculation**: Spatial exponential weighting.",
    "cognitive_load_label": 4.5
  },
  {
    "name": "Gibbs Sampling (Multivariate)",
    "formula": "p(x_i|x_{-i}, \\theta) = \\frac{p(x, \\theta)}{\\int p(x, \\theta) dx_i}",
    "text": "**Definition**: Multivariate MCMC with parameters. **Symbols**: θ = model parameters. **Usage**: Complex Bayesian inference. **Significance**: Incorporates parameter uncertainty. **Calculation**: Conditional updates with parameters.",
    "cognitive_load_label": 4.6
  },
  {
    "name": "Metropolis-Adjusted Langevin (Adaptive)",
    "formula": "x_{t+1} = x_t + \\epsilon(t) \\nabla \\log p(x_t) + \\sqrt{2\\epsilon(t)} z_t",
    "text": "**Definition**: Adaptive gradient-based MCMC. **Symbols**: ε(t) = time-varying step size. **Usage**: Dynamic high-dimensional sampling. **Significance**: Optimizes exploration. **Calculation**: Adaptive Langevin with MH correction.",
    "cognitive_load_label": 4.4
  },
  {
    "name": "Hamiltonian Monte Carlo (Stochastic)",
    "formula": "\\frac{d\\theta}{dt} = M^{-1} r + \\eta(t), \\quad \\frac{dr}{dt} = -\\nabla U(\\theta) + \\zeta(t)",
    "text": "**Definition**: Stochastic physics-inspired sampling. **Symbols**: η(t), ζ(t) = noise terms. **Usage**: Noisy Bayesian inference. **Significance**: Handles stochastic dynamics. **Calculation**: Simulate noisy Hamiltonian dynamics.",
    "cognitive_load_label": 4.5
  },
  {
    "name": "Variational Inference ELBO (Regularized)",
    "formula": "\\log p(x) \\geq E_{z\\sim q}[\\log p(x,z) - \\log q(z)] + \\lambda R(q)",
    "text": "**Definition**: Regularized evidence lower bound. **Symbols**: R(q) = regularization term. **Usage**: Stabilized Bayesian inference. **Significance**: Prevents overfitting. **Calculation**: Add regularization to ELBO.",
    "cognitive_load_label": 4.6
  },
  {
    "name": "Stein Variational Gradient (Dynamic)",
    "formula": "\\phi^*(t, \\cdot) = E_{z\\sim q(t)}[k(z,t,\\cdot)\\nabla_z \\log p(z,t) + \\nabla_z k(z,t,\\cdot)]",
    "text": "**Definition**: Dynamic particle-based variational inference. **Symbols**: k(t) = time-varying kernel. **Usage**: Temporal non-parametric VI. **Significance**: Adapts gradient flow. **Calculation**: Dynamic kernel-weighted expectations.",
    "cognitive_load_label": 4.7
  },
    {
      "name": "Hoeffding's Inequality (Multi-Sample)",
      "formula": "P(\\sum_{k=1}^K (\\bar{X}_k - E[\\bar{X}_k]) \\geq t) \\leq e^{-2nt^2 / \\sum_{k=1}^K (b_k - a_k)^2}",
      "text": "**Definition**: Probability bound for multiple bounded random variables. **Symbols**: [a_k, b_k] = value range per sample, K = number of samples. **Usage**: Advanced statistical learning. **Significance**: Multi-sample concentration bound. **Calculation**: Sum errors across samples, apply exponential decay.",
      "cognitive_load_label": 4.8
    },
    {
      "name": "VC Dimension (Shattered Subsets)",
      "formula": "\\text{VC}(\\mathcal{H}) = \\max\\{m : \\sum_{S \\in \\mathcal{S}_m} \\Pi_\\mathcal{H}(S) = 2^m\\}",
      "text": "**Definition**: Complexity measure via subset shattering. **Symbols**: S = subset, ℋ = hypothesis class. **Usage**: Theoretical ML analysis. **Significance**: Quantifies model capacity. **Calculation**: Sum shattering coefficients over subsets.",
      "cognitive_load_label": 4.6
    },
    {
      "name": "Rademacher Complexity (Weighted)",
      "formula": "\\hat{R}_n(\\mathcal{F}) = E_\\sigma[\\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n w_i \\sigma_i f(x_i)]",
      "text": "**Definition**: Weighted measure of function class richness. **Symbols**: w_i = weights, σ = ±1 labels. **Usage**: Generalization bounds. **Significance**: Data-dependent measure. **Calculation**: Weighted expectation of supremum.",
      "cognitive_load_label": 4.7
    },
    {
      "name": "EM Algorithm (Weighted Latent)",
      "formula": "Q(\\theta|\\theta^{(t)}) = E_{Z|X,\\theta^{(t)}}[\\sum_{i=1}^N w_i \\log p(X_i, Z_i|\\theta)]",
      "text": "**Definition**: Weighted iterative method for latent models. **Symbols**: w_i = sample weights. **Usage**: GMMs with weights. **Significance**: Handles weighted incomplete data. **Calculation**: Weighted expectation maximization.",
      "cognitive_load_label": 4.9
    },
    {
      "name": "Perceptron Learning Rule (Adaptive)",
      "formula": "w_{t+1} = w_t + \\eta_t (y_i - \\hat{y}_i) x_i / ||x_i||",
      "text": "**Definition**: Adaptive online classifier update. **Symbols**: η_t = adaptive learning rate. **Usage**: Neural network training. **Significance**: Faster convergence. **Calculation**: Normalize input before weight update.",
      "cognitive_load_label": 4.5
    },
    {
      "name": "Radial Basis Function (Multi-Centers)",
      "formula": "\\phi(\\mathbf{x}) = \\sum_{j=1}^M e^{-\\gamma_j ||\\mathbf{x} - \\mathbf{c}_j||^2}",
      "text": "**Definition**: Kernel function with multiple centers. **Symbols**: M = number of centers. **Usage**: Advanced kernel methods. **Significance**: Enhanced flexibility. **Calculation**: Sum over multiple RBFs.",
      "cognitive_load_label": 4.8
    },
    {
      "name": "Isomap Embedding (Weighted Graph)",
      "formula": "d_G(i,j) = \\min_P \\sum_{k=1}^{|P|-1} w_{k,k+1} d_X(p_k, p_{k+1})",
      "text": "**Definition**: Weighted geodesic distance approximation. **Symbols**: w = edge weights. **Usage**: Nonlinear reduction. **Significance**: Weighted manifold structure. **Calculation**: Weighted shortest path.",
      "cognitive_load_label": 4.7
    },
    {
      "name": "t-SNE Gradient (Regularized)",
      "formula": "\\frac{\\partial C}{\\partial y_i} = 4\\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + \\lambda ||y_i - y_j||^2)^{-1}",
      "text": "**Definition**: Regularized visualization loss gradient. **Symbols**: λ = regularization parameter. **Usage**: Enhanced visualization. **Significance**: Stabilized gradients. **Calculation**: Add regularization term.",
      "cognitive_load_label": 4.9
    },
    {
      "name": "Laplacian Eigenmaps (Regularized)",
      "formula": "L = D - W + \\lambda I \\quad \\text{(Regularized Graph Laplacian)}",
      "text": "**Definition**: Regularized manifold learning. **Symbols**: λ = regularization term. **Usage**: Improved dimensionality reduction. **Significance**: Prevents degeneracy. **Calculation**: Add identity matrix term.",
      "cognitive_load_label": 4.6
    },
    {
      "name": "Locally Linear Embedding (Sparse)",
      "formula": "\\min_W \\sum_i ||x_i - \\sum_{j \\in \\mathcal{N}_i} W_{ij}x_j||^2 \\quad \\text{s.t.} \\sum_{j \\in \\mathcal{N}_i} W_{ij} = 1",
      "text": "**Definition**: Sparse neighborhood embedding. **Symbols**: N_i = sparse neighbors. **Usage**: Efficient manifold learning. **Significance**: Reduced computation. **Calculation**: Constrain to sparse neighbors.",
      "cognitive_load_label": 4.7
    },
    {
      "name": "Word2Vec Skip-Gram (Contextual)",
      "formula": "p(w_{i+j}|w_i, c) = \\frac{\\exp(v_{w_{i+j}}^T (v_{w_i} + v_c))}{\\sum_{w=1}^V \\exp(v_w^T (v_{w_i} + v_c))}",
      "text": "**Definition**: Contextual word embedding model. **Symbols**: v_c = context vector. **Usage**: Enhanced NLP. **Significance**: Captures context. **Calculation**: Add context vector to softmax.",
      "cognitive_load_label": 4.8
    },
    {
      "name": "GloVe Objective (Regularized)",
      "formula": "J = \\sum_{i,j=1}^V f(X_{ij})(w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2 + \\lambda ||w||^2",
      "text": "**Definition**: Regularized word vector optimization. **Symbols**: λ = regularization. **Usage**: Stable word embeddings. **Significance**: Prevents overfitting. **Calculation**: Add regularization term.",
      "cognitive_load_label": 4.7
    },
    {
      "name": "Transformer Layer Normalization (Adaptive)",
      "formula": "\\text{LayerNorm}(x) = \\gamma_t \\frac{x - \\mu_t}{\\sqrt{\\sigma_t^2 + \\epsilon}} + \\beta_t",
      "text": "**Definition**: Adaptive feature normalization. **Symbols**: t = time step. **Usage**: Dynamic transformers. **Significance**: Adapts to input. **Calculation**: Time-dependent parameters.",
      "cognitive_load_label": 4.6
    },
    {
      "name": "BERT Masked LM (Weighted)",
      "formula": "p(w_m|w_{\\backslash m}) = \\frac{\\exp(w_m h_m^T e_{w_m})}{\\sum_{v=1}^V \\exp(w_v h_m^T e_v)}",
      "text": "**Definition**: Weighted masked word prediction. **Symbols**: w_m = weight. **Usage**: Improved pretraining. **Significance**: Focus on key tokens. **Calculation**: Weighted softmax.",
      "cognitive_load_label": 4.8
    },
    {
      "name": "Gradient Clipping (Dynamic)",
      "formula": "\\text{grad} = \\begin{cases} \\text{grad} & \\text{if } ||\\text{grad}|| \\leq \\tau_t \\\\ \\tau_t \\frac{\\text{grad}}{||\\text{grad}||} & \\text{otherwise} \\end{cases}",
      "text": "**Definition**: Dynamic gradient norm constraint. **Symbols**: τ_t = dynamic threshold. **Usage**: Stable RNN training. **Significance**: Adaptive clipping. **Calculation**: Adjust threshold per step.",
      "cognitive_load_label": 4.5
    },
    {
      "name": "Dropout Regularization (Layer-wise)",
      "formula": "y_l = \\begin{cases} \\frac{x_l}{1-p_l} & \\text{with probability } 1-p_l \\\\ 0 & \\text{otherwise} \\end{cases}",
      "text": "**Definition**: Layer-wise neuron deactivation. **Symbols**: p_l = layer dropout rate. **Usage**: Deep network regularization. **Significance**: Layer-specific dropout. **Calculation**: Apply per layer.",
      "cognitive_load_label": 4.6
    },
    {
      "name": "Early Stopping (Smoothed)",
      "formula": "\\hat{k} = \\arg\\min_k \\text{Smooth}(\\hat{L}_k) \\quad \\text{where} \\quad \\hat{L}_k = L(\\theta_k, \\mathcal{D}_{val})",
      "text": "**Definition**: Smoothed validation performance termination. **Symbols**: Smooth = smoothing function. **Usage**: Robust training. **Significance**: Reduces noise. **Calculation**: Apply smoothing to loss.",
      "cognitive_load_label": 4.7
    },
    {
      "name": "Knowledge Distillation (Multi-Teacher)",
      "formula": "L = \\alpha L_{task} + (1-\\alpha) \\sum_{i=1}^M T_i^2 L_{distill,i}",
      "text": "**Definition**: Multi-teacher model training. **Symbols**: M = number of teachers. **Usage**: Advanced compression. **Significance**: Combines multiple teachers. **Calculation**: Sum over teacher losses.",
      "cognitive_load_label": 4.9
    },
    {
      "name": "Neural Tangent Kernel (Parameterized)",
      "formula": "\\Theta(x,x'; \\lambda) = E_{\\theta \\sim p}[\\langle \\nabla_\\theta f(x;\\theta, \\lambda), \\nabla_\\theta f(x';\\theta, \\lambda) \\rangle]",
      "text": "**Definition**: Parameterized infinite-width kernel. **Symbols**: λ = kernel parameter. **Usage**: Advanced NN analysis. **Significance**: Flexible kernel. **Calculation**: Parameterized gradient product.",
      "cognitive_load_label": 4.8
    },
    {
      "name": "Graph Convolution Layer (Multi-Hop)",
      "formula": "H^{(l+1)} = \\sigma(\\sum_{k=1}^K (\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2})^k H^{(l)}W^{(l)})",
      "text": "**Definition**: Multi-hop message passing. **Symbols**: K = number of hops. **Usage**: Deep GNNs. **Significance**: Captures long-range dependencies. **Calculation**: Sum over K hops.",
      "cognitive_load_label": 4.9
    },
    {
      "name": "Hoeffding's Inequality (Generalized Integral Form)",
      "formula": "P(\\int_0^1 (\\bar{X}(s) - E[\\bar{X}(s)]) ds \\geq t) \\leq \\int_0^1 e^{-2n(s)t^2/(b(s)-a(s))^2} ds",
      "text": "Generalized probability bound for continuous bounded processes. [a(s), b(s)] = time-varying range, n(s) = sample density. Used in advanced stochastic processes. Ensures exponential concentration over continuum. Integrate exponential decay over time.",
      "cognitive_load_label": 7.8
    },
    {
      "name": "VC Dimension (Functional Dependency)",
      "formula": "\\text{VC}(\\mathcal{H}, \\mathcal{D}) = \\max\\{m : \\int_{\\mathcal{D}} \\Pi_\\mathcal{H}(m, d) d\\mu(d) = 2^m\\}",
      "text": "Functional complexity measure with data dependency. ℋ = hypothesis class, D = data distribution, μ = measure. Critical in theoretical ML for distribution-dependent learnability. Integrate shattering over distribution.",
      "cognitive_load_label": 7.6
    },
    {
      "name": "Rademacher Complexity (Functional Space)",
      "formula": "\\hat{R}_n(\\mathcal{F}) = E_\\sigma[\\sup_{f \\in \\mathcal{F}} \\int_0^1 \\sigma(s) f(x(s)) ds / n]",
      "text": "Functional measure of richness in RKHS. σ(s) = stochastic process, f in functional space. Used in generalization bounds for functional data. Captures data-dependent complexity via stochastic integral.",
      "cognitive_load_label": 7.9
    },
    {
      "name": "EM Algorithm (Hierarchical Latent)",
      "formula": "Q(\\theta|\\theta^{(t)}) = E_{Z_1,Z_2|X,\\theta^{(t)}}[\\sum_{i=1}^N \\log p(X_i, Z_{1i}, Z_{2i}|\\theta)]",
      "text": "Hierarchical iterative method for nested latent models. Z_1, Z_2 = multi-level latent variables. Applied in complex GMMs, deep generative models. Handles multi-layer incomplete data. Nested expectation maximization.",
      "cognitive_load_label": 7.7
    },
    {
      "name": "Perceptron Learning Rule (Kernelized)",
      "formula": "w_{t+1} = w_t + \\eta \\sum_{i=1}^N (y_i - \\hat{y}_i) K(x_i, x) / ||K(x_i, x)||",
      "text": "Kernelized online classifier in RKHS. K = kernel function, η = learning rate. Used in non-linear classification. Converges in reproducing kernel Hilbert space. Kernelized weight update.",
      "cognitive_load_label": 7.5
    },
    {
      "name": "Radial Basis Function (Infinite Sum)",
      "formula": "\\phi(\\mathbf{x}) = \\sum_{j=1}^\\infty e^{-\\gamma_j ||\\mathbf{x} - \\mathbf{c}_j||^2 / j^2}",
      "text": "Infinite-sum kernel function in Hilbert space. γ_j = decaying width, c_j = centers. Used in infinite-dimensional kernel methods. Projects data into infinite feature space. Convergent infinite series.",
      "cognitive_load_label": 7.8
    },
    {
      "name": "Isomap Embedding (Dynamic Graph)",
      "formula": "d_G(i,j,t) = \\min_{P(t)} \\int_0^t w(s) d_X(p_k(s), p_{k+1}(s)) ds",
      "text": "Dynamic geodesic distance in time-varying graphs. w(s) = time-dependent weights, P(t) = path. Used in dynamic manifold learning. Preserves evolving manifold structure. Integrate over time.",
      "cognitive_load_label": 7.7
    },
    {
      "name": "t-SNE Gradient (High-Dimensional)",
      "formula": "\\frac{\\partial C}{\\partial y_i} = 4\\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + \\int_0^1 ||y_i(s) - y_j(s)||^2 ds)^{-1}",
      "text": "High-dimensional visualization gradient. p, q = affinities in high-dim space. Used in complex data visualization. Preserves intricate local structures. Integral-based regularization term.",
      "cognitive_load_label": 7.9
    },
    {
      "name": "Laplacian Eigenmaps (Spectral Decomposition)",
      "formula": "L = D - W + \\int_0^1 \\lambda(s) I(s) ds \\quad \\text{(Dynamic Graph Laplacian)}",
      "text": "Dynamic spectral manifold learning. λ(s) = time-varying regularization. Used in advanced spectral clustering. Preserves evolving neighborhoods. Integrate regularization over time.",
      "cognitive_load_label": 7.6
    },
    {
      "name": "Locally Linear Embedding (Nonlinear Constraints)",
      "formula": "\\min_W \\sum_i ||x_i - \\sum_{j \\in \\mathcal{N}_i} W_{ij}x_j||^2 \\quad \\text{s.t.} \\sum_{j \\in \\mathcal{N}_i} e^{W_{ij}} = 1",
      "text": "Nonlinear constrained embedding. W = nonlinear weights. Used in complex manifold learning. Assumes nonlinear local structure. Solve nonlinear constrained optimization.",
      "cognitive_load_label": 7.7
    },
    {
      "name": "Word2Vec Skip-Gram (Hierarchical)",
      "formula": "p(w_{i+j}|w_i, c) = \\frac{\\exp(\\sum_{l=1}^L v_{w_{i+j},l}^T (v_{w_i,l} + v_{c,l}))}{\\sum_{w=1}^V \\exp(\\sum_{l=1}^L v_{w,l}^T (v_{w_i,l} + v_{c,l}))}",
      "text": "Hierarchical word embedding model. L = number of layers, v_l = layer-wise vectors. Used in deep NLP models. Captures multi-level linguistic patterns. Hierarchical softmax.",
      "cognitive_load_label": 7.8
    },
    {
      "name": "GloVe Objective (Dynamic Co-occurrence)",
      "formula": "J = \\int_0^1 \\sum_{i,j=1}^V f(X_{ij}(t))(w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}(t))^2 dt",
      "text": "Dynamic global word vector optimization. X(t) = time-varying co-occurrence. Used in temporal word embeddings. Captures evolving semantics. Integrate over time.",
      "cognitive_load_label": 7.7
    },
    {
      "name": "Transformer Layer Normalization (Stochastic)",
      "formula": "\\text{LayerNorm}(x) = \\gamma \\frac{x - E[\\mu_t]}{\\sqrt{E[\\sigma_t^2] + \\epsilon}} + \\beta",
      "text": "Stochastic feature normalization. E[μ_t], E[σ_t^2] = expected statistics. Used in probabilistic transformers. Stabilizes stochastic training. Compute expected statistics.",
      "cognitive_load_label": 7.6
    },
    {
      "name": "BERT Masked LM (Dynamic Masking)",
      "formula": "p(w_m|w_{\\backslash m}, t) = \\frac{\\exp(h_m(t)^T e_{w_m}(t))}{\\int_0^1 \\sum_{v=1}^V \\exp(h_m(t)^T e_v(t)) dt}",
      "text": "Dynamic masked word prediction. h(t), e(t) = time-varying representations. Used in temporal language models. Learns evolving bidirectional context. Integrate over time.",
      "cognitive_load_label": 7.9
    },
    {
      "name": "Gradient Clipping (Stochastic Threshold)",
      "formula": "\\text{grad} = \\begin{cases} \\text{grad} & \\text{if } ||\\text{grad}|| \\leq E[\\tau_t] \\\\ E[\\tau_t] \\frac{\\text{grad}}{||\\text{grad}||} & \\text{otherwise} \\end{cases}",
      "text": "Stochastic gradient norm constraint. E[τ_t] = expected threshold. Used in probabilistic RNNs. Prevents exploding gradients in stochastic settings. Expected threshold clipping.",
      "cognitive_load_label": 7.5
    },
    {
      "name": "Dropout Regularization (Dynamic Rate)",
      "formula": "y = \\begin{cases} \\frac{x}{1-p(t)} & \\text{with probability } 1-p(t) \\\\ 0 & \\text{otherwise} \\end{cases}",
      "text": "Dynamic neuron deactivation rate. p(t) = time-varying dropout rate. Used in adaptive deep networks. Prevents co-adaptation dynamically. Time-dependent dropout.",
      "cognitive_load_label": 7.6
    },
    {
      "name": "Early Stopping (Bayesian)",
      "formula": "\\hat{k} = \\arg\\min_k E_{p(\\theta_k)}[\\hat{L}_k] \\quad \\text{where} \\quad \\hat{L}_k = L(\\theta_k, \\mathcal{D}_{val})",
      "text": "Bayesian validation performance termination. p(θ_k) = parameter distribution. Used in probabilistic training. Reduces overfitting with uncertainty. Expected loss minimization.",
      "cognitive_load_label": 7.7
    },
    {
      "name": "Knowledge Distillation (Hierarchical Teachers)",
      "formula": "L = \\alpha L_{task} + (1-\\alpha) \\sum_{h=1}^H \\sum_{i=1}^{M_h} T_{h,i}^2 L_{distill,h,i}",
      "text": "Hierarchical multi-teacher training. H = hierarchy levels, M_h = teachers per level. Used in complex model compression. Transfers hierarchical soft targets. Nested summation of losses.",
      "cognitive_load_label": 7.8
    },
    {
      "name": "Neural Tangent Kernel (Dynamic Kernel)",
      "formula": "\\Theta(x,x',t) = E_{\\theta \\sim p(t)}[\\int_0^t \\langle \\nabla_\\theta f(x;\\theta,s), \\nabla_\\theta f(x';\\theta,s) \\rangle ds]",
      "text": "Dynamic infinite-width network kernel. p(t) = time-varying parameter distribution. Used in temporal NN analysis. Connects dynamic NNs to kernels. Integrate over time.",
      "cognitive_load_label": 7.9
    },
    {
      "name": "Graph Convolution Layer (Spectral Dynamic)",
      "formula": "H^{(l+1)} = \\sigma(\\int_0^1 (\\tilde{D}(t)^{-1/2}\\tilde{A}(t)\\tilde{D}(t)^{-1/2})^k H^{(l)}W^{(l)}(t) dt)",
      "text": "Spectral dynamic message passing. Ã(t) = time-varying adjacency. Used in temporal GNNs. Captures evolving graph structures. Integrate over time-varying matrices.",
      "cognitive_load_label": 7.8
    },
  {
    "cognitive_load_label": 2.5,
    "name": "Shannon Entropy",
    "formula": "H(X) = -\\\\sum_{i=1}^{n} p(x_i) \\\\log p(x_i)",
    "text": "**Definition**: Measures uncertainty in a random variable. **Symbols**: \\( p(x_i) \\) = probability of event \\( x_i \\), \\( n \\) = number of events. **Usage**: Information theory, data compression. **Significance**: Quantifies information content. **Calculation**: Sum over all events, weighted by log-probability."
  },
  {
    "cognitive_load_label": 2.3,
    "name": "Black-Scholes Option Pricing",
    "formula": "C(S, t) = S_0 N(d_1) - K e^{-rt} N(d_2)",
    "text": "**Definition**: Prices European call options. **Symbols**: \\( S_0 \\) = current stock price, \\( K \\) = strike price, \\( r \\) = risk-free rate, \\( t \\) = time to expiration, \\( N \\) = cumulative normal distribution, \\( d_1, d_2 \\) = intermediate terms. **Usage**: Financial derivatives. **Significance**: Models option pricing dynamics. **Calculation**: Difference of two terms adjusted by normal distribution."
  },
  {
    "cognitive_load_label": 2.7,
    "name": "Lagrangian Mechanics",
    "formula": "L = T - V = \\\\sum_{i=1}^{n} \\\\frac{1}{2} m_i (\\\\dot{q}_i)^2 - V(q_1, \\\\dots, q_n)",
    "text": "**Definition**: Describes system dynamics using kinetic and potential energy. **Symbols**: \\( T \\) = kinetic energy, \\( V \\) = potential energy, \\( m_i \\) = mass, \\( \\\\dot{q}_i \\) = velocity, \\( q_i \\) = generalized coordinates. **Usage**: Classical mechanics. **Significance**: Simplifies complex system analysis. **Calculation**: Difference between summed kinetic terms and potential energy."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Fourier Transform",
    "formula": "\\\\hat{f}(\\\\xi) = \\\\int_{-\\\\infty}^{\\\\infty} f(x) e^{-2\\\\pi i \\\\xi x} dx",
    "text": "**Definition**: Transforms a function into its frequency components. **Symbols**: \\( f(x) \\) = original function, \\( \\\\xi \\) = frequency, \\( e \\) = exponential base, \\( i \\) = imaginary unit. **Usage**: Signal processing, image analysis. **Significance**: Enables frequency-domain analysis. **Calculation**: Integrate over all \\( x \\) with an exponential kernel."
  },
  {
    "cognitive_load_label": 3.0,
    "name": "Navier-Stokes Equation",
    "formula": "\\\\rho \\\\left( \\\\frac{\\\\partial \\\\mathbf{v}}{\\\\partial t} + (\\\\mathbf{v} \\\\cdot \\\\nabla) \\\\mathbf{v} \\\\right) = -\\\\nabla p + \\\\mu \\\\nabla^2 \\\\mathbf{v} + \\\\mathbf{f}",
    "text": "**Definition**: Governs fluid dynamics. **Symbols**: \\( \\\\rho \\) = density, \\( \\\\mathbf{v} \\) = velocity, \\( p \\) = pressure, \\( \\\\mu \\) = viscosity, \\( \\\\nabla \\) = gradient, \\( \\\\mathbf{f} \\) = external force. **Usage**: Aerodynamics, weather prediction. **Significance**: Models fluid motion. **Calculation**: Balances momentum with pressure, viscosity, and forces."
  },
  {
    "cognitive_load_label": 2.9,
    "name": "Dirac Equation",
    "formula": "i \\\\hbar \\\\frac{\\\\partial \\\\psi}{\\\\partial t} = \\\\left( c \\\\sum_{k=1}^{3} \\\\alpha_k p_k + \\\\beta m c^2 \\\\right) \\\\psi",
    "text": "**Definition**: Describes relativistic spin-1/2 particles. **Symbols**: \\( \\\\psi \\) = wavefunction, \\( \\\\hbar \\) = reduced Planck constant, \\( c \\) = speed of light, \\( \\\\alpha_k, \\\\beta \\) = Dirac matrices, \\( p_k \\) = momentum, \\( m \\) = mass. **Usage**: Quantum mechanics. **Significance**: Unifies quantum mechanics and relativity. **Calculation**: Applies Dirac operator to wavefunction."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Bayes’ Theorem (Continuous)",
    "formula": "p(\\\\theta | x) = \\\\frac{p(x | \\\\theta) p(\\\\theta)}{\\\\int p(x | \\\\theta') p(\\\\theta') d\\\\theta'}",
    "text": "**Definition**: Updates probability distributions with new data. **Symbols**: \\( p(\\\\theta | x) \\) = posterior, \\( p(x | \\\\theta) \\) = likelihood, \\( p(\\\\theta) \\) = prior, denominator = marginal likelihood. **Usage**: Bayesian inference, machine learning. **Significance**: Foundation of probabilistic reasoning. **Calculation**: Normalizes likelihood-prior product over all possible \\( \\\\theta \\)."
  },
  {
    "cognitive_load_label": 2.3,
    "name": "Gradient Descent Update",
    "formula": "\\\\theta_{t+1} = \\\\theta_t - \\\\eta \\\\nabla_{\\\\theta} J(\\\\theta_t)",
    "text": "**Definition**: Optimizes parameters by minimizing a cost function. **Symbols**: \\( \\\\theta_t \\) = parameters at step \\( t \\), \\( \\\\eta \\) = learning rate, \\( \\\\nabla_{\\\\theta} J \\) = gradient of cost function. **Usage**: Machine learning, optimization. **Significance**: Core of many ML algorithms. **Calculation**: Updates parameters in direction of steepest descent."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Einstein Field Equations",
    "formula": "G_{\\\\mu\\\\nu} + \\\\Lambda g_{\\\\mu\\\\nu} = \\\\frac{8\\\\pi G}{c^4} T_{\\\\mu\\\\nu}",
    "text": "**Definition**: Relates spacetime curvature to energy-momentum. **Symbols**: \\( G_{\\\\mu\\\\nu} \\) = Einstein tensor, \\( \\\\Lambda \\) = cosmological constant, \\( g_{\\\\mu\\\\nu} \\) = metric tensor, \\( T_{\\\\mu\\\\nu} \\) = stress-energy tensor, \\( G \\) = gravitational constant, \\( c \\) = speed of light. **Usage**: General relativity. **Significance**: Describes gravity as spacetime curvature. **Calculation**: Balances geometry and energy."
  },
  {
    "cognitive_load_label": 3.1,
    "name": "Wave Equation",
    "formula": "\\\\frac{\\\\partial^2 u}{\\\\partial t^2} = c^2 \\\\nabla^2 u",
    "text": "**Definition**: Describes wave propagation. **Symbols**: \\( u \\) = wave amplitude, \\( t \\) = time, \\( c \\) = wave speed, \\( \\\\nabla^2 \\) = Laplacian operator. **Usage**: Physics, acoustics, electromagnetism. **Significance**: Models wave behavior in various media. **Calculation**: Relates second time derivative to spatial Laplacian."
  },
  {
    "cognitive_load_label": 3.0,
    "name": "Boltzmann Equation",
    "formula": "\\\\frac{\\\\partial f}{\\\\partial t} + \\\\mathbf{v} \\\\cdot \\\\nabla f + \\\\frac{\\\\mathbf{F}}{m} \\\\cdot \\\\nabla_{\\\\mathbf{v}} f = \\\\left( \\\\frac{\\\\partial f}{\\\\partial t} \\\\right)_{\\\\text{coll}}",
    "text": "**Definition**: Describes particle distribution evolution in a gas. **Symbols**: \\( f \\) = distribution function, \\( \\\\mathbf{v} \\) = velocity, \\( \\\\mathbf{F} \\) = force, \\( m \\) = mass, \\( \\\\nabla_{\\\\mathbf{v}} \\) = velocity gradient. **Usage**: Statistical mechanics, kinetic theory. **Significance**: Models non-equilibrium systems. **Calculation**: Balances transport and collision terms."
  },
  {
    "cognitive_load_label": 2.8,
    "name": "Schrödinger Equation",
    "formula": "i \\\\hbar \\\\frac{\\\\partial \\\\psi}{\\\\partial t} = -\\\\frac{\\\\hbar^2}{2m} \\\\nabla^2 \\\\psi + V \\\\psi",
    "text": "**Definition**: Governs quantum state evolution. **Symbols**: \\( \\\\psi \\) = wavefunction, \\( \\\\hbar \\) = reduced Planck constant, \\( m \\) = mass, \\( \\\\nabla^2 \\) = Laplacian, \\( V \\) = potential. **Usage**: Quantum mechanics. **Significance**: Foundation of quantum theory. **Calculation**: Combines kinetic and potential energy terms."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Kullback-Leibler Divergence",
    "formula": "D_{KL}(P || Q) = \\\\int_{-\\\\infty}^{\\\\infty} p(x) \\\\log \\\\left( \\\\frac{p(x)}{q(x)} \\\\right) dx",
    "text": "**Definition**: Measures divergence between two probability distributions. **Symbols**: \\( P, Q \\) = distributions, \\( p(x), q(x) \\) = probability densities. **Usage**: Information theory, machine learning. **Significance**: Quantifies information loss. **Calculation**: Integrates the log-ratio of densities over the entire domain."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Heat Equation",
    "formula": "\\\\frac{\\\\partial u}{\\\\partial t} = \\\\alpha \\\\nabla^2 u",
    "text": "**Definition**: Models heat diffusion over time. **Symbols**: \\( u \\) = temperature, \\( t \\) = time, \\( \\\\alpha \\) = thermal diffusivity, \\( \\\\nabla^2 \\) = Laplacian. **Usage**: Thermodynamics, physics. **Significance**: Describes heat transfer in materials. **Calculation**: Relates temperature change to spatial distribution."
  },
  {
    "cognitive_load_label": 2.6,
    "name": "Maxwell’s Equations (Faraday’s Law)",
    "formula": "\\\\nabla \\\\times \\\\mathbf{E} = -\\\\frac{\\\\partial \\\\mathbf{B}}{\\\\partial t}",
    "text": "**Definition**: Describes electromagnetic induction. **Symbols**: \\( \\\\mathbf{E} \\) = electric field, \\( \\\\mathbf{B} \\) = magnetic field, \\( \\\\nabla \\\\times \\) = curl operator. **Usage**: Electromagnetism. **Significance**: Links changing magnetic fields to electric fields. **Calculation**: Curl of electric field equals negative time derivative of magnetic field."
  },
  {
    "cognitive_load_label": 2.4,
    "name": "Poisson Equation",
    "formula": "\\\\nabla^2 \\\\phi = -\\\\frac{\\\\rho}{\\\\epsilon_0}",
    "text": "**Definition**: Relates potential to charge density. **Symbols**: \\( \\\\phi \\) = potential, \\( \\\\rho \\) = charge density, \\( \\\\epsilon_0 \\) = permittivity of free space, \\( \\\\nabla^2 \\) = Laplacian. **Usage**: Electrostatics, gravitational fields. **Significance**: Solves for potential in presence of sources. **Calculation**: Laplacian of potential equals scaled charge density."
  },
  {
    "cognitive_load_label": 2.1,
    "name": "Logistic Growth Model",
    "formula": "\\\\frac{dN}{dt} = r N \\\\left( 1 - \\\\frac{N}{K} \\\\right)",
    "text": "**Definition**: Models population growth with carrying capacity. **Symbols**: \\( N \\) = population size, \\( t \\) = time, \\( r \\) = growth rate, \\( K \\) = carrying capacity. **Usage**: Ecology, biology. **Significance**: Captures resource-limited growth dynamics. **Calculation**: Growth rate decreases as population approaches carrying capacity."
  },
  {
    "cognitive_load_label": 3.0,
    "name": "Euler-Lagrange Equation",
    "formula": "\\\\frac{\\\\partial L}{\\\\partial q_i} - \\\\frac{d}{dt} \\\\left( \\\\frac{\\\\partial L}{\\\\partial \\\\dot{q}_i} \\\\right) = 0",
    "text": "**Definition**: Derives equations of motion in Lagrangian mechanics. **Symbols**: \\( L \\) = Lagrangian, \\( q_i \\) = generalized coordinate, \\( \\\\dot{q}_i \\) = time derivative of \\( q_i \\). **Usage**: Classical mechanics, field theory. **Significance**: Minimizes action to find system dynamics. **Calculation**: Balances Lagrangian derivatives."
  },
  {
    "cognitive_load_label": 2.5,
    "name": "Riemann Zeta Function",
    "formula": "\\\\zeta(s) = \\\\sum_{n=1}^{\\\\infty} \\\\frac{1}{n^s}",
    "text": "**Definition**: Generalizes the harmonic series to complex numbers. **Symbols**: \\( s \\) = complex number, \\( n \\) = positive integer. **Usage**: Number theory, physics. **Significance**: Key to understanding prime number distribution. **Calculation**: Sums inverse powers over all positive integers."
  },
  {
    "cognitive_load_label": 2.8,
    "name": "Gaussian Mixture Model Likelihood",
    "formula": "p(\\\\mathbf{x}) = \\\\sum_{k=1}^{K} \\\\pi_k \\\\mathcal{N}(\\\\mathbf{x} | \\\\mu_k, \\\\Sigma_k)",
    "text": "**Definition**: Represents data as a mixture of Gaussians. **Symbols**: \\( \\\\mathbf{x} \\) = data point, \\( \\\\pi_k \\) = mixing coefficient, \\( \\\\mathcal{N} \\) = Gaussian, \\( \\\\mu_k \\) = mean, \\( \\\\Sigma_k \\) = covariance. **Usage**: Machine learning, clustering. **Significance**: Models complex data distributions. **Calculation**: Weighted sum of Gaussian densities."
  },
    {
        "text": "**Definition**: Dynamic graph partitioning algorithm.",
        "image_path": "",
        "cognitive_load_label": 6.8,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Partition Balance",
                    "description": "Measures the balance of graph partitions.",
                    "calculation": {
                        "methods": [
                            {"name": "Edge Cut Ratio", "description": "Calculate ratio of cut edges to total edges."},
                            {"name": "Node Distribution", "description": "Compute standard deviation of node counts."},
                            {"name": "Graph Entropy", "description": "Measure entropy of partition distribution."}
                        ],
                        "fusion_formula": "Weighted sum: Edge Cut (40%) + Node Distribution (30%) + Entropy (30%)"
                    }
                },
                {
                    "metric": "Partition Speed",
                    "description": "Evaluates the computational efficiency of partitioning.",
                    "calculation": {
                        "methods": [
                            {"name": "Runtime Analysis", "description": "Measure algorithm execution time."},
                            {"name": "Iteration Count", "description": "Count number of partitioning iterations."}
                        ],
                        "fusion_formula": "Weighted sum: Runtime (60%) + Iterations (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Graph neural networks for temporal data analysis.",
        "image_path": "",
        "cognitive_load_label": 6.5,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Temporal Accuracy",
                    "description": "Accuracy of predictions over time.",
                    "calculation": {
                        "methods": [
                            {"name": "Mean Squared Error", "description": "Compute MSE over temporal predictions."},
                            {"name": "Temporal Correlation", "description": "Measure correlation across time steps."},
                            {"name": "Dynamic Loss", "description": "Evaluate loss on dynamic graph changes."}
                        ],
                        "fusion_formula": "Weighted sum: MSE (50%) + Correlation (30%) + Dynamic Loss (20%)"
                    }
                },
                {
                    "metric": "Graph Connectivity",
                    "description": "Measures connectivity in temporal graphs.",
                    "calculation": {
                        "methods": [
                            {"name": "Edge Density", "description": "Calculate ratio of active edges."},
                            {"name": "Node Degree", "description": "Compute average node degree over time."}
                        ],
                        "fusion_formula": "Weighted sum: Edge Density (50%) + Node Degree (50%)"
                    }
                },
                {
                    "metric": "Scalability",
                    "description": "Evaluates model performance on large graphs.",
                    "calculation": {
                        "methods": [
                            {"name": "Runtime Scaling", "description": "Measure runtime with increasing graph size."}
                        ],
                        "fusion_formula": "Runtime Scaling (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "3.6.1 Dynamic Programming. Optimizes recursive solutions.",
        "image_path": "",
        "cognitive_load_label": 7.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Time Complexity",
                    "description": "Measures computational efficiency of DP.",
                    "calculation": {
                        "methods": [
                            {"name": "Big-O Analysis", "description": "Derive worst-case time complexity."},
                            {"name": "Runtime Profiling", "description": "Measure execution time on benchmarks."}
                        ],
                        "fusion_formula": "Weighted sum: Big-O (60%) + Profiling (40%)"
                    }
                },
                {
                    "metric": "Space Complexity",
                    "description": "Evaluates memory usage of DP.",
                    "calculation": {
                        "methods": [
                            {"name": "Memory Profiling", "description": "Measure peak memory usage."}
                        ],
                        "fusion_formula": "Memory Profiling (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Stochastic gradient clipping.",
        "image_path": "",
        "cognitive_load_label": 6.3,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Gradient Stability",
                    "description": "Measures stability of gradient updates.",
                    "calculation": {
                        "methods": [
                            {"name": "Gradient Norm", "description": "Compute norm of gradients."},
                            {"name": "Clipping Ratio", "description": "Measure proportion of clipped gradients."}
                        ],
                        "fusion_formula": "Weighted sum: Norm (50%) + Clipping Ratio (50%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.31: Visualizing high-dimensional embeddings.",
        "image_path": "dataset/images/embeddings_viz.png",
        "cognitive_load_label": 5.8,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Embedding Quality",
                    "description": "Evaluates quality of visualized embeddings.",
                    "calculation": {
                        "methods": [
                            {"name": "Cosine Similarity", "description": "Measure similarity between embeddings."}
                        ],
                        "fusion_formula": "Cosine Similarity (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "3.7 Hash Tables. Efficient key-value storage.",
        "image_path": "",
        "cognitive_load_label": 5.5,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Access Time",
                    "description": "Measures lookup efficiency.",
                    "calculation": {
                        "methods": [
                            {"name": "Average Lookup Time", "description": "Measure average time for key lookups."}
                        ],
                        "fusion_formula": "Lookup Time (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Time-varying attention mechanism.",
        "image_path": "",
        "cognitive_load_label": 7.2,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Attention Dynamics",
                    "description": "Evaluates temporal changes in attention.",
                    "calculation": {
                        "methods": [
                            {"name": "Attention Variance", "description": "Compute variance of attention weights."},
                            {"name": "Temporal Correlation", "description": "Measure correlation across time steps."}
                        ],
                        "fusion_formula": "Weighted sum: Variance (60%) + Correlation (40%)"
                    }
                },
                {
                    "metric": "Model Performance",
                    "description": "Measures impact on model accuracy.",
                    "calculation": {
                        "methods": [
                            {"name": "Accuracy Gain", "description": "Measure improvement in prediction accuracy."}
                        ],
                        "fusion_formula": "Accuracy Gain (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to parallel computing paradigms.",
        "image_path": "",
        "cognitive_load_label": 6.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Parallel Efficiency",
                    "description": "Evaluates efficiency of parallelization.",
                    "calculation": {
                        "methods": [
                            {"name": "Speedup Ratio", "description": "Measure speedup over sequential execution."}
                        ],
                        "fusion_formula": "Speedup Ratio (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Conditional variational autoencoder.",
        "image_path": "",
        "cognitive_load_label": 7.5,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Latent Representation",
                    "description": "Evaluates quality of latent space.",
                    "calculation": {
                        "methods": [
                            {"name": "KL Divergence", "description": "Measure divergence from prior distribution."},
                            {"name": "Reconstruction Loss", "description": "Compute reconstruction error."},
                            {"name": "Conditional Accuracy", "description": "Evaluate conditional generation accuracy."}
                        ],
                        "fusion_formula": "Weighted sum: KL (40%) + Reconstruction (40%) + Accuracy (20%)"
                    }
                },
                {
                    "metric": "Model Complexity",
                    "description": "Measures computational complexity.",
                    "calculation": {
                        "methods": [
                            {"name": "Parameter Count", "description": "Count total model parameters."}
                        ],
                        "fusion_formula": "Parameter Count (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "3.8 Sorting Algorithms. Comparing efficiency metrics.",
        "image_path": "",
        "cognitive_load_label": 6.7,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Sort Efficiency",
                    "description": "Evaluates sorting algorithm performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Time Complexity", "description": "Derive worst-case time complexity."},
                            {"name": "Comparison Count", "description": "Count number of comparisons."}
                        ],
                        "fusion_formula": "Weighted sum: Time Complexity (60%) + Comparisons (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.32: Tree-based data structure visualization.",
        "image_path": "dataset/images/tree_viz.png",
        "cognitive_load_label": 5.9,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Tree Balance",
                    "description": "Measures balance of tree structure.",
                    "calculation": {
                        "methods": [
                            {"name": "Height Difference", "description": "Compute max height difference."}
                        ],
                        "fusion_formula": "Height Difference (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Multi-layer perceptron architecture.",
        "image_path": "",
        "cognitive_load_label": 6.9,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Network Performance",
                    "description": "Evaluates MLP accuracy.",
                    "calculation": {
                        "methods": [
                            {"name": "Accuracy Metric", "description": "Measure classification accuracy."},
                            {"name": "Loss Curve", "description": "Analyze training loss convergence."}
                        ],
                        "fusion_formula": "Weighted sum: Accuracy (70%) + Loss (30%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.1 Machine Learning. Supervised learning basics.",
        "image_path": "",
        "cognitive_load_label": 6.2,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Model Accuracy",
                    "description": "Measures supervised learning performance.",
                    "calculation": {
                        "methods": [
                            {"name": "F1 Score", "description": "Compute F1 score for classification."}
                        ],
                        "fusion_formula": "F1 Score (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Kernelized support vector machine.",
        "image_path": "",
        "cognitive_load_label": 7.3,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Classification Margin",
                    "description": "Evaluates SVM margin quality.",
                    "calculation": {
                        "methods": [
                            {"name": "Margin Width", "description": "Measure distance between support vectors."},
                            {"name": "Kernel Efficiency", "description": "Evaluate kernel computation time."}
                        ],
                        "fusion_formula": "Weighted sum: Margin (60%) + Kernel (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.33: Neural network training pipeline.",
        "image_path": "dataset/images/nn_pipeline.png",
        "cognitive_load_label": 6.4,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Training Stability",
                    "description": "Evaluates stability of training process.",
                    "calculation": {
                        "methods": [
                            {"name": "Loss Variance", "description": "Measure variance in training loss."}
                        ],
                        "fusion_formula": "Loss Variance (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.2 Deep Learning. Convolutional neural networks.",
        "image_path": "",
        "cognitive_load_label": 6.8,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Feature Extraction",
                    "description": "Evaluates CNN feature quality.",
                    "calculation": {
                        "methods": [
                            {"name": "Activation Maps", "description": "Analyze activation map diversity."},
                            {"name": "Filter Efficiency", "description": "Measure filter response strength."}
                        ],
                        "fusion_formula": "Weighted sum: Activations (60%) + Filters (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Recurrent neural network dynamics.",
        "image_path": "",
        "cognitive_load_label": 7.4,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Sequence Modeling",
                    "description": "Evaluates RNN sequence performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Sequence Loss", "description": "Compute loss on sequence predictions."},
                            {"name": "Gradient Flow", "description": "Measure gradient flow stability."}
                        ],
                        "fusion_formula": "Weighted sum: Loss (70%) + Gradient (30%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to quantum computing principles.",
        "image_path": "",
        "cognitive_load_label": 7.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Quantum Advantage",
                    "description": "Evaluates quantum speedup.",
                    "calculation": {
                        "methods": [
                            {"name": "Circuit Depth", "description": "Measure quantum circuit depth."},
                            {"name": "Qubit Count", "description": "Count number of qubits used."}
                        ],
                        "fusion_formula": "Weighted sum: Depth (50%) + Qubits (50%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Probabilistic graphical model.",
        "image_path": "",
        "cognitive_load_label": 7.1,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Inference Accuracy",
                    "description": "Evaluates graphical model inference.",
                    "calculation": {
                        "methods": [
                            {"name": "Log-Likelihood", "description": "Compute log-likelihood of predictions."},
                            {"name": "Variable Dependency", "description": "Measure dependency strength."}
                        ],
                        "fusion_formula": "Weighted sum: Likelihood (60%) + Dependency (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.3 Reinforcement Learning. Policy gradient methods.",
        "image_path": "",
        "cognitive_load_label": 6.6,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Policy Performance",
                    "description": "Evaluates policy gradient effectiveness.",
                    "calculation": {
                        "methods": [
                            {"name": "Reward Curve", "description": "Analyze cumulative reward over episodes."}
                        ],
                        "fusion_formula": "Reward Curve (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.34: Graph traversal algorithm diagram.",
        "image_path": "dataset/images/graph_traversal.png",
        "cognitive_load_label": 6.1,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Traversal Efficiency",
                    "description": "Evaluates graph traversal performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Path Length", "description": "Measure average path length."}
                        ],
                        "fusion_formula": "Path Length (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Stochastic optimization algorithm.",
        "image_path": "",
        "cognitive_load_label": 6.7,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Optimization Convergence",
                    "description": "Evaluates convergence speed.",
                    "calculation": {
                        "methods": [
                            {"name": "Loss Reduction", "description": "Measure rate of loss reduction."}
                        ],
                        "fusion_formula": "Loss Reduction (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.4 Natural Language Processing. Tokenization basics.",
        "image_path": "",
        "cognitive_load_label": 5.7,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Tokenization Accuracy",
                    "description": "Evaluates tokenization quality.",
                    "calculation": {
                        "methods": [
                            {"name": "Boundary Detection", "description": "Measure accuracy of token boundaries."}
                        ],
                        "fusion_formula": "Boundary Detection (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Transformer-based language model.",
        "image_path": "",
        "cognitive_load_label": 7.6,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Language Modeling",
                    "description": "Evaluates transformer performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Perplexity", "description": "Compute model perplexity on test data."},
                            {"name": "Attention Efficiency", "description": "Measure attention computation time."},
                            {"name": "BLEU Score", "description": "Evaluate generated text quality."}
                        ],
                        "fusion_formula": "Weighted sum: Perplexity (50%) + Attention (30%) + BLEU (20%)"
                    }
                },
                {
                    "metric": "Scalability",
                    "description": "Measures model scalability.",
                    "calculation": {
                        "methods": [
                            {"name": "Parameter Scaling", "description": "Evaluate performance with increasing parameters."}
                        ],
                        "fusion_formula": "Parameter Scaling (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to distributed systems architecture.",
        "image_path": "",
        "cognitive_load_label": 6.9,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "System Scalability",
                    "description": "Evaluates distributed system performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Throughput", "description": "Measure system throughput under load."},
                            {"name": "Latency", "description": "Measure average response time."}
                        ],
                        "fusion_formula": "Weighted sum: Throughput (60%) + Latency (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Bayesian inference framework.",
        "image_path": "",
        "cognitive_load_label": 7.2,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Inference Quality",
                    "description": "Evaluates Bayesian model accuracy.",
                    "calculation": {
                        "methods": [
                            {"name": "Posterior Accuracy", "description": "Measure accuracy of posterior distribution."},
                            {"name": "Evidence Computation", "description": "Evaluate marginal likelihood."}
                        ],
                        "fusion_formula": "Weighted sum: Posterior (70%) + Evidence (30%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.35: Database query optimization diagram.",
        "image_path": "dataset/images/query_optimization.png",
        "cognitive_load_label": 6.3,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Query Efficiency",
                    "description": "Evaluates query optimization performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Execution Time", "description": "Measure query execution time."}
                        ],
                        "fusion_formula": "Execution Time (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.5 Cloud Computing. Scalable infrastructure design.",
        "image_path": "",
        "cognitive_load_label": 6.5,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Infrastructure Scalability",
                    "description": "Evaluates cloud system scalability.",
                    "calculation": {
                        "methods": [
                            {"name": "Resource Utilization", "description": "Measure resource usage efficiency."}
                        ],
                        "fusion_formula": "Resource Utilization (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Homomorphic encryption scheme.",
        "image_path": "",
        "cognitive_load_label": 7.7,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Encryption Security",
                    "description": "Evaluates homomorphic encryption strength.",
                    "calculation": {
                        "methods": [
                            {"name": "Key Strength", "description": "Measure cryptographic key robustness."},
                            {"name": "Computation Overhead", "description": "Evaluate encrypted computation time."},
                            {"name": "Security Proof", "description": "Analyze formal security guarantees."}
                        ],
                        "fusion_formula": "Weighted sum: Key (40%) + Overhead (30%) + Proof (30%)"
                    }
                },
                {
                    "metric": "Practicality",
                    "description": "Measures real-world applicability.",
                    "calculation": {
                        "methods": [
                            {"name": "Deployment Time", "description": "Measure time to deploy encryption."}
                        ],
                        "fusion_formula": "Deployment Time (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to cybersecurity threat models.",
        "image_path": "",
        "cognitive_load_label": 6.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Threat Detection",
                    "description": "Evaluates threat model effectiveness.",
                    "calculation": {
                        "methods": [
                            {"name": "Detection Rate", "description": "Measure true positive rate."}
                        ],
                        "fusion_formula": "Detection Rate (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Adversarial training objective.",
        "image_path": "",
        "cognitive_load_label": 7.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Robustness",
                    "description": "Evaluates model robustness to adversarial attacks.",
                    "calculation": {
                        "methods": [
                            {"name": "Adversarial Loss", "description": "Measure loss under adversarial inputs."},
                            {"name": "Attack Success Rate", "description": "Evaluate attack success probability."}
                        ],
                        "fusion_formula": "Weighted sum: Loss (60%) + Success Rate (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.6 Blockchain Technology. Consensus mechanisms.",
        "image_path": "",
        "cognitive_load_label": 6.8,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Consensus Efficiency",
                    "description": "Evaluates consensus mechanism performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Confirmation Time", "description": "Measure time to confirm transactions."},
                            {"name": "Energy Consumption", "description": "Evaluate energy usage of consensus."}
                        ],
                        "fusion_formula": "Weighted sum: Time (60%) + Energy (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.36: Network topology visualization.",
        "image_path": "dataset/images/network_topology.png",
        "cognitive_load_label": 6.2,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Topology Complexity",
                    "description": "Evaluates complexity of network topology.",
                    "calculation": {
                        "methods": [
                            {"name": "Edge Count", "description": "Count total edges in topology."}
                        ],
                        "fusion_formula": "Edge Count (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Differential privacy guarantee.",
        "image_path": "",
        "cognitive_load_label": 7.4,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Privacy Budget",
                    "description": "Evaluates differential privacy strength.",
                    "calculation": {
                        "methods": [
                            {"name": "Epsilon Value", "description": "Measure privacy budget epsilon."},
                            {"name": "Noise Scale", "description": "Evaluate noise added for privacy."}
                        ],
                        "fusion_formula": "Weighted sum: Epsilon (60%) + Noise (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to data science methodologies.",
        "image_path": "",
        "cognitive_load_label": 5.6,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Pipeline Efficiency",
                    "description": "Evaluates data science pipeline performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Processing Time", "description": "Measure pipeline execution time."}
                        ],
                        "fusion_formula": "Processing Time (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Monte Carlo simulation method.",
        "image_path": "",
        "cognitive_load_label": 6.9,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Simulation Accuracy",
                    "description": "Evaluates Monte Carlo simulation quality.",
                    "calculation": {
                        "methods": [
                            {"name": "Convergence Rate", "description": "Measure rate of convergence."},
                            {"name": "Sample Variance", "description": "Compute variance of simulation samples."}
                        ],
                        "fusion_formula": "Weighted sum: Convergence (60%) + Variance (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.7 Big Data Analytics. MapReduce framework.",
        "image_path": "",
        "cognitive_load_label": 6.4,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Processing Scalability",
                    "description": "Evaluates MapReduce scalability.",
                    "calculation": {
                        "methods": [
                            {"name": "Throughput", "description": "Measure data processing throughput."}
                        ],
                        "fusion_formula": "Throughput (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.37: Data pipeline architecture diagram.",
        "image_path": "dataset/images/data_pipeline.png",
        "cognitive_load_label": 6.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Pipeline Latency",
                    "description": "Evaluates data pipeline latency.",
                    "calculation": {
                        "methods": [
                            {"name": "End-to-End Time", "description": "Measure total pipeline execution time."}
                        ],
                        "fusion_formula": "End-to-End Time (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Gradient boosting ensemble.",
        "image_path": "",
        "cognitive_load_label": 7.3,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Ensemble Performance",
                    "description": "Evaluates gradient boosting accuracy.",
                    "calculation": {
                        "methods": [
                            {"name": "Prediction Error", "description": "Measure ensemble prediction error."},
                            {"name": "Tree Depth", "description": "Evaluate average tree depth."}
                        ],
                        "fusion_formula": "Weighted sum: Error (60%) + Depth (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to software engineering principles.",
        "image_path": "",
        "cognitive_load_label": 5.8,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Code Quality",
                    "description": "Evaluates software engineering practices.",
                    "calculation": {
                        "methods": [
                            {"name": "Lint Score", "description": "Measure code linting errors."}
                        ],
                        "fusion_formula": "Lint Score (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Regularized logistic regression.",
        "image_path": "",
        "cognitive_load_label": 7.1,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Model Fit",
                    "description": "Evaluates logistic regression performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Log-Loss", "description": "Compute logistic loss."},
                            {"name": "Regularization Strength", "description": "Measure impact of regularization."}
                        ],
                        "fusion_formula": "Weighted sum: Log-Loss (70%) + Regularization (30%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.8 Operating Systems. Process scheduling algorithms.",
        "image_path": "",
        "cognitive_load_label": 6.7,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Scheduling Efficiency",
                    "description": "Evaluates scheduling algorithm performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Turnaround Time", "description": "Measure average process turnaround time."}
                        ],
                        "fusion_formula": "Turnaround Time (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.38: Memory management visualization.",
        "image_path": "dataset/images/memory_management.png",
        "cognitive_load_label": 6.2,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Memory Utilization",
                    "description": "Evaluates memory management efficiency.",
                    "calculation": {
                        "methods": [
                            {"name": "Allocation Efficiency", "description": "Measure memory allocation success rate."}
                        ],
                        "fusion_formula": "Allocation Efficiency (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Virtual memory paging system.",
        "image_path": "",
        "cognitive_load_label": 7.2,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Paging Performance",
                    "description": "Evaluates virtual memory efficiency.",
                    "calculation": {
                        "methods": [
                            {"name": "Page Fault Rate", "description": "Measure frequency of page faults."},
                            {"name": "Swap Time", "description": "Evaluate time spent swapping pages."}
                        ],
                        "fusion_formula": "Weighted sum: Fault Rate (60%) + Swap Time (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to computer vision techniques.",
        "image_path": "",
        "cognitive_load_label": 6.1,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Detection Accuracy",
                    "description": "Evaluates computer vision model performance.",
                    "calculation": {
                        "methods": [
                            {"name": "mAP Score", "description": "Compute mean Average Precision."}
                        ],
                        "fusion_formula": "mAP Score (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Optical character recognition model.",
        "image_path": "",
        "cognitive_load_label": 6.8,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Recognition Accuracy",
                    "description": "Evaluates OCR model performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Character Error Rate", "description": "Measure character recognition errors."}
                        ],
                        "fusion_formula": "Character Error Rate (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "4.9 Robotics. Path planning algorithms.",
        "image_path": "",
        "cognitive_load_label": 6.6,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Path Efficiency",
                    "description": "Evaluates path planning performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Path Length", "description": "Measure total path length."},
                            {"name": "Collision Avoidance", "description": "Evaluate collision-free paths."}
                        ],
                        "fusion_formula": "Weighted sum: Length (60%) + Avoidance (40%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Figure 2.39: Sensor data fusion diagram.",
        "image_path": "dataset/images/sensor_fusion.png",
        "cognitive_load_label": 6.0,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Fusion Accuracy",
                    "description": "Evaluates sensor data fusion quality.",
                    "calculation": {
                        "methods": [
                            {"name": "Estimation Error", "description": "Measure error in fused estimates."}
                        ],
                        "fusion_formula": "Estimation Error (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "**Definition**: Kalman filter for state estimation.",
        "image_path": "",
        "cognitive_load_label": 7.5,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Estimation Accuracy",
                    "description": "Evaluates Kalman filter performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Mean Squared Error", "description": "Compute MSE of state estimates."},
                            {"name": "Covariance Consistency", "description": "Measure covariance matrix consistency."},
                            {"name": "Update Rate", "description": "Evaluate filter update frequency."}
                        ],
                        "fusion_formula": "Weighted sum: MSE (50%) + Covariance (30%) + Update (20%)"
                    }
                },
                {
                    "metric": "Computational Efficiency",
                    "description": "Measures filter computational cost.",
                    "calculation": {
                        "methods": [
                            {"name": "Runtime Analysis", "description": "Measure filter execution time."}
                        ],
                        "fusion_formula": "Runtime Analysis (100%)"
                    }
                }
            ]
        }
    },
    {
        "text": "Introduction to game theory applications.",
        "image_path": "",
        "cognitive_load_label": 5.9,
        "structured_data": {
            "table_type": "technical_metrics",
            "columns": ["Metric", "Description", "Calculation Method"],
            "rows": [
                {
                    "metric": "Strategy Effectiveness",
                    "description": "Evaluates game theory strategy performance.",
                    "calculation": {
                        "methods": [
                            {"name": "Payoff Analysis", "description": "Measure expected payoffs."}
                        ],
                        "fusion_formula": "Payoff Analysis (100%)"
                    }
                }
            ]
        }
    }
]
